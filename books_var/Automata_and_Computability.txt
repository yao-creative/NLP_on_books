
_____________________________________________
UNDERGRADUATE 
TEXTS 
IN 
COMPUTER 
SCIENCE 
Springer 
Science+Business 
Media, 
LLC 
Editors 
David 
Gries 
Fred 
B. 
Schneider 

_____________________________________________
UNDERGRADUATE 
TEXTS 
IN 
COMPUTER 
SCIENCE 
Beid/er, 
Data 
Structures 
and 
Aigorithms 
Bergin, 
Data 
Structure 
Programming 
Brooks, 
Problem 
Solving 
with 
Fortran 
90 
Brooks, 
C 
Programming: 
The 
Essentials 
for 
Engineers 
and 
Scientists 
Dandamudi, 
Introduction 
to 
Assembly 
Language 
Programming 
Grillmeyer, 
Exploring 
Computer 
Science 
with 
Scheme 
Ja/ote, 
An 
Integrated 
Approach 
to 
Software 
Engineering, 
Second 
Edition 
Kizza, 
Ethical 
and 
Social 
Issues 
in 
the 
Information 
Age 
Kozen, 
Automata 
and 
Computability 
Merritt 
and 
Stix, 
Migrating 
from 
Pascal 
to 
C++ 
Pearce, 
Programming 
and 
Meta-Programming 
in 
Scheme 
Zeigler, 
Objects 
and 
Systems 

_____________________________________________
Dexter 
C. 
Kozen 
Automata 
and 
Computability 
, 
Springer 

_____________________________________________
Dexter 
C. 
Kozen 
Department 
of 
Computer 
Science 
Cornell 
University 
Ithaca, 
NY 
14853-7501 
USA 
Series 
Editors 
David 
Gries 
Department 
of 
Computer 
Science 
Boyd 
Studies 
Research 
Center 
The 
University 
of 
Georgia 
Athens, 
Georgia 
30605 
USA 
SPIN: 
10875033 
Fred 
B. 
Schneider 
Department 
of 
Computer 
Science 
Cornell 
University 
Upson 
Hall 
Ithaca, 
NY 
14853-7501 
USA 
2002 
Reprint. 
"This 
reprint 
has 
been 
authorized 
by 
Sprinser-Verlas, 
Heidelberg, 
Germany, 
for 
sale 
in 
India, 
Pakistan, 
Bangladesh, 
Nepal 
and 
Sri 
Lanka 
only 
and 
not 
for 
export 
therefrom". 
Oll 
the 
cover: 
Cover 
photo 
taken 
by 
John 
Still/Photonica. 
With 
1 
figure. 
Library 
of 
Congress 
Catalosing-in-Publication 
Oata 
Kozen, 
Dexter, 
1951 
-
Automata 
and 
computability/Oexter 
C. 
Kozen. 
p. 
cm. 
-
(Undergraduate 
texts 
in 
computer 
science) 
Indudes 
bibliographical 
references 
and 
index 
ISBN 
978-3-642-85708-9 
ISBN 
978-3-642-85706-5 
(eBook) 
DOI 
10.1007/978-3-642-85706-5 
1. 
Machine 
theory. 
2. 
Computable 
functions. 
I. 
Title. 
11 
Series. 
QA267.K69 
1997 
511.3-dc21 
96-37409 
Cl 
1977 
Springer 
Science+Business 
Media 
New 
York 
Originally 
published 
by 
Springer-Verlag 
New 
York, 
Inc 
in 
1977. 
All 
RIGHTS 
RESERVEO. 
This 
work 
may 
not 
be 
translated 
or 
copied 
in 
whole 
or 
in 
part 
without 
the 
written 
permission 
of 
the 
publisher 
Springer 
Science+Business 
Media, 
LLC 
except 
for 
brief 
excerpts 
in 
connection 
with 
reviews 
of 
scholarly 
analysis. 
Use 
in 
connection 
with 
any 
form 
of 
information 
storage 
and 
retrieval, 
electronic 
adaptation, 
computer 
software, 
or 
by 
similar 
or 
dissimilar 
methodology 
now 
known 
or 
hereafter 
developed 
is 
forbidden. 
The 
use 
of 
general 
descriptive 
names, 
trade 
names, 
trademarks, 
etc., 
in 
this 
publication, 
even 
if 
the 
former 
are 
not 
especially 
identified, 
is 
not 
to 
be 
taken 
as 
a 
sign 
that 
such 
names, 
as 
understood 
by 
the 
Trade 
Marks 
and 
Merchandise 
Marks 
Act, 
may 
accordingly 
be 
used 
freely 
by 
anyone. 
Production 
managed 
by 
Francine 
McNeill; 
manufacturing 
supervised 
by 
Jacqui 
Ashri. 
Photocomposed 
copy 
prepared 
using 
Springer's 
laTeX 
style 
macro. 
Printed 
and 
bound 
at 
Eastem 
Press 
(Bangalore) 
Pvt. 
Ltd., 
100" 
EOU, 
11OB, 
Bommasandra 
Industrial 
Area, 
Hosur 
Road, 
Anekal 
Taluk, 
Bangalore 
562 
158 
(INDlA). 
9 8 7 6 5 4 
(Corrected 
third 
printing, 
1999) 

_____________________________________________
To 
Juri8 

_____________________________________________
Preface 
These 
are 
my 
lecture 
notes 
from 
CS381/481: 
Automata 
and 
Computability 
Theory, 
a 
one-semester 
senior-level 
course 
I 
have 
taught 
at 
Cornell 
versity 
for 
many 
years. 
I 
took 
this 
course 
myself 
in 
thc 
fall 
of 
1974 
as 
a 
first-year 
Ph.D. 
student 
at 
Cornell 
from 
Juris 
Hartmanis 
and 
have 
been 
in 
love 
with 
the 
subject 
ever 
sin,:e. 
The 
course 
is 
required 
for 
computer 
science 
majors 
at 
Cornell. 
It 
exists 
in 
two 
forms: 
CS481, 
an 
honors 
version; 
and 
CS381, 
a 
somewhat 
paced 
version. 
The 
syllabus 
is 
roughly 
the 
same, 
but 
CS481 
go 
es 
deeper 
into 
thc 
subject, 
covers 
more 
material, 
and 
is 
taught 
at 
a 
more 
abstract 
level. 
Students 
are 
encouraged 
to 
start 
off 
in 
one 
or 
the 
other, 
then 
switch 
within 
the 
first 
few 
weeks 
if 
they 
find 
the 
other 
version 
more 
suitaLle 
to 
their 
level 
of 
mathematical 
skill. 
The 
purpose 
of 
t.hc 
course 
is 
twofold: 
to 
introduce 
computer 
science 
students 
to 
the 
rieh 
heritage 
of 
models 
and 
abstractions 
that 
have 
arisen 
over 
the 
years; 
and 
to 
dew!c'p 
the 
capacity 
to 
form 
abstractions 
of 
their 
own 
and 
reason 
in 
terms 
of 
them. 
The 
course 
is 
quite 
mathematical 
in 
fiavor, 
and 
a 
certain 
dt'gree 
of 
vious 
mat.hematical 
expt'r!enrc 
is 
essf'ntial 
for 
survival. 
Stndt'llt:; 
shou
1
<i 
already 
be 
conversant 
with 
elementary 
discrete 
mathematics, 
iIlrIuding 
the 
not 
ions 
of 
set, 
function, 
relation, 
product, 
partial 
order, 
equivalence 
tion, 
graph, 
and 
tree. 
They 
should 
have 
a 
reperi;:Jin' 
of 
basic 
proof 
niqlles 
at 
their 
disposal, 
including 
a 
thorough 
undf'fstanui 
ng 
r,[ 
t 
hc 
principle 
of 
mathematical 
induction. 

_____________________________________________
VIII 
Preface 
The 
material 
covered 
in 
this 
text 
is 
somewhat 
more 
than 
can 
be 
covered 
in 
a 
one-semester 
course. 
It 
is 
also 
a 
mix 
of 
elementary 
and 
advanced 
topics. 
The 
basic 
course 
consists 
of 
the 
lectures 
numbered 
1 
through 
39. 
Additionally, 
I 
have 
included 
several 
supplementary 
lectures 
numbered 
A 
through 
K 
on 
various 
more 
advanced 
topics. 
These 
can· 
be 
included 
or 
omitted 
at 
the 
instructor's 
discretion 
or 
assigned 
as 
extra 
reading. 
They 
appear 
in 
roughly 
the 
order 
in 
which 
they 
should 
be 
covered. 
At 
first 
these 
notes 
were 
meant 
to 
supplement 
and 
not 
supplant 
a 
textbook, 
but 
over 
the 
years 
they 
gradually 
took 
on 
a 
life 
of 
their 
own. 
In 
addition 
to 
the 
notes, 
I 
depended 
on 
various 
texts 
at 
one 
time 
or 
another: 
Cutland 
[30J, 
HarrisQn 
[55], 
Hopcroft 
and 
Ullman 
[60], 
Lewis 
and 
Papadimitriou 
[79], 
Machtey 
and 
Young 
[81], 
and 
Manna 
[82]. 
In 
particular, 
the 
Hopcroft 
and 
Ullman 
text 
was 
the 
standard 
textbook 
for 
the 
course 
for 
many 
years, 
and 
for 
me 
it 
has 
been 
an 
indispensa.ble 
source 
of 
knowledge 
and 
insight. 
All 
of 
these 
texts 
are 
excellent 
references, 
and 
I 
recommend 
them 
highly. 
In 
addition 
to 
the 
lectures, 
I 
have 
included 
12 
homework 
sets 
and 
several 
miscellaneous 
exercises. 
Some 
of 
the 
exercises 
come 
with 
hints 
and/or 
lutions; 
these 
are 
indicated 
by 
the 
annotations 
"H" 
and 
"S," 
respectively. 
In 
addition, 
I 
have 
annotated 
exercises 
with 
zero 
to 
three 
stars 
to 
indicate 
relative 
difficulty. 
I 
have 
stuck 
with 
the 
format 
of 
my 
previous 
textbook 
[72], 
in 
which 
the 
main 
text 
is 
divided 
into 
more 
or 
less 
self-contained 
lectures, 
each 
4 
to 
8 
pages. 
Although 
this 
format 
is 
rather 
unusual 
for 
a 
textbook, 
I 
have 
found 
it 
quite 
successful. 
Many 
readers 
have 
commented 
that 
they 
like 
it 
because 
it 
partitions 
the 
subject 
into 
bite-sized 
chunks 
that 
can 
be 
covered 
more 
or 
less 
independently. 
I 
owe 
a 
supreme 
debt 
of 
gratitude 
to 
my 
wife 
Frances 
for 
her 
constant 
love, 
support, 
and 
superhuman 
patience, 
especially 
during 
the 
finäJ 
throes 
of 
this 
project. 
I 
am 
also 
indebted 
to 
the 
many 
teachers, 
colleagues, 
ing 
assistants, 
and 
students 
who 
over 
the 
years 
have 
shared 
the 
delights 
of 
this 
subject 
with 
me 
and 
from 
whom 
I 
have 
learned 
so 
much. 
I 
would 
especially 
like 
to 
thank 
Rick 
Aaron, 
Arash 
Baratloo, 
Jim 
Baumgartner, 
Steve 
Bloorn, 
Manucl 
Blum, 
Amy 
Briggs, 
Ashok 
Chandra, 
Wilfred 
Chen, 
Allan 
Cheng, 
Francis 
Chu, 
Bob 
Constable, 
Devdatt 
Dubhashi, 
Peter 
van 
Emde 
Boas, 
Allen 
Emerson, 
Andras 
Ferencz, 
Jeff 
Foster,'-Sophia 
giakaki, 
David 
Gries, 
Joe 
Halpern, 
David 
Harel, 
Basil 
Hayek, 
Tom 
zinger, 
John 
Hopcroft, 
Nick 
Howe, 
Doug 
Ierardi, 
Tibor 
Janosi, 
Jim 
nings, 
Shyam 
Kapur, 
Steve 
Kautz, 
Nils 
Klarlund, 
Peter 
Kopke, 
Vladimir 
Kotlyar, 
Alan 
Kwan, 
Georges 
Lauri, 
Michael 
Leventon, 
Jake 
Levirne, 
David 
Liben-Nowell, 
Yvonne 
Lo, 
Steve 
Mahaney, 
Nikolay 
Mateev, 
Frank 
Sherry, 
Albert 
Meyer, 
Bob 
Milnikel, 
Francesmary 
Modugno, 
Anil 
Nerode, 
Damian 
Niwinski. 
David 
de 
la 
Nuez, 
Dan 
Oberlin, 
Jens 
Palsberg, 
Rohit 

_____________________________________________
Preface 
IX 
Parikh, 
David 
Pearson, 
Paul 
Pedersen, 
Vaughan 
Pratt, 
Zulfikar 
Ramzan, 
Jon 
Rosenberger, 
Jonathan 
Rynd, 
Erik 
Schmidt, 
Michael 
Schwartzbach, 
Amitabh 
Shah, 
Frederick 
Smith, 
Kjartan 
Stefansson, 
Colin 
Stirling, 
Larry 
Stockmeyer, 
Aaron 
Stump, 
Jurek 
Tiuryn, 
Alex 
Tsow, 
Moshe 
Vardi, 
Igor 
Walukiewicz, 
Rafael 
Weinstein, 
Jim 
Wen, 
Dan 
Wineman, 
Thomas 
Yan, 
Paul 
Zimmons, 
and 
many 
others 
too 
numerous 
to 
mention. 
Of 
course, 
the 
greatest 
of 
these 
is 
Juris 
Hartmanis, 
whose 
boundless 
enthusiasm 
for 
the 
subject 
is 
the 
ultimate 
source 
of 
my 
own. 
I 
would 
be 
most 
grateful 
für 
suggestions 
and 
criticism 
from 
readers. 
Note 
added 
for 
thc 
third 
pnnting. 
I 
am 
indebted 
to 
Chris 
Jeuell 
for 
pointing 
out 
several 
typographical 
errors, 
wh 
ich 
have 
been 
corrected 
in 
this 
printing. 
Ithaca, 
Ncw 
York 
Dexter 
C. 
Kozcn 

_____________________________________________
Contents 
Preface 
Lectures 
Introduction 
1 
Course 
Roadmap 
and 
Historical 
Perspective 
. 
2 
Strings 
and 
Sets 
............... 
. 
Finite 
Automata 
and 
Regular 
Sets 
3 
Finite 
Automata 
and 
Regular 
Sets 
4 
More 
on 
Regular 
Sets 
...... 
. 
5 
Nondeterministic 
Finite 
Automata 
6 
The 
Subset 
Construction 
..... 
. 
7 
Pattern 
Matching 
. . . . . . . . . . 
8 
Pattern 
Matching 
and 
Regular 
Expressions 
9 
Regular 
Expressions 
and 
Finite 
Automata 
. 
A 
Kleene 
Algebra 
and 
Regular 
Expressions 
. 
10 
Homomorphisms 
. . . . . . . . . 
Limitations 
of 
Finite 
Automata 
. 
12 
Using 
the 
Pumping 
Lemma 
13 
DFA 
State 
Minimization 
.. 
14 
A 
Minimization' 
Algorithm 
. 
15 
Myhill-Nerode 
Relations 
.. 
16 
The 
Myhill-Nerode 
Theorem 
vii 
1 
3 
7 
14 
19 
25 
32 
40 
44 
49 
55 
61 
67 
72 
77 
84 
89 
95 

_____________________________________________
XII 
Contents 
B 
Collapsing 
Nondeterministic 
Automata. 
. . . . . . 
100 
C A 
utomata 
on 
Terms 
. . . . . . . . . . . . . . . . . 
108 
D 
The 
11yhill-Nerode 
Theorem 
for 
Term 
Automata 
. 
114 
17 
Two-
Way 
Finite 
Automata 
119 
18 
2DFAs 
and 
Regular 
Sets. 
. . . . . . . . . . . . . . 
124 
Pushdown 
Automata 
and 
Context-Free 
Languages 
19 
Context-Free 
Grammars 
and 
Languages 
129 
20 
Balanced 
Parentheses 
...... 
135 
21 
Normal 
Forms. 
. . . . . . . . . . 
140 
22 
The 
Pumping 
Lemma 
for 
CFLs 
. 
148 
23 
Pushdown 
Automata. 
. . . . . . 
157 
E 
Final 
State 
Versus 
Empty 
Stack. 
164 
24 
PDAs 
and 
CFGs 
. . . . . . . . . 
167 
25 
Simulating 
NP 
DAs 
by 
CFGs 
.. 
172 
F 
Deterministic 
Pushdown 
Automata 
. 
176 
26 
Parsing 
................ 
181 
27 
The 
Cocke-Kasami-Younger 
Algorithm 
191 
G 
The 
Chomsky-Schützenberger 
Theorem 
198 
H 
Parikh's 
Theorem. 
. . . . . . . . . . 
201 
Turing 
Machines 
and 
Effective 
Computability 
28 
Turing 
Machines 
and 
Effective 
Computability 
206 
29 
More 
on 
Turing 
Machines 
. . . . . . . . 
215 
30 
Equivalent 
Models 
. . . . . . . . . . . . 
221 
31 
Universal 
Machines 
and 
Diagonalization 
228 
32 
Decidable 
and 
Undecidable 
Problems. 
235 
33 
Reduction............... 
239 
34 
Rice's 
Theorem 
. . . . . . . . . . . . 
245 
35 
Undecidable 
Problems 
About 
CFLs 
. 
249 
36 
Other 
Formalisms 
256 
37 
The 
'x-Calculus 
. . . . . 
262 
I 
While 
Programs 
. . . . 
269 
J 
Beyond 
Undecidability 
. 
274 
38 
Gödel's 
Incompleteness 
Theorem 
282 
39 
Proof 
of 
the 
Incompleteness 
Theorem 
287 
K 
Gödel's 
Proof 
. . . . . . . . . . . . . 
.'. 
292 
Exercises 
299 
Homework 
Sets 
Homework 
1 
Homework 
2 
301 
302 

_____________________________________________
Homework 
3 
Homework 
4 
Homework 
5 
Homework 
6 
Homework 
7 
Homework 
8 
Homework 
9 
Homework 
10 
Homework 
11' 
Homework 
12 
Miscellaneous 
Exercises 
Finite 
Automata 
and 
Regular 
Sets 
. . . . . . . . . . 
Pushdown 
Automata 
and 
Context-Free 
Languages 
. 
Turing 
Machines 
and 
Effective 
Computability 
Hints 
and 
Solutions 
Hints 
for 
Selected 
Miscellaneous 
Exercises 
. 
Solutions 
to 
Seiected 
Miscellaneous 
Exercises 
. 
References 
Notation 
and 
Abbreviations 
Index 
Contents 
XIII 
303 
304 
306 
307 
308 
309 
310 
311 
312 
313 
315 
333 
340 
351 
357 
373 
381 
389 

_____________________________________________
Lectures 

_____________________________________________
Lecture 
1 
Course 
Roadmap 
and 
Historical 
Perspective 
The 
goal 
of 
this 
course 
is 
to 
understand 
the 
foundations 
of 
computation. 
We 
will 
ask 
some 
very 
basic 
questions, 
such 
as 
Ł 
What 
does 
it 
mean 
for 
a 
function 
to 
be 
computable? 
Ł 
Are 
there 
any 
honcomputable 
functions? 
Ł 
How 
does 
computational 
power 
depend 
on 
programming 
constructs? 
These 
questions 
may 
appear 
simple, 
but 
they 
are 
not. 
They 
have 
intrigued 
scientists 
for 
decades, 
and 
the 
subject 
is 
still 
far 
from 
closed. 
In 
the 
quest 
for 
answers 
to 
these 
questions, 
we 
will 
encounter 
some 
dainental 
and 
pervasive 
concepts 
along 
the 
way: 
state, 
transition, 
terminism, 
reduction, 
and 
undecidability, 
to 
name 
a 
few. 
Some 
of 
the 
most 
important 
acltievements 
in 
theoretical 
computer 
science 
have 
been 
the 
tallization 
of 
these 
concepts. 
They 
have 
shown 
aremarkable 
persistence, 
even 
as 
technology 
changes 
from 
day 
to 
day. 
They 
are 
crucial 
for 
every 
good 
computer 
scientist 
to 
know, 
so 
that 
they 
can 
be 
recognized 
when 
they 
are 
encountered, 
as 
they 
surely 
will 
be. 
Various 
models 
of 
computation 
have 
been 
proposed 
over 
the 
years, 
all 
of 
which 
capture 
some 
fundamental 
aspect 
of 
computation. 
We 
will 
centrate 
on 
the 
following 
three 
classes 
of 
models, 
in 
order 
of 
increasing 
power: 

_____________________________________________
4 
Lecture 
1 
(i) 
finite 
memory: 
finite 
automata, 
regular 
expressions; 
(ii) 
finite 
memory 
with 
stack: 
pushdown 
automataj 
(iii) 
unrestricted: 
Ł 
Turing 
machines 
(Alan 
Turing 
[120]), 
Ł 
Post 
systems 
(Emil 
Post 
[99, 
100]), 
Ł 
JL-recursive 
functions 
(Kurt 
Gödel 
[51], 
Jacques 
Herbrand), 
Ł 
.A-calculus 
(Alonzo 
Church 
[23), 
Stephen 
C. 
Kleene 
[66)), 
Ł 
combinatory 
logic 
(Moses 
Schönfinkel 
[111], 
Haskell 
B. 
Curry 
[29)). 
These 
systems 
were 
developed 
long 
before 
computers 
existed. 
days 
one 
could 
add 
PASCAL, 
FORTRAN, 
BASIC, 
LISP, 
SCHEME, 
C++, 
JAVA, 
or 
any 
sufficiently 
powerful 
programming 
language 
to 
this 
list. 
In 
parallel 
with 
and. 
independent 
of 
the 
development 
of 
these 
models 
of 
computation, 
the 
linguist 
Noam 
Chomsky 
attempted 
to 
formalize 
the 
tion 
öf 
grammar 
and 
language. 
This 
effort 
resulted 
in 
thc 
definition 
of 
the 
Chomsky 
hierarchy, 
a 
hierarchy 
of 
language 
classes 
defined 
by 
grammars 
of 
increasing 
complexity: 
(i) 
right-linear 
grammarsj 
(ii) 
context-free 
grammarsj 
(Hi) 
unrestricted 
grammars. 
AIthough 
gramm 
ars 
and 
machine 
models 
appear 
quite 
different 
on 
a 
ficial 
level, 
the 
process 
of 
parsing 
a 
sentence 
in 
a 
language 
bears 
a 
strong 
resemblance 
to 
computation. 
Upon 
doser 
inspection, 
it 
turns 
out 
that 
each 
of 
the 
grammar 
types 
(i), 
(ii), 
and 
(iii) 
are 
equivalent 
in 
computational 
power 
to 
the 
machine 
models 
(i), 
(ii), 
and 
(iii) 
above, 
respectively. 
There 
is 
even 
a 
fourth 
natural 
dass 
called 
the 
context-sensitive 
grammars 
and 
languages, 
which 
fits 
in 
between 
(ii) 
and 
(iii) 
and 
which 
corresponds 
to 
a 
certain 
natural 
dass 
of 
machine 
models 
called 
linear 
bounded 
automata. 
It 
is 
quite 
surprising 
that 
a 
naturally 
defined 
hierarchy 
in 
one 
field 
should 
correspond 
so 
closely 
to 
a 
naturally 
defined 
hierarchy 
in 
a 
completely 
ferent 
field. 
Could 
this 
be 
me 
re 
coincidence? 

_____________________________________________
Course 
Roadmap 
and 
Historical 
Perspective 
5 
Abstraction 
The 
machine 
models 
mentioned 
above 
were 
first 
identified 
in 
the 
same 
way 
that 
theories 
in 
physics 
or 
any 
other 
scientific 
discipline 
arise. 
When 
ing 
real-world 
phenomena, 
one 
Iilecomes 
aware 
of 
recurring 
patterns 
and 
themes 
that 
appear 
in 
various 
guises. 
These 
guises 
may 
differ 
substantially 
on 
a 
superficiallevel 
but 
may 
bear 
enough 
resemblance 
to 
one 
another 
to 
suggest 
that 
there 
are 
common 
underlying 
principles 
at 
work. 
When 
this 
happens, 
it 
makes 
sense 
to 
try 
to 
construct 
an 
abstract 
model 
that 
tures 
these 
underlying 
principles 
in 
the 
simplest 
possible 
way, 
devoid 
of 
the 
unimportant 
details 
of 
each 
particular 
manifestation. 
This 
is 
the 
process 
of 
abstraction. 
Abstraction 
is 
the 
essen 
ce 
of 
scientific 
progress, 
because 
it 
focuses 
attention 
on 
the 
important 
principles, 
unencumbered 
by 
irrelevant 
details. 
Perhaps 
the 
most 
striking 
example 
of 
this 
phenomenon 
we 
will 
see 
is 
the 
formalization 
of 
the 
concept 
of 
effective 
computability. 
This 
quest 
started 
around 
the 
beginning 
of 
the 
twentieth 
century 
with 
the 
development 
of 
the 
formalist 
school 
of 
mathematics, 
championed 
by 
the 
philosopher 
Bertrand 
Russell 
and 
the 
mathematician 
David 
Hilbert. 
They 
wanted 
to 
reduce 
all 
of 
mathematics 
to 
the 
formal 
manipulation 
of 
synibols. 
Of 
course, 
the 
formal 
manipulation 
of 
symbols 
is 
a 
form 
of 
computation, 
although 
there 
were 
no 
computers 
around 
at 
the 
time. 
However, 
there 
tainly 
existed 
an 
awareness 
of 
computation 
and 
algorithms. 
cians, 
logicians, 
and 
philosophers 
knew 
a 
constructive 
method 
when 
they 
saw 
it. 
There 
followed 
several 
attempts 
to 
come 
to 
grips 
with 
the 
eral 
notion 
of 
effective 
computability. 
Several 
definitions 
emerged 
(Turing 
machines, 
Post 
systems, 
etc.), 
each 
with 
its 
own 
peculiarities 
and 
differing 
radically 
in 
appearance. 
However, 
it 
turned 
out 
that 
as 
different 
as 
all 
these 
formalisms 
appeared 
to 
be, 
they 
could 
all 
simulate 
one 
another, 
thus 
they 
were 
all 
computationally 
equivalent. 
The 
formalist 
program 
was 
eventually 
shattered 
by 
Kurt 
Gödel's 
pleteness 
theorem, 
which 
states 
that 
no 
matter 
how 
strong 
a 
deductive 
system 
for 
number 
theory 
you 
take, 
it 
will 
always 
be 
possible 
to 
construct 
simple 
statements 
that 
are 
true 
but 
unprovable. 
This 
theorem 
is 
widely 
regarded 
as 
one 
of 
the 
crowning 
intellectual 
achievements 
of 
twentieth 
tury 
mathematics. 
It 
is 
essentially 
a 
statement 
about 
computability, 
and 
we 
will 
be 
in 
a 
position 
to 
give 
a 
full 
account 
of 
it 
by 
the 
end 
of 
the 
course. 
The 
process 
of 
abstraction 
is 
inherently 
mathematical. 
It 
involves 
ing 
models 
that 
capture 
observed 
behavior 
in 
the 
simplest 
possible 
way. 
Although 
we 
will 
consider 
plenty 
of 
concrete 
examples 
and 
applications 
of 
these 
models, 
we 
will 
work 
primarily 
in 
terms 
of 
their 
mathematical 
erties. 
We 
will 
always 
be 
as 
explicit 
as 
possible 
about 
these 
properties. 

_____________________________________________
6 
Lecture 
1 
We 
will 
usually 
start 
with 
definitions, 
then 
subsequently 
reason 
purely 
in 
terms 
of 
those 
definitions. 
For 
some, 
this 
will 
undoubtedly 
be 
a 
new 
way 
of 
thinking, 
but 
it 
is 
a 
skill 
that 
is 
worth 
cultivating. 
Keep 
in 
mind 
that 
a 
large 
intellectual 
effort 
often 
goes 
into 
coming 
up 
with 
just 
the 
right 
definition 
or 
model 
that 
captures 
the 
essen 
ce 
of 
the 
principle 
at 
hand 
with 
the 
least 
amount 
of 
extraneous 
baggage. 
After 
the 
fact, 
the 
reader 
often 
sees 
only 
the 
finished 
product 
and 
is 
not 
exposed 
to 
all 
the 
misguided 
false 
attempts 
and 
pitfalls 
that 
were 
encountered 
along 
the 
way. 
Remember 
that 
it 
took 
many 
years 
of 
intellectual 
struggle 
to 
arrive 
at 
the 
theory 
as 
it 
exists 
today. 
This 
is 
not 
to 
say 
that 
the 
book 
is 
closed-far 
from 
it! 

_____________________________________________
Lecture 
2 
Strings 
and 
Sets 
Decision 
Problems 
Versus 
Functions 
Adecision 
problem 
is 
a 
function 
with 
a 
one-bit 
output: 
"yes" 
or 
"no." 
To 
specify 
adecision 
probleij]., 
one 
must 
specify 
Ł 
the 
set 
A 
of 
possible 
inputs, 
and 
Ł 
the 
subset 
B 
A 
of 
"yes" 
instances. 
For 
example, 
to 
decide 
if 
a 
given 
graph 
is 
connected, 
the 
set 
of 
possible 
inputs 
is 
the 
set 
of 
all 
(encodings 
of) 
graphs, 
and 
the 
"yes" 
instances 
are 
die 
connected 
graphs. 
To 
decide 
if 
a 
given 
number 
is 
a 
prime, 
the 
set 
of 
possible 
inputs 
is 
the 
set 
of 
all 
(binary 
encodings 
of) 
integers, 
and 
the 
"yes" 
instances 
are 
the 
primes. 
In 
this 
course 
we 
will 
mostly. 
consider 
decision 
problems 
as 
opposed 
to 
functions 
with 
more 
general 
outputs. 
We 
do 
this 
for 
mathematical 
simplicity 
and 
because 
the 
behavior 
we 
want 
to 
study 
is 
already 
present 
at 
this 
level. 
Strings 
N 
ow 
to 
our 
first 
abstraction: 
we 
will 
always 
take 
the 
set 
of 
possible 
inputs 
to 
adecision 
problem 
to 
be 
the 
set 
of 
finite-Iength 
strings 
over 
some 
fixed 
finite 

_____________________________________________
8 
Lecture 
2 
Definition 
2.1 
alphabet 
(formal 
definitions 
below). 
We 
do 
this 
for 
uniformity 
and 
ity. 
Other 
types 
of 
data-graphs, 
the 
natural 
numbers 
N 
= 
{O, 
1,2, 
... 
}, 
trees, 
even 
programs-can 
be 
encoded 
naturally 
as 
strings. 
By 
making 
this 
abstraction, 
we 
have 
to 
deal 
with 
only 
one 
data 
type 
and 
a 
few 
basic 
operations. 
Ł 
An 
alphabet 
is 
any 
finite 
set. 
For 
example, 
we 
might 
use 
the 
bet 
{O, 
1, 
2, 
... 
,9} 
if 
we 
are 
talking 
about 
decimal 
numbers; 
the 
set 
of 
all 
ASCII 
characters 
if 
talking 
about 
text; 
{O, 
I} 
if 
talking 
about 
bit 
strings. 
The 
only 
restriction 
is 
that 
the 
alphabet 
he 
finite. 
When 
speaking 
about 
an 
arbitrary 
finite 
alphabet 
abstractly, 
we 
usually 
note 
it 
by 
the 
Greek 
letter 
We 
call 
elements 
of 
letters 
or 
symbols 
and 
denote 
them 
by 
a, 
b, 
c, 
.... 
We 
usually 
do 
not 
care 
at 
all 
about 
the 
nature 
of 
the 
elements 
of 
only 
that 
there 
are 
finitely 
many 
of 
them. 
Ł 
Astring 
over 
is 
any 
finite-Iength 
sequence 
of 
elements 
Example: 
if 
= 
{a, 
b}, 
then 
aabab 
is 
astring 
over 
of 
length 
five. 
We 
use 
x, 
y, 
z, 
... 
to 
refer 
to 
strings. 
Ł 
The 
length 
of 
astring 
x 
is 
the 
number 
of 
symbols 
in 
x. 
The 
length 
of 
x 
is 
denoted 
lxi. 
'For 
example, 
laababl 
= 
5. 
Ł 
There 
is 
a 
unique 
string 
of 
length 
0 
over 
called 
the 
null 
string 
or 
empty 
string 
and 
denoted 
by 
E 
(Greek 
epsilon, 
not 
to 
be 
confused 
with 
the 
symbol 
for 
set 
containment 
E). 
Thus 
lEI 
= 
o. 
Ł 
We 
write 
an 
for 
astring 
of 
a's 
of 
length 
n. 
For 
example, 
a
5 
= 
aaaaa, 
a
1 
= 
a, 
and 
a
O 
= 
E. 
Formally, 
an 
is 
defined 
inductively: 
oder 
a 
= 
E, 
n+l 
der 
n 
a 
= 
a 
a. 
Ł 
The 
set 
of 
all 
strings 
over 
alphabet 
is 
denoted 
For 
example, 
{a, 
b} 
* 
= 
{E, 
a, 
b, 
aa, 
ab, 
ba, 
bb, 
aaa, 
aab, 
.
.. 
}, 
{a} 
* 
= 
{E, 
a, 
aa, 
aaa, 
aaaa, 
... 
} 
= 
{an 
I 
n 
O}. 
By 
convention, 
we 
take 
o 
where 
0 
denotes 
the 
empty 
set. 
This 
may 
seem 
a 
bit 
strange, 
but 
there 
is 
good 
mathematical 
justification 
for 
it, 
which 
will 
become 
apparent 
shortly. 

_____________________________________________
Definition 
2.2 
Strings 
and 
Sets 
9 
If 
is 
nonempty, 
then 
is 
an 
infinite 
set 
of 
finite-Iength 
strings. 
Be 
careful 
not 
to 
confuse 
strings 
and 
sets. 
We 
won't 
see 
any 
infinite 
strings 
until 
much 
later 
in 
the 
course. 
Here 
are 
some 
differences 
between 
strings 
and 
sets: 
Ł 
{a,b} 
= 
{b,a}, 
but 
ab:f:. 
ba; 
Ł 
{a,a,b} 
= 
{a,b}, 
but 
aab:f:. 
ab. 
Note 
also 
that 
0, 
{•}, 
and 
E 
are 
three 
different 
things. 
The 
first 
is 
a 
set 
with 
no 
elements; 
the 
second 
is 
a 
set 
with 
one 
element, 
namely 
E; 
and 
the 
last 
is 
astring, 
not 
a 
set. 
Operations 
on 
Strings 
The 
operation 
of 
concatenation 
takes 
two 
strings 
x 
and 
y 
and 
makes 
a 
new 
string 
xy 
by 
putting 
them 
together 
end 
to 
end. 
The 
strillg 
xy 
is 
called 
the 
concatenation 
of 
x 
and 
y. 
Note 
that 
xy 
and 
yx 
are 
different 
in 
general. 
Here 
are 
some 
useful 
properties 
of 
concatenation. 
Ł 
concatenation 
is 
associative: 
(xy)z 
= 
x(yz); 
Ł. 
the 
null 
string 
E 
is 
an 
identity 
for 
concatenation: 
EX 
= 
XE 
= 
x; 
Ł 
Ixyl 
= 
lxi 
+ 
lyl· 
A 
special 
case 
of 
the 
last 
equation 
is 
a
m 
an 
= 
a
m
+
n 
for 
all 
m, 
n 
O. 
A 
monoid 
is 
any 
algebraic 
structure 
consisting 
of 
a 
set 
with 
an 
associative 
binary 
operatiop. 
and 
an 
identity 
for 
that 
operation. 
By 
our 
definitions 
above, 
the 
set 
with 
string 
concatenation 
as 
the 
binary 
operation 
and 
E 
as 
the 
identity 
is 
a 
monoid. 
We 
will 
see 
some 
other 
examples 
later 
in 
the 
course. 
Ł 
We 
write 
x
n 
for 
the 
string 
obtained 
by 
concatenating 
n 
copies 
of 
x. 
For 
example, 
(aab)5 
= 
aabaabaabaabaab, 
(aab)l 
= 
aab, 
and 
(aab)O 
= 
E. 
Formally, 
x
n 
is 
defined 
inductively: 
o 
def 
X 
= 
E, 
n+l 
def 
n 
X 
= 
X 
x. 
Ł 
If 
a 
and 
x 
E 
we 
write 
#a(x) 
for 
the 
number 
of 
a's 
in 
x. 
For 
example, 
#0(001101001000) 
= 
8 
and 
#1(00000) 
= 
O. 
Ł 
Aprefix 
of 
astring 
x 
is 
an 
initial 
substring 
of 
Xj 
that 
is, 
astring 
y 
for 
which 
there 
exists 
astring 
z 
such 
that 
x 
= 
yz. 
For 
example, 
abaab 
is 
aprefix 
of 
abaababa. 
The 
null 
string 
is 
aprefix 
of 
every 
string, 
and 

_____________________________________________
10 
Lecture 
2 
every 
string 
is 
a 
prefixof 
itself. 
Aprefix 
y 
of 
x 
is 
a 
proper 
prefix 
of 
x 
if 
y 
=f 
fand 
y 
=f 
x. 
0 
Operations 
on 
Sets 
We 
usually 
denote 
sets 
of 
strings 
(subsets 
of 
by 
A, 
B, 
C, 
.
... 
The 
cardinality 
(number 
of 
elements) 
of 
set 
A 
is 
denoted 
lAI. 
The 
empty 
set 
0 
is 
the 
unique 
set 
of 
cardinality 
O. 
Let 
's 
define 
some 
useful 
operations 
on 
sets. 
Some 
of 
these 
you 
have 
probably 
seen 
before, 
some 
probably 
not. 
Ł 
Set 
union: 
Au 
B 
{x 
I 
x 
E 
A 
or 
x 
E 
B}. 
In 
other 
words, 
x 
is 
in 
the 
union 
of 
A 
and 
B 
iff
1 
either 
x 
is 
in 
A 
or· 
x 
is 
in 
B. 
For 
example, 
{a,ab} 
U 
{ab,aab} 
= 
{a,ab,aab}. 
Ł 
Set 
intersection: 
An 
B 
{x 
I 
x 
E 
A 
and 
x 
E 
B}. 
In 
other 
words, 
x 
is 
in 
the 
intersection 
of 
A 
and 
B 
iff 
x 
is 
in 
both 
A 
and 
B. 
Forexample, 
{a,ab} 
n 
{ab,aab} 
=. 
{ab}. 
Ł 
Complement 
in 
{x 
E 
I 
x 
A}. 
For 
example, 
{strings 
in 
of 
even 
length} 
= 
{strings 
in 
of 
odd 
length}. 
U 
nlike 
U 
and 
n, 
the 
definition 
of 
depends 
on 
The 
set 
f'J 
A 
is 
sometimes 
denoted 
-
A 
to 
emphasize 
this 
dependence. 
Ł 
Set 
concatenation: 
AB 
{xy 
I 
x 
E 
A 
and 
y 
E 
B}. 
In 
other 
words, 
z 
is 
in 
AB 
iff 
z 
can 
be 
written 
as 
a 
concatenation 
of 
two 
strings 
x 
and 
y, 
where 
x 
E 
A 
and 
y 
E 
B. 
For 
example, 
{ 
a, 
ab 
H 
b, 
ba} 
= 
{ab, 
aba, 
abb, 
abba}. 
When 
forming 
a 
set 
tion, 
you 
include 
all 
strings 
that 
can 
be 
obtained 
in 
this 
way. 
Note 
that 
AB 
and 
BA 
are 
different 
sets 
in 
general. 
For 
example, 
{b,baHa,ab} 
= 
{ba;bap,baa,baab}. 
1 
iff 
= 
if 
and 
only 
if. 

_____________________________________________
Strings 
and 
Sets 
11 
Ł 
The 
powers 
An 
of 
a 
set 
Aare 
defined 
inductively 
as 
follows: 
AO 
{t}, 
An+1 
AA
n
. 
In 
other 
words, 
An 
is 
formed 
by 
concatenating 
neopies 
of 
A 
together. 
Taking 
AO 
= 
{t} 
makes 
the 
property 
Am+n 
= 
AmAn 
hold, 
even 
when 
one 
of 
m 
or 
n 
is 
O. 
For 
exarnple, 
{ab.aab}O 
= 
{f}, 
{ab,aab}l 
= 
{ab,a,ab}, 
{ab, 
(Lab}2 
= 
{abab, 
abaa"b. 
aabab, 
aabaab}, 
{ab. 
aab}3 
= 
{ababab. 
ababaab, abaabab, aababah, 
abaabaab. 
aababaab, 
aabaabab, 
aabaabaab}. 
Also, 
{a,b}n 
= 
{xE 
{a,b}* 
Ilxl 
= 
n} 
= 
{strings 
over 
{a, 
b} 
of 
length 
n}. 
Ł 
The 
asterate 
A 
* 
of 
a 
set 
A 
is 
the 
union 
of 
all 
finite 
powers 
of 
A: 
A* 
U 
An 
= A
O 
U 
Al 
U 
A
2 
U 
A
3 
U···. 
Another 
way· 
to 
say 
this 
is 
A* 
= 
{XIX2'" 
x
n 
I 
n 
0 
and 
Xi 
E 
A, 
1 
i 
n}. 
Note 
that 
n 
can 
be 
0; 
thus 
the 
null 
string 
E 
is 
in 
A* 
for 
any 
A. 
We 
previously 
defined 
E* 
to 
be 
the 
set 
of 
all 
finite-length 
strings 
over 
the 
alphabet 
E. 
This 
is 
exactly 
the 
asterate 
of 
the 
set 
E, 
so 
our 
notation 
is 
consistent. 
Ł 
We 
define 
A 
+ 
to 
be 
the 
union 
of 
all 
nonzero 
powers 
of 
A: 
AA*= 
U 
An. 
Here 
are 
sorne 
useful 
properties 
of 
these 
set 
operations: 
Ł 
Set 
union, 
set 
intersection, 
and 
set 
concatenation 
are 
associatü't: 
(A 
U 
B) 
U C 
= 
AU 
(B 
U 
C), 
(A 
n 
B) 
n 
C 
= 
An 
(B 
n 
C), 
(AB)C 
= 
A(BC). 

_____________________________________________
12 
Lecture 
2 
Ł 
Set 
union 
and 
set 
intersection 
are 
commu.tative: 
AUB=BuA, 
AnB 
= 
BnA. 
As 
noted 
above, 
set 
concatenation 
is 
not. 
Ł 
The 
null 
set 
0 
is 
an 
identity 
for 
U: 
AU0 
= 
0uA=A. 
Ł 
The 
set 
{f} 
is 
an 
identity 
for 
set 
concatenation: 
Ł 
The 
null 
set 
0 
is 
an 
annihilator 
for 
set 
concatenation: 
A0=0A=0. 
Ł 
Set 
union 
and 
intersection 
distribu.te 
over 
each 
other: 
AU 
(B 
n 
C) 
= 
(A 
U 
B) 
n 
(A 
U 
C), 
An 
(B 
U 
C) 
= 
(A 
n 
B) 
U 
(A 
n 
C). 
Ł 
Set 
concatenation 
distributes 
over 
union: 
A(BUC) 
= 
AB 
U 
AC, 
(AU 
B)C 
= 
ACU 
BC. 
In 
fact, 
concatenation 
distributes 
over 
the 
union 
of 
any 
family 
of 
sets. 
If 
{Bi 
I 
i 
E 
I} 
is 
any 
family 
of 
sets 
indexed 
by 
another 
set 
I, 
finite 
or 
infinite, 
then 
A(U 
Bi) 
= 
U 
ABi, 
iEI 
iEl 
(U 
Bi)A 
= 
U 
BiA. 
iEI 
iEI 
Here 
UiEI 
Bi 
denotes 
the 
union 
of 
all 
the 
sets 
Bi 
for 
i 
E 
I. 
An 
element 
x 
is 
in 
this 
union 
iff 
it 
is 
in 
one 
of 
the 
Bi. 
Set 
concatenation 
does 
not 
distribute 
over 
intersection. 
For 
example, 
take 
A. 
= 
{a,ab}, 
B 
= 
{b}, 
C 
= 
{f}, 
and 
see 
what 
you 
get 
when 
you 
compute 
A(B 
n 
C) 
and 
AB 
n 
AC. 
Ł 
The 
De 
Morgan 
laws 
hold: 
""(AUB)=""AnrvB, 
rv(AnB) 
=rvAUrvB. 

_____________________________________________
Strings 
and 
Sets 
13 
Ł 
The 
asterate 
operation 
* 
satbfies 
the 
following 
properties: 
A*A* 
= 
A*. 
A** 
= 
A*, 
A* 
= 
{ti 
L.J 
.L-t' 
:::: 
{t} 
U 
A*.4, 
0* 
= 
{t}-

_____________________________________________
Lecture 
3 
Finite 
Automata 
and 
Regular 
Sets 
States 
a 
nd 
Tra 
nsitions 
Intuitively, 
astate 
of 
a 
system 
is 
an 
instantaneous 
description 
of 
that 
tem, 
a 
snapshot 
of 
reality 
frozen 
in 
time. 
Astate 
gives 
all 
relevant 
mation 
necessary 
to 
determine 
how 
the 
system 
can 
evolve 
from 
that 
point 
on. 
Transitions 
are 
changes 
of 
state; 
they 
can 
happen 
spontaneously 
or 
in 
response 
to 
external 
inputs. 
We 
assume 
that 
state 
transitions 
are 
instantaneous. 
This 
is 
a 
mathematical 
abstraction. 
In 
reality, 
transitions 
usually 
take 
time. 
Clock 
cycles 
in 
tal 
computers 
enforce 
this 
abstraction 
and 
allow 
us 
to 
treat 
computers 
as 
digital 
instead 
of 
analog 
devices. 
There 
are 
innumerable 
examples 
of 
state 
transition 
systems 
in 
the 
real 
world: 
electronic 
circuits, 
digital 
watches, 
elevators, 
Rubik's 
cube 
(54!/9!6 
states 
and 
12 
transitions, 
!lot 
counting 
peeling 
the 
little 
sticky 
squares 
off), 
the 
game 
of 
Life 
(2
k 
states 
on 
a 
screen 
with 
k 
ceIls, 
one 
transition). 
A 
system 
that 
consists 
of 
only 
finitely 
many 
states 
and 
transitions 
among 
them 
is 
called 
a 
finite-state 
transition 
system. 
We 
model 
these 
abstractly 
by 
a 
mathematical 
model 
called 
a 
finite 
automaton. 

_____________________________________________
Finite 
Automata 
and 
Regular 
Sets 
15 
Finite 
Automata 
.' 
." 
Formally, 
a 
deterministic 
finite 
automaton 
(DFA) 
is 
a 
structure 
M= 
(Q, 
E, 
6, 
s, 
F), 
where 
Ł Q 
is 
a 
finite 
set; 
elements 
of 
Q 
are 
called. 
states; 
Ł E 
is 
a 
finite 
set, 
the 
input 
alphabet; 
Ł 6 : 
Q 
x 
E 
--+ 
Q 
is 
the 
transition 
junction 
(recall 
that 
Q 
x 
E 
is 
the 
set 
of 
ordered 
pairs 
{( 
q, 
a) 
I 
q 
E 
Q 
and 
a 
E 
E}). 
Intuitively, 
6 
is 
a 
function 
that 
teIls 
which 
state 
to 
move 
to 
in 
response 
to 
an 
input: 
if 
M 
is 
in 
state 
q 
and 
sees 
input 
a, 
it 
moves 
to 
state 
6(q,a). 
Ł s 
E 
Q 
is 
the 
start 
state; 
Ł F 
is 
a 
subset 
of 
Q; 
elements 
of 
F 
are 
called 
accept 
or 
final 
states. 
When 
you 
specify 
a 
finite 
automaton, 
you 
must 
give 
all 
five 
parts. 
Automata 
may 
be 
specified 
in 
this 
set-theoretic 
form 
or 
as 
a 
transition 
diagram 
or 
table 
as 
in 
the 
following 
example. 
Example 
3.1 
Here 
is 
an 
example 
of 
a 
simple 
four-state 
finite 
automaton. 
We'll 
take 
the 
set 
of 
states 
to 
be 
{O, 
1,2, 
3}; 
the 
input 
alphabet 
to 
be 
{a, 
b}; 
the 
start 
state 
to 
be 
0; 
the 
set 
of 
accept 
states 
to 
be 
{3}; 
and 
the 
transition 
function 
to 
be 
6(0,a)=1, 
6(1,a) 
= 
2, 
6(2,a) 
= 
6(3,a) 
= 
3, 
6(q,b)=q, 
qE{O,1,2,3}. 
All 
parts 
of 
the 
automaton 
are 
completely 
specified. 
We 
can 
also 
specify 
the 
automaton 
by 
means 
of 
a 
table 
a 
b 
1IT1 
2 3 2 
3F 
3 3 
or 
transition 
The 
final 
states 
are 
indicated 
by 
an 
F 
in 
the 
table 
and 
by 
a 
circle 
in 
the 
transition 
diagram. 
In 
both, 
the 
start. 
state 
is 
indicated 
by 
--+. 
The 
states 
in 

_____________________________________________
16 
Lecture 
3 
the 
transition 
diagram 
from 
left 
to 
right 
correspond 
to 
the 
states 
0,1,2,3 
in 
the 
table. 
One 
advantage 
of 
transition 
diagrams 
is 
that 
you 
don't 
have 
to 
name 
the 
states. 
0 
Another 
convenient 
of 
finite 
automata 
is 
transition 
matricesj 
see 
Miscellaneous 
Exercise 
7. 
Informally, 
here 
is 
how 
a 
finite 
automaton 
operates. 
An 
input 
can 
be 
any 
string 
x 
E 
1::*. 
Put 
a 
pebble 
down 
on 
the 
start 
state 
8. 
Scan 
the 
input 
string 
x 
from 
left 
to 
right, 
one 
symbol 
at 
a 
time, 
moving 
the 
pebble 
according 
to 
0: 
if 
the 
next 
symbol 
of 
x 
is 
band 
the 
pebble 
is 
on 
state 
q, 
move 
the 
pebble 
to 
o(q,b). 
When 
we 
come 
to 
the 
end 
of 
the 
input 
string, 
the 
pebble 
is 
on 
so 
me 
state 
p. 
The 
string 
x 
is 
said 
to 
be 
accepted 
by 
the 
machine 
M 
if 
p 
E 
F 
and 
rejected 
if 
p 
<t. 
F. 
There 
is 
no 
formal 
mechanism 
for 
scanning 
or 
moving 
the 
pebble; 
these 
are 
just 
intuitive 
devices. 
For 
example, 
the 
automaton 
of 
Example 
3.1, 
beginning 
in 
its 
start 
state 
0, 
will 
be 
in 
state 
3 
after 
scanning 
the 
input 
string 
baabbaab, 
so 
that 
string 
is 
accepted; 
however, 
it 
will 
be 
in 
state 
2 
after 
scanning 
the 
string 
babbbab, 
so 
that 
string 
is 
rejected. 
For 
this 
automaton, 
a 
moment.'s 
thought 
reveals 
that 
when 
scanning 
any 
input 
string, 
the 
automaton 
will 
be 
in 
state 
0 
if 
it 
'las 
seen 
no 
a's, 
state 
1 
if 
it 
has 
seen 
one 
a, 
state 
2 
if 
it 
has 
seen 
two 
a's, 
and 
state 
3 
if 
it 
has 
seen 
three 
or 
more 
a's. 
This 
is 
how 
we 
do 
formally 
what. 
w\' 
jnst 
described 
informally 
above. 
We 
first 
define 
a 
function 
'6: 
Q 
x 
-+ 
Q 
from 
0 
by 
induction 
on 
the 
length 
of 
x: 
def 
b(q,t:) 
= 
q, 
def 
o(q,xa) 
= 
b(b(q,x),a). 
(3.1) 
(3.2) 
The 
function 
"6 
maps 
astate 
q 
and 
astring 
x 
to 
a 
new 
state 
"6( 
q, 
x). 
itively, 
'6 
is 
the 
multistep 
version 
of 
6. 
The 
state 
'6(q, 
x) 
is 
the 
state 
M 
ends 
up 
in 
when 
started 
in 
state 
q 
and 
fed 
the 
input 
x, 
moving 
in 
response 
to 
each 
symbol 
of 
x 
according 
to 
6. 
Equation 
(3.1) 
is 
the 
basis 
ofthe 
inductive 
definitionj 
it 
says 
that 
the 
machine 
doesn't 
move 
anywhere 
under 
the 
null 
input. 
Equation 
(3.2) 
is 
the 
induction 
step; 
it 
says 
that 
the 
state 
reachable 
from 
q 
under 
input 
string 
xa 
is 
the 
state 
reachable 
from 
p 
under 
input 
symbol 
a, 
where 
p 
is 
the 
state 
reachable 
from 
q 
under 
input 
string 
x. 
Note 
that. 
the 
second 
argument 
to 
'6 
can 
be 
any 
string 
in 
not 
just 
a 
string 
of 
length 
one 
as 
with 
6j 
but 
6 
and 
6 
agree 
on 
strings 
of 
length 
one: 
'6(q,a) 
= 
'6(q,m) 
since 
a 
= 
•a 
= 
b(b(q,•),a) 
by 
(3.2), 
taking 
x 
= 
• 

_____________________________________________
Finite 
Automata 
and 
Regular 
Sets 
17 
= 
c5(q,a) 
by 
(3.1). 
Formally, 
astring 
X 
is 
said 
to 
be 
accepted 
by 
the 
automaton 
M 
if 
8(s,x) 
E 
F 
and 
rejected 
by 
the 
automaton 
M 
if 
8(s, 
x) 
't 
F, 
where 
s 
is 
the 
start 
state 
and 
F 
is 
the 
set 
of 
accept 
states. 
This 
captures 
formally 
the 
intuitive 
notion 
of 
acceptance 
and 
rejection 
described 
above. 
The 
set 
or 
language 
accepted 
by 
M 
is 
the 
set 
of 
all 
strings 
accepted 
by 
M 
and 
is 
denoted 
L(M): 
def 
* 
L(M) 
= 
{x 
E 
I 
c5(s, 
x) 
E 
F}. 
A 
subset 
A 
is 
said 
to 
be 
regular 
if 
A 
= 
L(M) 
for 
some 
finite 
tomaton 
M. 
The 
set 
of 
strings 
accepted 
by 
the 
automaton 
of 
Example 
3.1 
is 
the 
set 
{x 
E 
{a, 
b} 
* 
I 
x 
contains 
at 
least 
three 
a's}, 
so 
this 
is 
a 
regular set. 
Example 
3.2 
Here 
is 
another 
example 
of 
a 
regular 
set 
and 
a 
finite 
automaton 
accepting 
it. 
Consider 
the 
set 
{xaaay 
I 
x,y 
E 
{a,b}*} 
= 
{x 
E 
{a, 
b} 
* 
I 
x 
contains 
a 
substring 
of 
t 
hree 
consecutive 
a's}. 
For 
example, 
baabaaaab 
is 
in 
the 
set 
and 
should 
be 
accepted, 
whereas 
babbabab 
is 
riot 
in 
the 
set 
and 
should 
be 
rejected 
(because 
the 
three 
a's 
are 
not 
consecutive). 
Here 
is 
an 
automaton 
for 
this 
set, 
specified 
in 
both 
table 
and 
transition 
diagram 
form: 
a 
b 
fIT1 
2 3 0 
3F 
3 3 
o 

_____________________________________________
18 
Lecture 
3 
The 
idea 
he 
re 
is 
that 
you 
use 
the 
states 
to 
count 
the 
number 
of 
consecutive 
a's 
you 
have 
seen. 
If 
you 
haven't 
seen 
three 
a's 
in 
a 
row 
and 
yOt1 
see 
ab, 
you 
must 
go 
back 
to 
the 
start. 
Onee 
you 
have 
seen 
three 
a's 
in 
a 
row, 
though, 
you 
stay 
in 
the 
accept 
state. 

_____________________________________________
Lecture 
4 
More 
on 
Regular 
Sets 
Here 
is 
another 
example 
of 
a 
regular 
set 
that 
is 
a 
little 
harder 
than 
the 
example 
given 
last 
time. 
Consider 
the 
set 
{x 
E 
{O, 
1} 
* 
I 
x 
represents 
a 
multiple 
of 
three 
in 
binary} 
( 
4.1) 
(leading 
zeros 
permitted, 
f 
represents 
the 
number 
0). 
For 
example, 
the 
following 
binar)" 
strings 
represent 
of 
three 
and 
should 
be 
accepted: 
Binary 
Decimal 
equivalent 
0 
0 
11 
i3 
, 
110 
6 
1001 
9 
1100 
12 
1111 
15 
10010 
18 
Strings 
not 
representing 
multiples 
of 
three 
should 
be 
rejected. 
Here 
is 
an 
automaton 
accepting 
the 
set 
(4.1): 
o 
1 
OFIITI 
1 2 0 
2 1 2 

_____________________________________________
20 
Lecture 
4 
The 
states 
0, 
1, 
2 
are 
written 
in 
boldface 
to distinguish 
them 
from 
the 
input 
symbols 
0,1. 
In 
the 
diagram, 
the 
states 
are 
0, 
1, 
2 
from 
left 
to 
right. 
We 
prove 
thatthis 
automaton 
accepts 
exactly 
the 
set 
(4.1) 
by 
induction 
on 
the 
length 
of 
the 
input 
string. 
First 
we 
associate 
a 
meaning 
to 
each 
state: 
if 
the 
number 
represented 
by 
then 
the 
machine 
the 
string 
scanned 
so 
far 
is 
1 
will 
be 
in 
state 
o 
mod 
3 
° 
1 
mod 
3 
1 
2 
mod 
3 
2 
Let 
#x 
denote 
the 
number 
represented 
by 
string 
x 
in 
binary. 
For 
example, 
#f.=0, 
#0 
= 
0, 
#11 
= 
3, 
#100 
= 
4, 
and 
so 
on. 
Formally, 
we 
want 
to 
show 
that 
for 
any 
string 
x 
in 
{O, 
I} 
* , 
6(0, 
x) 
= 
° 
iff 
#x 
== 
0 
mod 
3, 
6(0, 
x) 
= 
1 
iff 
#x 
== 
1 
mod 
3, 
6(0, 
x) 
= 
2 
iff 
#x 
== 
2 
mod 
3, 
or 
in 
short, 
6(0, 
x) 
= 
#x 
mod 
3. 
( 
4.2) 
(4.3) 
This 
will 
be 
our 
induction 
hypothesis. 
The 
final 
result 
we 
want, 
namely 
(4.2), 
is 
a 
weaker 
consequence 
of 
(4.3), 
but 
we 
need 
the 
more 
general 
statement 
(4.3) 
for 
the 
induction 
hypothesis. 
We 
have 
by 
elementary 
nu 
mb 
er 
theory 
that 
#(xO) 
= 
2(#x) 
+ 
0, 
IHere 
a 
mod 
n 
denotes 
the 
remainder 
when 
dividing 
a 
by 
n 
using 
ordinary 
integer 
division. 
We 
also 
write 
a 
== 
b 
mod 
n 
(read: 
a 
is 
congruent 
to 
b 
modulo 
n) 
to 
mean 
that 
a 
and 
b 
have 
the 
same 
remainder 
when 
divided 
by 
n; 
in 
other 
words, 
that 
n 
divides 
b 
-
a. 
Note 
that 
a 
== 
b 
mod 
n 
should 
be 
parsed 
(a 
== 
b) 
mod 
n, 
and 
that 
in 
general 
a 
== 
b 
mod 
n 
and 
a 
= 
b 
mod 
n 
mean 
different 
things. 
For 
example, 
7 
== 
2 
mod 
5 
but 
not 
7 
= 
2 
mod 
5. 

_____________________________________________
More 
on 
Regular 
Sets 
21 
#(x1) 
= 
2(#x) 
+ 
1, 
or 
in 
short, 
#(xc) 
= 
2(#x) 
+ 
c 
( 
4.4) 
for 
c 
E 
{O, 
I}. 
Prom 
the 
machine 
above, 
we 
see 
that 
for 
any 
state 
q 
E 
{O, 
1, 
2} 
and 
input 
symbol 
c 
E 
{a, 
I}, 
c5(q,c) 
= 
(2q+c)mod3. 
(4.5) 
This 
can 
be 
verified 
by 
checking 
all 
six 
cases 
corresponding 
to 
possible 
choices 
of 
q 
and 
c. 
(In 
fact, 
(4.5) 
would 
have 
been 
a 
great 
way 
to 
define 
the 
transition 
function 
formally-then 
we 
wouldn't 
have 
had 
to 
prove 
it!) 
Now 
we 
use 
the 
inductive 
definition 
ef 
8 
to 
show 
(4.3) 
by 
induction 
on 
lxi. 
Basis 
For 
x 
= 
•, 
8(0,•) 
= 
° 
= 
#• 
by 
definition of 
8 
since 
#• 
= 
0 
= 
#• 
mod 
3. 
Induction 
step 
Assuming 
that 
(4.3) 
is 
true 
for 
x 
E 
{a, 
I} 
*., 
we 
show 
that 
it 
is 
true 
for 
xc, 
where 
c E 
{a, 
I}. 
8(0, 
xc) 
= 
c5(8(0,x),c) 
= 
c5(#x 
mod 
3,c) 
= 
(2(#x 
mod 
3) 
+ 
c) 
mod 
3 
= 
(2(#x) 
+ 
c) 
mod 
3 
= 
#xc 
mod 
3 
definition 
of 
8 
induction 
hypothesis 
by 
(4.5) 
elementary 
number 
theory 
by 
(4.4). 
Note 
that 
each 
step 
has 
it:l 
reason. 
We 
used 
the 
definition 
of 
15, 
which 
is 
specific 
to 
this 
automaton; 
the 
definition 
of 
8 
from 
15, 
which 
is 
the 
same 
for 
all 
automata; 
and 
elementary 
properties 
of 
numbers 
and 
strings. 
Some 
Closure 
Properties 
of 
Regular 
Sets 
For 
A, 
B 
recall 
the 
following 
definitions: 
Au 
B 
= 
{x 
I 
x 
E 
A 
or 
xE 
B} 
An 
B 
= 
{x 
I 
x 
E 
A 
and 
x 
E 
B} 
'" 
A 
= 
{x 
E 
I 
x 
rt 
A} 
union 
intersection 
complement 

_____________________________________________
22 
Lecture 
4 
AB 
= 
{xy 
I 
x 
E 
A 
and 
y 
E 
B} 
concatenation 
A* 
= 
{X1:C2··· 
X
n 
I 
n 
0 
and 
Xi 
E 
A, 
1 
$ 
i 
$ 
n} 
= 
AO 
U 
Al 
U 
A
2 
U 
A
3 
U··· 
asterate. 
Do 
not 
confuse 
set 
concatenation 
with 
string 
concatenation. 
Sometimes 
....., 
A 
is 
written 
E* 
-
A. 
We 
show 
below 
that 
if 
A 
and 
B 
are 
regular, 
then 
so 
are 
Au 
B, 
An 
B; 
and 
.....,A. 
We'H 
show 
later 
that 
AB 
and 
A* 
are 
also 
regular. 
The 
Product 
Construction 
Assurne 
that 
A 
and 
B 
are 
regular. 
Then 
there 
are 
automata 
M
1 
= 
(Q1, 
E, 
eh, 
81, 
FI), 
M
2 
= 
(Q2,' 
E, 
8
2
, 
82, 
F
2
) 
with 
L(M
1
) 
= 
A 
and 
L(M
2
) 
= 
B. 
To 
show 
that 
An 
B 
is 
regular, 
we 
will 
build 
an 
automaton 
M
3 
such 
that 
L(M
3
) 
= 
An 
B. 
Intuitively, 
M
3 
will 
have 
the 
states 
of 
M
1 
and 
M2 
encoded 
somehow 
in 
its 
states. 
On 
input 
X 
E 
E*, 
it 
will 
simulate 
M
1 
and 
M2 
simultaneously 
on 
x, 
accepting 
iff 
both 
M
1 
and 
M
2 
would 
accept. 
Think 
about 
putting 
pebble 
down 
on 
the 
start 
state 
of 
M
1 
and 
another 
bn 
the 
start 
state 
of 
M2. 
As 
the 
input 
come 
in, 
move 
both 
pebbles 
according 
to 
the 
rules 
of 
each 
machine. 
Accept 
if 
both 
pebbles 
occupy 
accept 
states 
in 
their 
respective 
machines 
when 
the 
end 
of 
the 
input 
string 
is 
reached. 
FormaHy, 
let 
M
s 
= 
(Q3' 
E, 
03, 
8a, 
F
3
), 
where 
Q3 
= 
Q1 
X 
Q2 
= 
{(p,q) 
I 
p 
E 
Q1 
and 
q 
E 
Q2}, 
F
3 
= 
F
1 
X 
F2 
= 
{(p,q) 
I 
p 
E 
F1 
and 
q 
E 
F
2
}, 
83 
= 
(81,82), 
and 
let 
83 
: 
Q3 
x 
E 
--+ 
Q3 
be 
the 
transition 
function 
defined 
by 
8
3
((p,q),a) 
= 
(81(p,a),82(q,a)). 
The 
automaton 
M3 
is 
caHed 
the 
product 
of 
Ml 
and 
M2. 
Astate 
(p, 
q) 
of 
M
3 
encodes 
a 
configuration 
of 
pebbles 
on 
M
1 
and 
M
2
Ł 

_____________________________________________
More 
on 
Regular 
Sets 
23 
Recall 
the 
inductive 
definition 
(3.1) 
and 
(3.2) 
of 
the 
extended 
transition 
function 
8 
from 
Lecture 
2. 
Applied 
to 
63, 
this 
gives 
8
3
((p,q),•) 
= 
(p,q), 
6
3
( 
(p, 
q), 
xa) 
= 
03(6
3 
((p, 
q), 
x), 
a). 
Lemma 
4.1 
For 
alt 
x 
E 
2;*, 
8
3
((p,q),x) 
= 
(8
1
(p,X),82(q,X)). 
Proof. 
By 
induction 
on 
lxi. 
Basis 
For 
x 
= 
•, 
8
3
((p,q),•) 
= 
(p,q) 
= 
(8
1
(p,•),82(q,C)). 
Induction 
step 
Assuming 
the 
lemma 
holds 
for 
x 
E 
2;*, 
we 
show 
that 
it 
holds 
for 
xa, 
where 
a 
E 
2;. 
6s( 
(p, 
q), 
xa) 
= 
6
3
(8
3
((p,q),x),a) 
= 
= 
= 
Theorem 
4.2 
L(Af
3
) 
= 
L(Mt} 
n 
L(M
2
). 
Proof. 
For 
all 
x 
E 
2;*, 
xE 
L(M
3
) 
definition 
of 
83 
induction 
hypothesis 
definition 
of 
03 
definition 
of 
8
1 
and 
6
2
. 
8
3 
(83, 
x) 
E 
F
3 
definition 
of 
acceptance 
6s((Sl,82),X) 
E 
F
1 
x 
F
2 
definition 
of 
83 
and 
F
3 
(8
1
(Sl,X),8
2
(S2'X)) 
E 
F
1 
x 
F
2 
Lemma 
4.1 
8
1
(Sl,X) 
E 
F
1 
and 
8
2
(S2,X) 
E 
F
2 
definition 
ofset 
product 
x 
E 
L(Mt} 
and 
x 
E 
L(M
2
) 
definition 
of 
acceptance 
o 
x 
E 
L(Mr) 
n 
L(M
2
) 
definition 
of 
intersection. 
0 
To 
show 
that 
regular 
sets 
are 
closed 
under 
complement, 
take 
a 
istic 
automaton 
accepting 
A 
and 
interchange 
the 
set 
of 
accept 
and 
ce 
pt 
states. 
The 
resulting 
automaton 
accepts 
exactly 
when 
the 
original 
automaton 
would 
reject, 
so 
the 
set 
accepted 
is 
,...., 
A. 

_____________________________________________
24 
Lecture 
4 
Onee 
we 
know 
regular 
sets 
are 
closed 
under 
n 
and 
"", 
it 
follows 
that 
they 
are 
closed 
under 
U 
by 
one 
of 
the 
De 
Morgan 
laws: 
AUB 
= 
""(",,An 
""B). 
If 
you 
use 
the 
eonstructions 
for 
n 
and 
"" 
given,above, 
this 
gives 
an 
ton 
for 
Au 
B 
that 
looks 
exaetly 
like 
the 
produet 
automaton 
for 
An 
B, 
exeept 
that 
the 
aeeept 
states 
are 
F
3 
= 
{(p,q) 
I 
p 
E 
Flor 
q 
E 
f2} 
= 
(F
I 
X 
Q2) 
U 
(QI 
X 
F
2
) 
instead 
of 
F
I 
x 
F
2
Ł 
Historical 
Notes 
Finite-state 
transition 
systems 
were 
introdueed 
by 
MeCulloeh 
and 
Pitts 
in 
1943 
[84]. 
Deterministic 
finite 
automata 
in 
the 
form 
presented 
here 
were 
studied 
by 
Kleene 
[70]. 
Our 
notation 
is 
borrowed 
from 
Hoperoft 
and 
Ullman 
[60]. 

_____________________________________________
Lecture 
5 
Nondeterministic 
Finite 
Automata 
Nondeterm 
i n 
ism 
Nondeterminism 
is 
an 
important 
abstraction 
in 
computer 
science. 
It 
refers 
to 
situations 
in 
wh 
ich 
the 
next 
state 
of 
a 
computation 
is 
not 
uniquely 
determined 
by 
the 
current 
state. 
Nondeterminism 
arises 
in 
real 
life 
when 
there 
is 
incomp1ete 
information 
ab 
out 
the 
state 
or 
when 
there 
are 
external 
forces 
at 
work 
that 
can 
affect 
the 
course 
of 
a 
computation. 
For 
example, 
the 
behavior 
of 
a 
process 
in 
a 
distributed 
system 
might 
depend 
on 
messages 
from 
other 
processes 
that 
arrive 
at 
unpredictable 
times 
wit;h 
unpredictable 
contents. 
Nondeterminism 
is 
also 
important 
in 
the 
design 
of 
efficient 
algorithms. 
There 
are 
many 
instances 
of 
important 
combinatorial 
problems 
with 
ficient 
nondeterministic 
solutions 
but 
no 
known 
efficient 
deterministic 
lution. 
The 
famous 
P 
= 
NP 
problem-whether 
all 
problems 
solvable 
in 
nondeterministic 
polynomial 
time 
can 
be 
solved 
in 
deterministic 
mial 
time-is 
a 
major 
open 
problem 
in 
computer 
science 
and 
arguably 
one 
of 
the 
most 
important 
open 
problems 
in 
all 
of 
mathematics. 
In 
nondeterministic 
situations, 
we 
may 
not 
know 
how 
a 
computation 
will 
evolve, 
but 
we 
may 
have 
some 
idea 
of 
the 
range 
of 
possibi
1
:·;c.s. 
This 
is 
modeled 
formally 
by 
allowing 
automata 
to 
have 
multiple-valued 
transition 
fUnctions. 

_____________________________________________
26 
Lecture 
5 
In 
this 
lecture 
and 
the 
next, 
we 
will 
show 
how 
nondeterminism 
is 
porated 
naturally 
in 
the 
context 
of 
finite 
automata. 
One 
might 
think 
that 
adding 
nondeterminism 
might 
increase 
expressive 
power, 
but 
in 
fact 
for 
finite 
automata 
it 
does 
not: 
in 
terms 
of 
the 
accepted, 
tic 
finite 
automata 
are 
no 
more 
powerful 
than 
deterministic 
ones. 
Inother 
words, 
for 
every 
nondeterministic 
finite 
automaton, 
ihere 
is 
a 
deterministic 
one 
accepting 
the 
same 
set. 
However, 
nondeterministic 
machines 
may 
be 
exponentially 
more 
succinct. 
Nondeterministic 
Finite 
Automata 
A 
nondeterministie 
finite 
automaton 
(NFA) 
is 
one 
for 
which 
the 
next 
state 
is 
not 
necessarily 
uniquely 
determined 
by 
the 
current 
state 
and 
input 
bol. 
In 
a 
deterministic 
automaton, 
there 
is 
exactly 
one 
start 
state 
and 
exactly 
one 
transition 
out 
of 
each 
state 
for 
each 
symbol 
in 
L 
In 
a 
terministic 
automaton, 
there 
may 
be 
one, 
more 
than 
one, 
or 
zero. 
The 
set 
of 
possible 
next 
states 
that 
the 
automaton 
may 
move 
to 
from 
a 
particular 
state 
q 
in 
response 
to 
a 
particular 
input 
symbol 
a 
is 
part 
of 
the 
tion 
of 
the 
automaton, 
but 
there 
is 
no 
mechanism 
for 
deciding 
which 
one 
will 
actually 
be 
taken. 
Formally. 
we 
won't 
be 
able 
to 
represent 
this 
with 
a 
function 
fJ 
: 
Q 
x 
1: 
--+ 
Q 
anymore; 
we 
will 
have 
to 
use 
something 
more 
general. 
Also, 
a 
nondeterministic 
automaton 
may 
have 
many 
start 
states 
and 
may 
start 
in 
any 
one 
of 
them. 
Informally, 
a 
nondeterministic 
automaton 
is 
said 
to 
accept 
its 
input 
x 
if 
it 
is 
possible 
to 
start 
in 
some 
start 
state 
and 
scan 
x, 
moving 
according 
to 
the 
transition 
rules 
and 
making 
choices 
along 
the 
way 
whenever 
the 
next 
state 
is 
not 
uniquely 
determined, 
such 
that 
when 
the 
end 
of 
x 
is 
reached, 
the 
machine 
is 
in 
an 
accept 
state. 
Because 
the 
start 
state 
is 
not 
determined 
and 
because 
of 
the 
choices 
along 
the 
way, 
there 
may 
be 
several 
possible 
paths 
through 
the 
automaton 
in 
response 
to 
the 
input 
x; 
some 
may 
lead 
to 
accept 
states 
while 
others 
may 
lead 
to 
reject 
states. 
The 
automat 
on 
is 
said 
to 
aeeept 
x 
if 
at 
least 
one 
computation 
path 
on 
input 
x 
starting 
from 
at 
least 
one 
start 
state 
leads 
to 
an 
accept 
state. 
The 
automaton 
is 
said 
to 
rejeet 
x 
if 
no 
computation 
path 
on 
input 
x 
from 
any 
start 
state 
leads 
to 
an 
accept 
state. 
Another 
way 
of 
saying 
this 
is 
that 
x 
is 
accepted 
iff 
there 
exists 
a 
path 
with 
label 
x 
from 
some 
start 
state 
to 
some 
accept 
state. 
Again, 
there 
is 
no 
mechanism 
for 
determining 
which 
state 
to 
start 
in 
or 
which 
of 
the 
possible 
next 
möves 
to 
take 
in 
response 
to 
an 
input 
symbol. 
It 
is 
helpful 
to 
think 
about 
this 
process 
in 
terms 
of 
guessing 
and 
verifying. 
On 
a 
given 
input, 
imagine 
the 
automaton 
guessing 
a 
successful 
computation 
or 
proof 
that 
the 
input 
is 
a 
"yes" 
instance 
of 
the 
decision 
problem, 
then 
verifying 
that 
its 
guess 
was 
indeed 
correct. 

_____________________________________________
Nondeterministic 
Finite 
Automata 
27 
For 
example, 
consider 
the 
set 
A 
= 
{x 
E 
{O, 
I} 
* 
I 
the 
fifth 
symbol 
from 
the 
right 
is 
I}. 
Thus 
11010010 
E' 
A 
but 
11000010 
tI. 
A. 
Here 
is 
a 
six-state 
nondeterministic 
automaton 
accepting 
A: 
o 
1 
Öl.. 
0, 
1.. 
0, 
1 
ŁŁ 
0, 
1.. 
0, 
1 
o@ 
There 
is 
only 
one 
start 
state, 
namely 
the 
leftmost, 
and 
only 
one 
accept 
state, 
namely 
the 
rightmost. 
The 
automaton 
is 
not 
deterministic, 
because 
there 
are 
two 
transitions 
from 
the 
leftmost 
state 
labeled 
1 
(one 
ba.ck 
to 
itself 
and 
one 
to 
the 
second 
state) 
and 
no 
transitions 
from 
the 
rightmost 
state. 
This 
automaton 
accepts 
the 
set 
A, 
because 
for 
any 
string 
x 
whose 
fifth 
symbol 
from 
the 
right 
is 
1, 
there 
exists 
a 
sequence 
of 
legal 
transitions 
leading 
from 
the 
start 
state 
to 
the 
a.ccept 
state 
(it 
moves 
from 
the 
first 
state 
to 
the 
second 
when 
it 
scans 
the 
fifth 
symbol 
from 
theright)j 
and 
for 
any 
string 
x 
whose 
fifth 
symbol 
from 
the 
right 
is 
0, 
there 
is 
no 
possible 
sequence 
of 
legal 
transitions 
leading 
to 
the 
accept 
state, 
no 
matter 
what 
choices 
it 
makes 
(recall 
that 
to 
a.ccept, 
the 
ma.chine 
must 
be 
in 
an 
accept 
state 
when 
the 
end 
of 
the 
input 
string 
is 
reached). 
Intuitively, 
we 
can 
think 
of 
the 
machine 
in 
the 
leftmost 
state 
as 
gue6ling, 
every 
time 
it 
sees 
a 
1, 
whether 
that 
1 
is 
the 
fifth 
letter 
from 
the 
right. 
It 
might 
be 
and 
it 
might 
not 
be-the 
machine 
doesn't 
know, 
and 
there 
is 
no 
way 
for 
it 
to 
tell 
at 
that 
point. 
If 
it 
guesses 
that 
it 
is 
not, 
then 
it 
goes 
around 
the 
loop 
again. 
If 
it 
guesses 
that 
it 
is, 
then 
it 
commits 
to 
that 
guess 
by 
moving 
to 
the 
second 
state, 
an 
irrevocable 
decision. 
Now 
it 
must 
tJerihl 
that 
its 
guess 
was 
correctj 
this 
is 
the 
purpose 
of 
the 
tail 
of 
the 
automaton 
leading 
to 
the 
a.ccept 
state. 
If 
the 
1 
that 
it 
guessed 
was 
fifth 
!rom 
the 
right 
really 
is 
fifth 
from 
the 
right, 
then 
the 
machine 
will 
be 
in 
its 
a.ccept 
state 
exactly 
when 
it 
comes 
to 
the 
end 
of 
the 
input 
string, 
therefore 
it 
will 
a.ccept 
the 
string. 
If 
not, 
then 
maybe 
the 
symbol 
fifth 
from 
the 
right 
is 
a 
0, 
and 
no 
guess 
wo'!ld 
have 
workedj 
or 
maybe 
the 
symbol 
fifth 
from 
the 
right 
was 
a 
1, 
but 
the 
ma.chine 
just 
guessed 
the 
wrong 
1. 
Note, 
however, 
that 
for 
any 
string 
x 
E 
A 
(that 
is, 
for 
any 
string 
with 
a 
1 
fifth 
from 
the 
right) 
, 
there 
is 
a 
lucky 
guess 
that 
leads 
to 
a.cceptancej 
whereas 
for 
any 
string 
:z: 
ft 
A 
(that 
is, 
for 
any 
string 
with 
a 
° 
flfth 
from 
the 
right), 
no 
guess 
can 
possibly 
lead 
to 
acceptance, 
no 
matter 
how 
lucky 
the 
automaton 
iso 
In 
general, 
to 
show 
that 
a 
nondeterministic 
ma.chine 
a.ccepts 
a 
set 
B, 
we 
must 
argue 
that 
for 
any 
string 
x 
E 
B, 
there 
is 
a 
lucky 
sequence 
of 
guesses 
that 
leads 
from 
astart 
state 
to 
an 
a.ccept 
state 
when 
the 
end 
of 
x 
is 
reachedj 

_____________________________________________
28 
Lecture-
5 
but 
for 
any 
string 
x 
rt 
B, 
no 
sequence 
of 
guesses 
leads 
to 
an 
accept 
state 
when 
the 
end 
of 
x 
is 
reached, 
no 
matter 
how 
lucky 
the 
automaton 
iso 
Keep 
in 
mind 
that 
this 
process 
of 
guessing 
and 
verifying 
is 
just 
an 
intuitive 
aid. 
The 
formal 
definition 
of 
nondeterministic 
acceptance 
will 
be 
given 
in 
Lecture 
6. 
There 
does 
exist 
a-
deterministic 
automaton 
accepting 
the 
set 
A, 
but 
any 
such 
automaton 
must 
have 
at 
least 
2
5 
= 
32 
states, 
since 
a 
deterministic 
machine 
essentially 
has 
to 
remember 
the 
last 
five 
symbols 
seen. 
The 
Subset 
Construction 
We 
will 
prove 
&. 
rather 
remarkable 
fact: 
in 
terms 
of 
the 
sets 
accepted, 
terministic 
finite 
automata 
are 
no 
more 
powerful 
than 
deterministic 
ones. 
In 
other 
words, 
for 
every 
nondeterministic 
finite 
automaton, 
there 
is 
a 
ministic 
one 
accepting 
the 
same 
set. 
The 
deterministic 
automaton, 
however, 
may 
require 
more 
states. 
This 
theorem 
can 
be 
proved 
using 
the 
subset 
construction. 
Here 
is 
the 
itive 
idea; 
we 
will 
give 
a 
formal 
treatment 
in 
Lecture 
6. 
Given 
a 
ministic 
machine 
N, 
think 
of 
putting 
pebbles 
on 
the 
states 
to 
keep 
track 
of 
all 
the 
states 
N 
could 
possibly 
be 
in 
after 
scanning 
aprefix 
of 
the 
input. 
We 
start 
with 
pebbles 
on 
all 
the 
start 
states 
of 
the 
nondeterministic 
machine. 
Say 
after 
scanning 
some 
prefix 
y 
of 
the 
input 
string, 
we 
have 
pebbles 
on 
some 
set 
P 
of 
states, 
and 
say 
P 
is 
the 
set 
of 
all 
states 
N 
could 
possibly 
be 
in 
after 
scanning 
y, 
depending 
on 
the 
nondeterministic 
choices 
that 
N 
could 
have 
made 
so 
far. 
If 
input 
symbol 
b 
comes 
in, 
pick 
the 
pebbles 
up 
off 
the 
states 
of 
P 
and 
put 
a 
pebble 
down 
on 
each 
state 
reachable 
from 
a 
state 
in 
P 
under 
input 
symbol 
b. 
Let 
P' 
be 
the 
new 
set 
of 
states 
covered 
by 
pebbles. 
Then 
P' 
is 
the 
set 
of 
states 
that 
N 
could 
possibly 
be 
in 
after 
scanning 
yb. 
Although 
for 
astate 
q 
of 
N, 
there 
may 
be 
many 
possible 
next 
states 
after 
scanning 
b, 
note 
that 
the 
set 
P' 
is 
uniquely 
determined 
by 
band 
the 
set 
P. 
We 
will 
thus 
build 
a 
deterministic 
automaton 
M 
whose 
states 
are 
these 
sets. 
That 
is, 
astate 
of 
M 
will 
be 
a 
set 
of 
states 
of 
N. 
The 
start 
state 
of 
M 
will 
be 
the 
set 
of 
start 
states 
of 
N, 
indicating 
that 
we 
start 
with 
one 
pebble 
on 
each 
of 
the 
start 
states 
of 
N. 
A 
final 
state 
of 
M 
will 
be 
any 
set 
P 
containing 
a 
final 
state 
of 
N, 
since 
we 
want 
to 
accept 
x 
if 
it 
is 
possible 
for 
N 
to 
have 
made 
choices 
while 
scanning 
x 
that 
lead 
to 
an 
accept 
state 
of 
N. 
It 
takes 
a 
stretch 
of 
the 
imagination 
to 
regard 
a 
set 
of 
states 
of 
Nasa 
single 
state 
of 
M. 
Let's 
illustrate 
the 
construction 
with 
a 
shortened 
version 
of 
the 
example 
above. 

_____________________________________________
Nondeterministic 
Finite 
Automata 
29. 
Example 
5.1 
Consider 
the 
set 
A 
= 
{x 
E 
{O,l}* 
I 
the 
second 
symbol 
from 
the 
right 
is 
1}. 
o 
1 
O 
1 
0,1· 
1:\ 
-
...
.. 
p q 
r 
Label 
the 
states 
p, 
q, 
r 
from 
left 
to 
right, 
as 
illustrated. 
The 
states 
of 
M 
will 
be 
subsets 
of 
the 
set 
of 
states 
of 
N. 
In 
this 
example 
there 
are 
eight 
such 
subsets: 
0, 
{p}, {q}, 
{r}, 
{p,q}, 
{p,r}, 
{q,r}, 
{p,q,r}. 
Here 
is 
the 
deterministic 
automaton 
M: 
0 
1 
0 0 
0 
--+ 
{p} 
{p} 
{p,q} 
{q} 
{r} 
{r} 
{r}F 
0 0 
{p,q} 
{p,r} 
{p,q,r} 
{p,r}F 
{p} 
{p,q} 
{q,r}F 
{r} 
{r} 
{p,q,r}F 
{p,r} 
{p,q,r} 
For 
example, 
if 
we 
have 
pebbles 
on 
p 
and 
q 
(the 
fifth 
row 
of 
the 
table), 
and 
if 
we 
see 
input 
symbol 
0 
(first 
column), 
then 
in 
the 
next 
step 
there 
will 
be 
pebbles 
on 
p 
and 
r. 
This 
is 
because 
in 
the 
automaton 
N, 
p 
is 
reachable 
from 
p 
under 
input 
0 
and 
r 
is 
reachable 
from 
q 
under 
input 
0, 
and 
these 
are 
the 
only 
states 
reachable 
from 
p 
and 
q 
under 
input 
O. 
The 
accept 
states 
of 
M 
(marked 
F 
in 
the 
table) 
are 
those 
sets 
containing 
an 
accept 
state 
of 
N, 
The 
start 
state 
of 
M 
is 
{p}, 
the 
set 
of 
all 
start 
states 
of 
N. 
Following 
0 
and 
1 
transitions 
from 
the 
start 
state 
{p} 
of 
M, 
one 
can 
see 
that 
states 
{q, 
r 
}, 
{q}, 
{r}, 
0 
of 
M 
can 
never 
be 
reached. 
These 
states 
of 
Mare 
inaccessible, 
and 
we 
might 
as 
weIl 
throw 
them 
out. 
This 
leaves 
o 
1 
--+ 
{p} 
p 
{p,q} 
{p,r} 
{p, 
r}F 
{p} 
{p,q,r}F 
{p,r} 
This 
four-state 
automaton 
is 
exactly 
the 
one 
you 
would 
have 
come 
up 
with 
if 
you 
had 
buHt 
a 
deterministic 
automaton 
direct1y 
to 
remember 
the 
last 
two 
bits 
seen 
and accept 
if 
the 
next-to-Iast 
bit 
is 
a 
1: 

_____________________________________________
30 
Lecture 
5 
1 
[01] 
1 
'0<> 
0 
[10J 
0 
Here 
the 
state 
labels 
[be] 
indieate 
the 
last 
twb 
bits 
seen 
(for 
our 
purposes 
the 
null 
string 
is 
as 
good 
as 
having 
just 
seen 
two 
O's). 
Note 
that 
these 
two 
automata 
are 
isomorphie 
(Le., 
they 
are 
the 
same 
automaton 
up 
to 
the 
renaming 
of 
states): 
{p} 
[00], 
{p,q} 
[01], 
{p, 
T} 
[10], 
{p, 
q, 
T} 
[11]. 
o 
Example 
5.2 
Consider 
the 
set 
{x 
E 
{a}* 
Ilxl 
is 
divisible 
by 
3 
or 
5}. 
(5.1) 
Here 
is 
an 
eight-state 
nondeterministie 
automaton 
N 
with 
two 
start 
states 
accepting 
this 
set 
(labels 
a 
on 
transitions 
are 
omitted 
since 
there 
is 
only 
one 
input 
symbol). 
1\ 
5 8 
\ I 
2--...3 
6--+-7 
The 
only 
nondeterminism 
is 
in 
the 
choiee 
of 
start 
state. 
The 
machine 
guesses 
at 
the 
out 
set 
whether 
to 
check 
for 
divisibility 
by 
3 
or· 
5. 
After 
that, 
the 
computation 
is 
deterministic. 
Let 
Q 
be 
the 
states 
of 
N. 
We 
will 
build 
a 
deterministic 
macl\ine 
M 
whose 
states 
are 
subsets 
of 
Q. 
There 
are 
2
8 
= 
256 
of 
these 
in 
all,hut 
most 
will 
be 
inaccessible 
(not 
reachable 
from 
the 
start 
state 
of 
M 
under 
any 
input). 
Think 
about 
moving 
pebbles-for 
this 
particular 
automaton, 
if 
you 
start 
with 
pebbles 
on 
the 
start 
states 
and 
move 
pebbles 
to 
mark 
all 
states 
the 
machine 
could 
possibly 
be 
in, 
you 
always 
have 
exactly 
two 
pebbles 
on 
N. 
This 
says 
that 
only 
subsets 
of 
Q 
with 
two 
elements 
will 
be 
accessible 
as 
states 
of 
M. 
The 
subset 
construction 
gives 
the 
following 
deterministic 
automaton 
M 
with 
15 
accessible 
states: 

_____________________________________________
Nondeterministic 
Finite 
Automata 
{3, 
8} 
--
{2, 
7} 
{3, 
5} 
{3, 
7} 
-+ 
{2, 
6} 
31 
o 
In 
the 
next 
lecture 
we 
will 
give 
a 
formal 
definition 
of 
nondeterministic 
finite 
automata 
and 
a 
general 
account 
of 
the 
subset 
construction. 

_____________________________________________
Lecture 
6 
The 
Subset 
Construction 
Formal 
Definition 
of 
Nondeterministic 
Finite 
Automata 
A 
nondeterministic 
finite 
automaton 
(NFA) 
is 
a 
five-tuple 
N 
= 
(Q, 
E, 
ß, 
5, 
F), 
where 
everything 
is 
the 
same 
as 
in 
a 
deterministic 
automaton, 
except 
for 
the 
following 
two 
differences. 
Ł 5 
is 
a 
set 
of 
states, 
that 
is, 
S 
Q, 
instead 
of 
a 
single 
state. 
The 
elements 
of 
S 
are 
called 
start 
states. 
Ł 
t:.. 
is 
a 
function 
t:..: 
Q 
x 
E -
2
Q
, 
where 
2
Q 
denotes 
the 
power 
set 
of 
Q 
or 
the 
set 
of 
all 
subsets 
of 
Q: 
2
Q 
{A 
I 
A 
Q}. 
Intuitively, 
t:..(p, 
a) 
gives 
the 
set 
of 
all 
states 
that 
N 
is 
allowed 
to 
move 
to 
from 
p 
in 
one 
step 
under 
input 
symbol 
a. 
We 
often 
write 
a 
p--q 

_____________________________________________
The 
Subset 
Construction 
33 
if 
q 
E 
ß(p, 
a). 
Thc 
set 
ß(p, 
a) 
can 
bc 
the 
empty 
set 
0. 
The 
function 
ß 
is 
caBed 
the 
transition 
function. 
Now 
we 
define 
acceptance 
for 
NFAs. 
The 
function 
ß 
extends 
in 
a 
natural 
way 
by 
induction 
to 
a 
function 
: 2
Q 
x 
E* 
-+ 
2
Q 
according 
to 
the 
rules 
def 
ß(A,•) 
= 
A, 
def 
U 
ß(A,xa) 
= 
ß(q,a). 
(6.1) 
(6.2) 
Intuitively, 
for 
A 
Q 
and 
XE 
E*, 
x) 
is 
the 
set.of 
aB 
states 
reaehable 
under 
input 
string 
x 
from 
some 
state 
in 
A. 
Note 
that 
ß 
takes 
a 
single 
state 
as 
its 
first 
argument 
and 
a 
single 
symbol 
as 
its 
second 
argument, 
whereas 
takes 
a 
set 
of 
states 
as 
its 
first 
argument 
and 
astring 
of 
symbols 
as 
its 
second 
argument. 
Equation 
(6.1) 
says 
that 
the 
set 
of 
aB 
states 
reaehable 
from 
astate 
in 
A 
under 
the 
null 
input 
is 
just 
A. 
In 
(6.2), 
the 
notation 
on 
the 
right-hand 
side 
means 
the 
union 
of 
all 
the 
sets 
ß(q,a) 
for 
q 
E 
in 
other 
words, 
T 
E 
if 
there 
exists 
q 
E 
such 
that 
TE 
ß(q,a). 
x 
a 
p 
-----------------
... 
q 
_ 
T 
Thus 
q 
E 
if 
N 
ean 
move 
from 
some 
state 
pE 
A 
to 
state 
q 
under 
input 
x. 
This 
is 
the 
nondeterministie 
analog 
of 
the 
construction 
of 
6 
for 
deterministic 
automata 
we 
have 
already 
seen. 
Note 
that 
for 
a 
E 
E, 
= 
U 
ß(p,a) 
PEA(A,<) 
= 
U 
ß(p,a). 
pEA 
The 
automaton 
N 
is 
said 
to 
accept 
x 
E 
E* 
if 
x) 
n 
F 
1'H.,. 
In 
other 
words, 
N 
accepts 
x 
if 
there 
exists 
an 
accept 
state 
q 
(i.e., 
q 
E 
F) 
such 
that 
q 
is 
reachable 
from 
astart 
state 
under 
input 
string 
x 
(i.e., 
q 
E 
x)). 
We 
define 
L(N) 
to 
be 
the 
set 
of 
all 
strings 
accepted 
by 
N: 
L(N) 
= 
{x 
E 
E* 
IN 
accepts 
x}. 

_____________________________________________
34 
Lecture 
6 
Under 
this 
definition, 
every 
DFA 
(Q, 
6, 
s, 
F) 
is 
equivalent 
to 
an 
NFA 
(Q, 
ß, 
{s}, 
F), 
where 
ß(p,a) 
{6(p,a)}. 
Below 
we 
will 
show 
that 
the 
converse 
holds 
as 
weH: 
every 
NFA 
is 
equivalent 
to 
some 
DFA. 
Here 
are 
some 
basic 
lemmas 
that 
we 
will 
find 
useful 
when 
dealing 
with 
NFAs. 
The 
first 
corresponds 
to 
Exercise 
3 
of 
Homework 
1 
for 
deterministic 
automata. 
Lemma 
6.1 
For 
any 
x, 
y 
E 
and 
A 
Q, 
L\(A,xy) 
= 
L\(L\(A,x),y). 
Proo/. 
The 
proof 
is 
by 
induction 
on 
lyl. 
Basis 
For 
y 
= 
•, 
L\(A,u) 
= 
L\(A, 
x) 
= 
L\(.&(A, 
x),•) 
by 
(6.1). 
Induction 
step 
For 
any 
y 
E 
I:* 
and 
a 
E 
L\(A,xya) 
= 
U 
ß(q,a) 
by 
(6.2) 
qEA(A,zy) 
= 
U 
ß(q,a) 
induction 
hypothesis 
qEA(A(A,z),y) 
= 
L\(L\(A,x),ya) 
by 
(6.2). 
o 
Lemma 
6.2 
The 
junction 
L\ 
commutes 
with 
set 
union: 
for 
any 
indexed 
family 
Ai 
of 
subsets 
of 
Q 
and 
x 
E 
. 
L\(UAi,x) 
= 
UL\(Ai,x). 
i 
Proof. 
By 
induction 
on 
lxi. 

_____________________________________________
The 
Subset 
Construction 
Basis 
By 
(6.1), 
= 
UA, 
= 
, 
, 
, 
lnduction 
step 
= 
U 
A(p,a) 
by 
(6.2) 
Ai 
Ł 
..,) 
= 
U 
A(p,a) 
induction 
hypothesis 
peUi 
Ł 
..,) 
=u 
U 
A(p,a) 
basic 
set 
theory 
, 
Ł 
..,) 
= 
by 
(6.2). 
In 
particular, 
expressing 
a 
set 
as 
the 
union 
of 
its 
singleton 
subsets, 
= 
U 
pEA 
The 
Subset 
Construction: 
General 
Account 
The 
subset 
construction 
works 
in 
general. 
Let 
N 
= 
(QN, 
E, 
AN, 
SN, 
FN) 
35 
0 
(6.3) 
be 
an 
arbitrary 
NFA. 
We 
will 
use 
the 
subset 
construction 
to 
produce 
an 
equivalent 
DFA. 
Let 
M 
be 
the 
DFA 
M 
= 
(QM, 
E, 
6M, 
SM, 
FM), 
where 
Q 
'!!,f 
2
QN 
M-
, 
def 
...... 
6M(A,a) 
= 
AN(A,a), 
def 
SM 
= 
SN, 
FM 
{A 
S; 
QN 
I 
AnFN 
#: 
0}. 
Note 
that 
6M 
is 
a 
function 
from 
states 
of 
M 
and 
input 
symbols 
to 
states 
of 
M. 
as 
it 
should 
be, 
because 
states 
of 
M 
are 
sets 
of 
states 
of 
N. 

_____________________________________________
36 
Lecture 
6 
Lemma 
6.3 
For 
any 
A 
QN 
and 
x 
E 
E*, 
6
M
(A,x) 
= 
3.
N
(A, 
x). 
Proo/. 
Induction 
on 
lxi. 
BasiS 
For 
x 
= 
E, 
we 
want 
to 
show 
6M(A,E) 
= 
3.
N
(A,E). 
But 
both 
of 
these 
are 
A, 
by 
definition 
of 
6
M 
and 
3.
N
. 
Induction 
step 
Assurne 
that 
6
M
(A,x) 
= 
3.
N
(A,x). 
We 
want 
to 
show 
the 
same 
is 
true 
for 
xa, 
a 
E 
E. 
bM(A,xa) 
= 
ÖM(bM(A,x),a) 
= 
OM(3.
N
(A,x),a) 
= 
.6.N(.6.
N
(A,x),a) 
= 
.6.N(A,xa) 
definition 
of 
induction 
hypothesis 
definition 
of 
8 
M 
Lemma 
6.1. 
Theorem 
6.4 
The 
automata 
M 
and 
N 
accept 
the 
same 
set. 
Proof. 
For 
any 
x 
E 
B*, 
xE 
L(M) 
<=> 
6
M
(SM 
.. 
X) 
E 
FM 
<=> 
b.N(SN,x) 
n 
FN 
i=-
{21 
<=> 
x 
E 
L(N) 
E-
Transitions 
definition 
of 
acceptance 
for 
M 
definition 
of 
SM 
and 
FM, 
Lemma 
6.3 
definition 
of 
acceptance 
for 
N. 
0 
Here 
is 
another 
extension 
of 
finite 
automata 
that 
turns 
out 
to 
be 
quite 
useful 
but 
really 
adds 
no 
more 
power 
.. 
An 
f-transition 
is 
a 
transition 
with 
label 
f, 
a 
letter 
that 
stands 
for 
the 
null 
string 
f: 
Ł 
p 
----> 
q. 
The 
automaton 
can 
take 
such 
a 
transition 
anytime 
without 
reading 
an 
input 
symbol. 

_____________________________________________
Example 
6.5 
The 
Subset 
Construction 
37 
• 
• 
-rb/tb/rb 
p 
.q 
If 
the 
machine 
is 
in 
state 
sand 
the 
next 
input 
symbol 
is 
b, 
it 
can 
terministically 
decide 
to 
do 
one 
of 
three 
things: 
Ł 
read 
the 
band 
move 
to 
state 
p; 
Ł 
slide 
to 
t 
without 
reading 
an 
input 
symbol, 
then 
read 
the 
band 
move 
to 
state 
q; 
or 
Ł 
slide 
to 
t 
without 
reading 
an 
input 
symbol, 
then 
slide 
to 
u 
without 
reading 
an 
input 
symbol, 
then 
read 
the 
band 
move 
to 
state 
T. 
The 
set 
of 
strings 
accepted 
by 
this 
automat 
on 
is 
{b, 
bb, 
bbb}. 
o 
Example 
6.6 
Here 
is 
a 
nondeterministic 
automaton 
with 
•-transitions 
accepting 
the 
set 
{x 
E 
{a}* 
Ilxl 
is 
divisible 
by 
3 
or 
5}: 
The 
automaton 
chooses 
at 
the 
outset 
which 
of 
the 
two 
conditions 
to 
check 
for 
(divisibility 
by 
3 
or 
5) 
and 
slides 
to 
one 
of 
the 
two 
loops 
accordingly 
without 
reading 
an 
input 
symbol. 
0 
The 
main 
benefit 
of 
•-transitions 
is 
convenience. 
They 
do 
not 
really 
add 
any 
power: 
a 
modified 
subset 
construction 
involving 
the 
notion 
of 
•-closure 
can 
be 
used 
to 
show 
that 
every 
NFA 
with 
•-transitions 
can 
be 
simulated 
by 
a 
DFA 
without 
•-transitions 
(Miscellaneous 
Exercise 
10); 
thus 
all 
sets 
accepted 
by 
nondeterministic 
automata 
with 
•-transitions 
are 
regular. 
We 
will 
also 
give 
an 
alternative 
treatment 
in 
Lecture 
10 
using 
homomorphisms. 
More 
Closure 
Properties 
Recall 
that 
the 
concatenation 
of 
sets 
A 
and 
B 
is 
the 
set 
AB 
= 
{xy 
I 
x 
E 
A 
and 
y 
E 
B}. 

_____________________________________________
38 
Lecture 
6 
Fot 
example, 
{a, 
ab}{ 
b, 
ba} 
= 
{ab, 
aba, 
abb, 
abba}. 
If 
A 
and 
Bare 
regular, 
then 
so 
is 
AB. 
To 
see 
this, 
let 
M 
be 
an 
automaton 
for 
A 
and 
N 
an 
automaton 
for 
B. 
Make 
a 
new 
automaton 
P 
whose 
states 
are 
the 
union 
of 
the 
state 
sets 
of 
M 
and 
N, 
and 
take 
all 
the 
transitions 
of 
M 
and 
N 
as 
transitions 
of 
P. 
Make 
the 
start 
states 
of 
M 
the 
start 
states 
of 
P 
and 
the 
final 
states 
of 
N 
the 
final 
states 
of 
P. 
Finally, 
put 
f-transitions 
from 
all 
the 
finalstates 
of 
M 
to 
all 
the 
start 
states 
of 
N. 
Then 
L{P) 
= 
AB. 
Example 
6.7 
Let 
1-
= 
{aal, 
B 
= 
{bb}. 
Here 
are 
automata 
for 
A 
and 
B: 
a 
.. 
... 
b 
ŁŁ 
ŁŁ 
Here 
is 
the 
automaton 
you 
get 
by 
the 
construction 
above 
for 
AB: 
.. 
a.. 
a.. 
E.. 
b.. 
b 
.® 
If 
A 
is 
regular, 
then 
so 
is 
its 
asterate: 
A* 
= 
{f} 
U 
A 
U 
A
2 
U 
A
3 
U'" 
= 
{XIX2'" 
X
n 
I 
n 
0 
and 
Xi 
E 
A, 
1 
i 
n}. 
o 
To 
see 
this, 
take 
an 
automaton 
M 
for 
A. 
Build 
an 
automaton 
P 
for 
A 
* 
as 
folIows. 
Start 
with 
all 
the 
states 
and 
transitions 
of 
M. 
Add 
a 
new 
state 
s. 
Add 
f-transitions 
from 
s 
to 
all 
the 
start 
states 
of 
M 
and 
from 
aU 
the 
final 
states 
of 
M 
to 
s. 
Make 
s 
the 
only 
start 
state 
of 
P 
and 
also 
the 
only 
final 
state 
of 
P 
,( 
thus 
the 
start 
and 
final 
states 
of 
M 
are 
not 
start 
and 
final 
states 
of 
P). 
Then 
P 
accepts 
exactlythe 
set 
A*. 
Example 
6.8 
Let 
A 
= 
{aal. 
Consider 
the 
three-state 
automaton 
for 
A 
in 
Example 
6.7. 
Here 
is 
the 
automaton 
you 
get 
for 
A 
* 
by 
the 
construction 
above: 
a 
ŁŁ 
o 
In 
this 
construction, 
you 
must 
add 
the 
new 
start/final 
state 
s. 
You 
might 
think 
that 
it 
suffices 
to 
put 
in 
•-transiÜons 
from 
the 
old 
final 
states 
back 
to 
the 
old 
start 
states 
and 
make 
the 
old 
start 
states 
final 
states, 
hut 
this 
doesn't 
always 
work. 
Here's 
a 
counterexample: 

_____________________________________________
The 
Subset 
Construction 
39 
The 
set 
accepted 
is 
{aftb 
I 
n 
O}. 
The 
asterate 
of 
this 
set 
is 
{ 
E} 
U 
{strings 
ending 
with 
b}, 
but 
if 
you 
put 
in 
an 
E-transition 
from 
the 
final 
state 
back 
to 
the 
start 
state' 
and 
made 
the 
start 
state 
a 
final 
state, 
then 
the 
set 
accepted 
would 
be 
{a,b}*. 
Historical 
Notes 
Rabin 
and 
Scott 
[102] 
introduced 
nondeterministic 
finite 
automata 
and 
showed 
using 
the 
subset 
construction 
that 
they 
were 
no 
more 
powerful 
than 
deterministic 
finite 
automata. 
Closure 
properties 
of 
regular 
sets 
were 
studied 
by 
Ginsburg 
and 
Rose 
[46, 
48], 
GinsbUrg 
[43], 
McNaughton 
and 
Yamada 
[85], 
and 
Rabin 
and 
Scott 
[102], 
among 
others. 

_____________________________________________
Lecture 
7 
Pattern 
Match 
i 
ng 
What 
happens 
when 
one 
types 
rm 
* 
in 
UNIX? 
(If 
you 
don't 
know, 
don't 
try 
it 
to 
find 
out!) 
What 
if 
the 
current 
directory 
contains 
the 
files 
a. 
tex 
bc. 
tex 
a.dvi 
bc 
.. 
dvi 
and 
one 
types 
rm 
*. 
dvi? 
What 
would 
happen 
if 
there 
were 
a 
file 
named 
.dvi? 
What 
is 
going 
on 
here 
is 
pattern 
matching. 
The 
* 
in 
UNIX 
is 
a 
pattern 
that 
matches 
any 
string 
of 
symbols, 
including 
the 
null 
string. 
Pattern 
matching 
is 
an 
important 
application 
of 
finite 
automata. 
The 
UNIX 
commands 
grep, 
fgrep, 
and 
egrep 
are 
basic 
pattern-matching 
utilities 
that 
use 
finite 
automata 
in 
their 
implementation. 
Let 
E 
be 
a 
finite 
alphabet. 
A 
pattern 
is 
astring 
of 
symbols 
of 
a 
certain 
form 
representing 
a 
(possibly 
infinite) 
set 
of 
strings 
in 
E*. 
The 
set 
of 
patterns 
is 
defined 
formally 
by 
induction 
below. 
They 
are 
either 
atomic 
patterns 
or 
compound 
patterns 
built 
up 
inductively 
from 
atomic 
patterns 
using 
certain 
operators. 
We'll 
denote 
patterns 
by 
Greek 
letters 
a, 
ß, 
,,{, 
.... 
As 
we 
define 
patterns, 
we 
will 
tell 
which 
strings 
x 
E 
E* 
match 
them. 
The 
set 
of 
strings 
in 
E* 
matching 
a 
given 
pattern 
a 
will 
be 
denoted 
L(a). 
Thus 
L(a) 
= 
{x 
E 
E* 
I 
x 
matches 
a}. 

_____________________________________________
Pattern 
Matching 
41 
In 
the 
following, 
forget 
the 
UNIX 
definition 
of 
*. 
We 
will 
use 
the 
symbol 
* 
for 
something 
else. 
The 
atomi 
patterns 
are 
Ł a 
for 
each 
a 
E 
matched 
by 
the 
symbol 
a 
only; 
in 
symbols, 
L(a) 
= 
{al; 
Ł 
tE, 
matched 
only 
by 
f. 
the 
null 
string; 
in 
symbols, 
L(::) 
= 
{•}; 
Ł 
0, 
matched 
by 
nothing; 
in 
symbols, 
L(0) 
= 
0, 
the 
empty 
set; 
Ł 
#, 
matched 
by 
any 
symbol 
in 
in 
symbols, 
L( 
#) 
= 
E; 
Ł 
@, 
matched 
by 
any 
string 
in 
E*; 
in 
symbols, 
L(@) 
= 
Compound 
patterns 
are 
formed 
inductively 
using 
binary 
operators 
+, 
n, 
and 
. 
(usually 
not 
written) 
and 
unary 
operators 
+, 
*, 
and 
"'. 
If 
0 
and 
ß 
are 
patterns, 
then 
so 
are 
(.I: 
+ 
ß, 
0 
n 
ß, 
0*, 
0+, 
"'0, 
and 
oß. 
The 
last 
of 
these 
is 
short 
for 
0 . 
ß. 
We 
also 
define 
inductively 
which 
strings 
match 
each 
pattern. 
We 
have 
ready 
said 
which 
strings 
match 
the 
atomic 
patterns. 
This 
is 
the 
basis 
of 
the 
inductive 
definition. 
Now 
suppose 
we 
have 
already 
defined 
the 
sets 
of 
strings 
L(o) 
and 
L(ß) 
matching 
0 
and 
ß, 
respectively. 
Then 
we'll 
say 
that 
Ł x 
matches 
0 
+ 
ß 
if 
x 
matches 
either 
0 
or 
ß: 
L(o 
+ 
ß) 
= 
L(o) 
U 
L(ß); 
Ł x 
matches 
0 
n 
ß 
if 
x 
matches 
both 
0 
and 
ß: 
L(o 
n 
ß) 
= 
L(o) 
n 
L(ß)j 
Ł x 
matches 
oß 
if 
x 
can 
be 
broken 
down 
as 
x 
= 
yz 
such 
that 
y 
matches 
o 
and 
z 
matches 
ß: 
L(oß) 
= 
L(o)L(ß) 
= 
{yz 
I 
y 
E 
L(o) 
and 
z 
E 
L(ß)}; 
Ł x 
matches 
'" 
0 
if 
x 
does 
not 
match 
0: 
L{",o) 
= 
"'L{o) 
= 
E* 
-
L(o); 
Ł x 
matches 
0* 
if 
x 
can 
be 
expressed 
as 
a 
concatenation 
of 
zero 
or 
more 
strings, 
all 
of 
which 
match 
0: 
L(o*) 
= 
{XIX2'" 
X
n 
I 
n 
0 
and 
Xi 
E 
L(o), 
1 
i 
n} 
= 
L(o)o 
U 
L(o)l 
U 
L(0)2 
U··· 

_____________________________________________
42 
Lecture 
7 
Example 
7.1 
Example 
7.2 
= 
L(a)*. 
The 
null 
string 
f 
always 
matches 
a*, 
since 
f 
is 
a 
concatenation 
of 
zero 
strings, 
all 
of 
which 
(vacuously) 
match 
a . 
Ł x 
matches 
a+ 
if 
x 
can 
be 
expressed 
as 
a 
concatenation 
of 
one 
or 
more 
strings, 
all 
of 
which 
match 
a: 
L(a+) 
= 
{XIX2"'Xn 
I 
n 
1 
and 
Xi 
E 
L(a), 
1 
i 
n} 
= 
L(a)l 
U 
L(a)2 
U 
L(a)3 
U 
... 
= 
L(a)+. 
Note 
that 
patterns 
are 
just 
certain 
strings 
of 
symbols 
over 
the 
alphabet 
Eu 
{ 
E, 
P, 
#, 
@, 
+, 
n, 
....... 
, 
*, 
+, 
(, 
) 
}. 
Note 
also 
that 
the 
meanings 
of 
#, 
@, 
and 
....... 
depend 
on 
E. 
For 
example, 
if 
E 
= 
{a,b,c} 
then 
L(#) 
= 
{a,b,c}, 
but 
if 
E 
= 
{al 
then 
L(#) 
= 
{al 
. 
Ł 
E* 
= 
L(@) 
= 
L(#*). 
Ł 
Singleton 
sets: 
if 
x 
E 
E*, 
then 
x 
itself 
is 
a 
pattern 
and 
is 
matched 
only 
by 
the 
string 
x; 
i.e., 
{x} 
= 
L(x). 
Ł 
Finite 
sets: 
if 
Xl, 
..Ł 
,X
m 
E 
E*, 
then 
{Xl. 
X2,"" 
X
m
} 
= 
L(Xl 
+ 
X2 
+ 
... 
+ 
X
m
). 
o 
Note 
that 
we 
can 
write 
the 
last 
pattern 
Xl 
+ 
X2 
+ . " + 
X
m 
without 
theses, 
since 
the 
two' 
patterns 
(a 
+ 
ß) 
+ 
'Y 
and 
a 
+ 
(ß 
+ 
'Y) 
are 
matched 
by 
the 
same 
set 
of 
strings; 
Le., 
L((a 
+ 
ß) 
+ 
'Y) 
= 
L(a 
+ 
(ß 
+ 
'Y)). 
Mathematically 
speaking, 
the 
operator 
+ 
is 
associative. 
The 
concatenation 
operator· 
is 
associative, 
too. 
Hence 
we 
can 
also 
unambiguously 
write 
aß'Y 
without 
parentheses. 
Ł 
strings 
containing 
at 
least 
three 
occurrences 
of 
a: 
@a@a@a@; 
Ł 
strings 
containing 
an 
a 
followed 
later 
by 
a 
b; 
that 
is, 
strings 
of 
the 
form 
xaybz 
for 
some 
x, 
1/, 
z: 
@a@b@; 
Ł 
all 
single 
letters 
except 
a: 
#n-aj 

_____________________________________________
Ł 
strings 
with 
no 
occurrence 
of 
the 
letter 
a: 
(# 
n 
"-'a)*j 
Pattern 
Matching 
43 
Ł 
strings 
in which 
every 
occurrence 
of 
a 
is 
followed 
sometime 
later 
by 
an 
occurrence 
of 
bj 
in 
otther 
words, 
strings 
in 
which 
there 
are 
either 
no 
occurrences 
of 
a, 
or 
there 
is 
an 
occurrence 
of 
b· 
followed 
by 
no 
occurrence 
of 
aj 
for 
example, 
aab 
matches 
but 
bba 
doesn't: 
(# 
n 
"-'a)* 
+ 
@b(# 
n 
"'a)* 
If 
the 
alphabet 
is 
{a,b}, 
then 
this 
takes 
a 
much 
simpler 
form: 
•+@b. 
o 
Before 
we 
go 
too 
much 
further, 
there 
is 
a 
subtlety 
that 
needs 
to 
be 
tioned. 
Note 
the 
slight 
difference 
in 
appearance 
between 
• 
and 
E 
and 
tween 
0 
and 
0. 
The 
objects 
• 
and 
0 
are 
symbols 
in 
the 
language 
of 
patterns, 
whereas 
E 
and 
0 
are 
metasymbols 
that 
we 
are 
using 
to 
name 
the 
null 
string 
and 
the 
empty 
set, 
respectively. 
These 
are 
different 
sorts 
of 
things: 
• 
and 
o 
are 
symbols, 
that 
is, 
strings 
of 
length 
one, 
whereas 
E 
is 
astring 
of 
length 
zero 
and 
0 
isn't 
even 
astring. 
We'll 
maintain 
the 
distinction 
for 
a 
few 
lectures 
until 
we 
get 
used 
to 
the 
idea, 
but 
at 
some 
point 
in 
the 
near 
future 
we'll 
drop 
the 
boldface 
and 
use 
E 
and 
0 
exclusively. 
We'll 
aiways 
be 
able 
to 
infer 
from 
context 
whether 
we 
mean 
the 
symbols 
or 
the 
metasymbols. 
This 
is 
a 
little 
more 
convenient 
and 
conforms 
to 
standard 
usage, 
but 
bear 
in 
mind 
that 
they 
are 
still 
different 
things. 
While 
we're 
on 
the 
subject 
of 
abuse 
of 
notation, 
we 
should 
also 
mention 
that 
very 
often 
you 
will 
see 
things 
like 
x 
E 
a*b* 
in 
texts 
and 
articles. 
Strictly 
speaking, 
one 
should 
write 
x 
E 
L(a*b*), 
since 
a*b* 
is 
a 
pattern, 
not 
a 
set 
of 
strings. 
But 
as 
long 
as 
you 
know 
what 
you 
really 
mean 
and 
can 
stand 
the 
guilt, 
it 
is 
okay 
to 
write 
x 
E 
a*b* 

_____________________________________________
Lecture 
8 
Pattern 
Matching 
and 
Regular 
Expressions 
Here 
are 
some 
interesting 
and 
important 
questions: 
Ł 
How 
hard 
is 
it 
to 
determine 
whether 
a 
given 
string 
x 
matches 
a 
given 
pattern 
a? 
This 
is 
an 
important 
practical 
question. 
There 
are 
very 
efficient 
algorithms, 
as 
we 
will 
see. 
Ł 
Is 
every 
set 
represented 
by 
some 
pattern? 
Answer: 
no. 
For 
example, 
the 
set 
is 
not 
represented 
by 
any 
pattern. 
We'll 
prove 
this 
later. 
Ł 
Patterns 
a 
and 
ß 
are 
equivalent 
if 
L(a) 
= 
L(ß). 
How 
do 
you 
tell 
whether 
a 
and 
ß 
are 
equivalent? 
Sometimes 
it 
is 
obvious 
and 
times 
not. 
Ł 
Which 
operators 
are 
Jedundant? 
For 
example, 
we 
can 
get 
rid 
of 
• 
since 
it 
is 
equivalent-
to 
'" 
(#@) 
and 
also 
to 
We 
can 
get 
rid 
of 
@ 
since 
it 
is 
equivalent 
to 
#*. 
We 
can 
get 
rid 
of 
unary 
+ 
since 
a+ 
is 
equivalent 
to 
aa*. 
We 
can 
get 
rid 
of 
#, 
since 
if 
I; 
= 
{al, 
... 
, 
an} 
then 
# 
is 
equivalent 
to 
the 
pattern 

_____________________________________________
Pattern 
Matching 
and 
Regular 
Expressions 
45 
The 
operator 
n 
is 
also 
redundant, 
by 
one 
of 
the 
De 
Morgan 
laws: 
0: 
n 
ß 
is 
equivalent 
to 
I"V 
( 
I"V 
0: 
+ 
I"V 
ß). 
Redundancy 
is 
an 
important 
question. 
From 
a 
user's 
point 
of 
view, 
we 
would 
like 
to 
have 
a 
lot 
of 
operators 
since 
this 
lets 
us 
write 
more 
succinct 
patterns; 
but 
from 
a 
programmer's 
point 
of 
view, 
we 
would 
like 
to 
have 
as 
few 
as 
possible 
.since 
there 
is 
less 
code 
to 
write. 
,Also, 
from 
a 
theoretical 
point 
of 
view, 
fewer 
operators 
mean 
fewer 
cases 
we 
have 
to 
treat 
in 
giving 
formal 
semantics 
and 
proofs 
of 
correctness. 
An 
amazing 
and 
difficult-to-prove 
fact 
is 
that 
the 
operator 
I"V 
is 
redundant. 
Thus 
every 
pattern 
is 
equivalent 
to 
one 
using 
only 
atomic 
patterns 
a 
E 
E, 
•, 
ß, 
and 
operators 
+, 
" 
and 
*. 
Patterns 
using 
only 
these 
symbols 
are 
called 
regular 
expressions. 
Actually, 
as 
we 
have 
observed, 
even 
• 
is 
redundant,but 
we 
include 
it 
in 
the 
definition 
of 
regular 
expressions 
because 
it 
occurs 
so 
often. 
Our 
goal 
for 
this 
lecture 
and 
the 
next 
will 
be 
to 
show 
that 
the 
family 
of 
subsets 
of 
E* 
represented 
by 
patterns 
is 
exactly 
the 
family 
of 
regular 
sets. 
Thus 
as 
a 
way 
of 
describing 
subsets 
of 
E*, 
finite 
automata, 
patterns, 
and 
regular 
expressions 
are 
equally 
expressive. 
Some 
Notational 
Conveniences 
Since 
the 
binary 
operators 
+ 
and 
. 
are 
associative, 
that 
is, 
L(o: 
+ 
(ß 
+ 
'Y)) 
= 
L((o: 
+ 
ß) 
+ 
'Y), 
L(o:(ß'Y)) 
= 
L((o:ßh), 
we 
can 
write 
0: 
+ 
ß 
+ 
'Y 
and 
o:ß'Y 
without 
a,mbiguity. 
To 
resolve 
ambiguity 
in 
other 
situations, 
we 
assign 
precedence 
to 
operators. 
For 
example, 
could 
be 
interpreted 
as 
either 
0: 
+ 
(ß'Y) 
or 
(0: 
+ 
ßh, 
which 
are 
not 
equivalent. 
We 
adopt 
the 
convention 
that 
the 
concatenation 
operator· 
has 
high 
er 
precedence 
than 
+, 
so 
that 
we 
would 
prefer 
the 
former 
interpretation. 
Similarly, 
we 
assign 
* 
higher 
precedence 
than 
+ 
or 
" 
so 
that 
0: 
+ 
ß* 

_____________________________________________
46 
Lecture 
8 
is 
interpreted 
as 
0: 
+ 
(ß*) 
and 
not 
as 
(o:+ß)*. 
All 
else 
failing, 
use 
parentheses. 
Equivalence 
of 
Patterns, 
Regular 
Expressions, 
and 
Finite 
Automata 
Patterns, 
regular 
expressions 
(patterns 
built 
from 
atomic 
patterns 
a 
E 
•, 
0, 
and 
operators 
+, 
*, 
and 
. 
only), 
and 
finite 
automata 
are 
all 
equivalent 
in 
expressive 
power: 
they 
all 
represent 
the 
regular 
sets. 
Theorem 
8.1 
Let 
A 
The 
jollowing 
three 
statements 
are 
equivalent: 
(i) 
A 
is 
regular; 
that 
is, 
A 
= 
L( 
M) 
for 
·some 
finite 
automaton 
M; 
(ii) 
A 
= 
L(o:) 
for 
some 
pattern 
0:; 
(iii) 
A 
= 
L(o:) 
for 
some 
regular 
expression 
0:. 
Praof. 
The 
implication 
(iii) 
=> 
(ii) 
is 
trivial, 
since 
every 
regular 
expression 
is 
a 
pattern. 
We 
prove 
(ii) 
=> 
(i) 
hefe 
and 
(i) 
=> 
(iii) 
,in 
Lecture 
9. 
The 
heart 
of 
the 
proof 
(ii) 
=> 
(i) 
involves 
showing 
that 
certain 
basic 
sets 
(corresponding 
to 
atomic 
patterns) 
are 
regular, 
and 
the 
regular 
sets 
are 
closed 
under 
certain 
closure 
operations 
corresponding 
to 
the 
operators 
used 
to 
build 
patterns. 
Note 
that 
Ł 
the 
singleton 
set 
{al 
is 
regular, 
a 
E 
Ł 
the 
singleton 
set 
{•} 
is 
regular, 
and 
Ł 
the 
empty 
set 
0 
is 
regular, 
since 
each 
of 
these 
sets 
is 
the 
set 
accepted 
by 
so 
me 
automaton. 
Here 
are 
nondeterministic 
automata 
for 
these 
three 
sets, 
respectively: 
ŁŁ 
Also, 
we 
have 
previously 
shown 
that 
the 
regular 
sets 
are 
closed 
under 
the 
set 
operations 
U, 
n, 
"", 
" 
,." 
and 
+; 
that 
is, 
if 
A 
and 
B 
are 
regular 
sets, 
then 
so 
are 
A 
U 
B, 
An 
B, 
"" 
A 
= 
-
A, 
AB, 
A"', 
and 
A 
+ . 
These 
facts 
can 
be 
used 
to 
prove 
inductively 
that 
(ii) 
=> 
(i). 
Let 
0: 
be 
a 
given 
pattern. 
We 
wish 
to 
show 
that 
L(o:) 
is 
a 
regular 
set. 
We 
proceed 
by 

_____________________________________________
Pattern 
Matching 
and 
Regular 
Expressions 
47 
induction 
on 
the 
structure 
of 
Cl:. 
The 
pattern 
Cl: 
is 
of 
one 
of 
the 
following 
forms: 
(i) 
a, 
where 
a 
E 
1ji 
(vi) 
ß 
+'Yj 
(ii) 
f' 
, 
(vii) 
ß 
n 
'Yi 
(iii) 
( 
viii) 
ß'Yi 
(iv) 
#i 
(ix) 
'" 
ßi 
(v) 
@. 
, 
(x) 
ß*j 
(xi) 
ß+ 
There 
are 
five 
base 
cases 
(i) 
through 
(v) 
corresponding 
to 
the 
atomic 
terns 
and 
six 
induction 
cases 
(vi) 
through 
(xi) 
corresponding 
to 
compound 
patterns. 
Each 
of 
these 
cases 
m;es 
a 
closure 
property 
of 
the 
regular 
sets 
previously 
observed. 
For 
(i), 
(ii), 
and 
(iii), 
we 
have 
L(a) 
=-= 
{al 
for 
a 
E 
1j, 
L(f) 
= 
{f}, 
and 
= 
0, 
and 
these 
are 
regular 
sets. 
For 
(iv), 
(v), 
and 
(xi), 
we 
observed 
earlier 
that 
the 
operators 
#, 
@, 
and 
+ 
were 
redundant, 
so 
we 
may 
disregard 
these 
cases 
since 
they 
are 
already 
covered 
b! 
the 
other 
cases. 
For 
(vi), 
recall 
that 
L(ß+'Y) 
= 
L(ß)UL('Y) 
by 
definition 
ofthe 
+ 
operator. 
By 
the 
induct.ion 
hypothesis, 
L(ß) 
and 
L( 
'Y) 
are 
regular. 
Since 
the 
regular 
sets 
are 
closed 
under 
union, 
L(ß 
+ 
'Y) 
= 
L(ß) 
U 
L('Y) 
is 
also 
regular. 
The 
arguments 
for 
the 
re}1laining 
cases 
(vii) 
through 
(x) 
are 
similar 
to 
the 
argument 
for 
(vi). 
Each 
of 
these 
cases 
uses 
a 
closure 
property 
of 
the 
regular 
sets 
that 
we 
have 
observed 
previously 
in 
Lectures 
4 
and 
6. 
0 
Example 
8.2 
Let's 
convert 
the 
regular 
expression 
(aaa)* 
+ 
(aaaaa)* 
för 
the 
set 
{x 
E 
{a}'" 
Ilxl 
is 
divisible 
by 
either 
3 
or 
5} 
to 
an 
equivalent 
NFA. 
First 
we 
show 
how 
to 
construct 
an 
automaton 
for 
(aaa)*. 
We 
take 
an 
automaton 
accepting 
only 
the 
string 
aaa, 
say 
a a 
.., 
. 
... 
... 
a 
-® 
Applying 
the 
construction 
of 
Lecture 
6, 
we 
add 
a 
new 
start 
state 
and 
transitions 
from 
the 
new 
start 
state 
to 
all 
the 
old 
start 
states 
and 
from 
all 
old 
accept 
states 
to 
the 
new 
start 
state. 
We 
let 
the 
new 
start 
state 
be 
the 
only 
accept 
state 
of 
the 
new 
automaton. 
This 
gives 

_____________________________________________
48 
Lecture 
8 
a a 
.. 
. 
.. 
. 
The 
construction 
for 
(aaaaa)* 
is 
similar, 
giving 
a 
a a 
a 
To 
get 
an 
NFA 
for 
(aaa)*+(aaaaa)*, 
we 
can 
simply 
take 
the 
disjoint 
union 
of 
these 
two 
automata: 
• 
a a 
a,:\ 
.. 
. 
.. 
. 
. 
E 
a. 
a 
a 
a 
a,:\ 
... 
0 

_____________________________________________
Lecture 
9 
Regular 
Expressions 
and 
Finite 
Automata 
Simplification 
of 
Expressions 
For 
small 
regular 
expressions, 
one 
can 
often 
see 
how 
to 
construct 
an 
lent 
automaton 
directly 
without 
going 
through 
the 
mechanical 
procedure 
of 
the 
previous 
lecture. 
It 
is 
therefore 
useful 
to 
try 
to 
simplify 
the 
expression 
first. 
For 
regular 
expressions 
a,ß, 
if 
L(a) 
= 
L(ß), 
we 
write 
a 
== 
ß 
and 
say 
that 
a 
and 
ß 
are 
equivalent. 
The 
relation 
== 
on 
regular 
expressions 
is 
an 
equivalenee 
relation; 
that 
is, 
it 
is 
Ł 
reflexive: 
a 
== 
a 
for 
all 
a; 
Ł 
symmetrie: 
if 
a 
== 
ß, 
then 
ß 
== 
a; 
and 
Ł 
transitive: 
if 
a 
== 
ß 
and 
ß 
== 
'Y, 
then 
a 
== 
'Y. 
If 
a 
== 
ß, 
one 
ean 
substitute 
a 
for 
ß( 
or 
viee 
versa) 
in 
any 
regular 
expression, 
and 
the 
resulting 
expression 
will 
be 
equivalent 
to 
the 
original. 
Here 
are 
a 
few 
laws 
that 
ean 
be 
used 
to 
simplify 
regular 
expressions. 
a+(ß+'Y) 
== 
(a+ß)+'Y 
a+ß 
== 
ß+a 
(9.1) 
(9.2) 

_____________________________________________
50 
Lecture 
9 
a+a 
= 
a 
a(ß"{) 
= 
(aßh 
a(ß 
+ 
"() 
= 
aß 
+ 
a"{ 
(a 
+ 
ßh 
= 
a"{ 
+ 
ß"{ 
!iSa 
= 
a!iS 
= 
• 
+aa* 
= 
alle 
•+a*a 
= 
a* 
ß 
+ 
a"{ 
"{ 
:::} 
a* 
ß 
"{ 
ß 
+ 
"{Ü'. 
"{ 
:::} 
ßa* 
"( 
In 
(9.12) 
and 
(9.13), 
refers 
to 
the 
subset 
order: 
a 
ß 
L(a) 
L(ß) 
L(a+ß) 
= 
L(ß) 
(9.3) 
(9.4) 
(9.5) 
(9.6) 
(9.7) 
(9.8) 
(9.9) 
(9.10) 
(9.11) 
(9.12) 
(9.13) 
Laws 
(9.12) 
and 
(9.13) 
are 
not 
equations 
but 
rules 
from 
which 
one 
can 
derive 
equations 
from 
other 
equations. 
Laws 
{9.1) 
through 
(9.13) 
can 
be 
justified 
by 
replacing 
each 
expression 
by 
its 
definition 
and 
reasoning 
set 
theoretically. 
Here 
are 
so 
me 
useful 
equations 
that 
follow 
from 
(9.1) 
through 
(9.13) 
that 
you 
can 
use 
to 
simplify 
expressions. 
(aß)*a 
= 
a(ßa)* 
(a*ß)*a* 
= 
(a 
+ 
ß)* 
a*(ßa*)* 
= 
(a 
+ 
ß)* 
(•+a)* 
=a* 
aa* 
= 
a*a 
(9.14) 
(9.15) 
(9.16) 
(9.17) 
(9.18) 
An 
interesting 
fact 
that 
is 
beyond 
the 
scope 
of 
this 
course 
is 
that 
all 
true 
equations 
between 
regular 
expressions 
can 
be 
proved 
purely 
algebraically 
from 
the 
axioms 
and 
rules 
(9.1) 
through 
(9.13) 
plus 
the 
laws 
of 
equational 
logic 
[73J. 
To 
illustrate, 
let's 
convert 
some 
regular 
expressions 
to 
finite 
automata. 

_____________________________________________
Regular 
Expressions 
and 
Finite 
-Automata 
. 
51 
Example 
9.1 
(11 
+ 
0)*(00 
+ 
1)* 
o 
1 
1 
o 
This 
expression 
is 
simple 
enough 
that 
the 
easiest 
thing 
to 
do 
is 
eyeball 
it. 
The 
mechanical 
method 
described 
in 
Lecture 
8 
would 
give 
more 
states 
and 
f-transitions 
than 
shown 
here. 
The 
two 
states 
connected 
by 
an 
f-transition 
cannot 
be 
collapsed 
into 
one 
state, 
since 
then 
10 
would 
be 
accepted, 
which 
does 
not 
match 
the 
regular 
expression. 
0 
Example 
9.2 
(1 
+ 
01 
+ 
001)*(e 
+ 
0 
+ 
00) 
Using 
the 
algebraic 
laws 
above, 
we 
can 
rewrite 
the 
expression: 
(1 
+ 
01 
+ 
001)*(e 
+ 
0 
+ 
00) 
== 
((e 
+ 
0 
+ 
OO)l)*(e 
+ 
0 
+ 
00) 
== 
((e 
+ 
O)(e 
+ 
O)l)*(e 
+ 
O)(e 
+ 
0). 
It 
is 
now 
easier 
to 
see 
that 
the 
set 
represented 
is 
the 
set 
of 
all 
strings 
over 
{O, 
1} 
with 
no 
substring 
of 
more 
than 
two 
adjacent 
O's. 
o 
Just 
because 
all 
states 
of 
an 
NFA 
are 
accept 
states 
doesn't 
mean 
that 
all 
strings 
are 
accepted! 
Note 
that 
in 
Example 
9.2, 
000 
is 
not 
accepted. 
Converting 
Automata 
to 
Regular 
Expressions 
To 
finish 
the 
proof 
of 
Theorem 
8.1, 
it 
remains 
to 
show 
how 
to 
convert 
a 
given 
finite 
automaton 
M 
to 
an 
equivalent 
regular 
expression. 
Given 
an 
NFA 
M 
= 
(Q, 
6, 
S, 
P), 
a 
subset 
X 
Q, 
and 
states 
u, 
v 
E 
Q, 
we 
show 
how 
to 
construct 
a 
regular 
expression 
x 
QUtl 

_____________________________________________
52 
Lec:ture 
9 
representing 
the 
set 
of 
all 
strings 
x 
such 
that 
there 
is 
a 
path 
from 
11. 
to 
v 
in 
M 
labeled 
x 
(i.e., 
such 
that 
v 
E 
.&( 
{u}, 
x)) 
and 
all 
states 
along 
that 
path, 
with 
the 
possible 
exception 
of 
11. 
and 
v, 
He 
in 
X. 
The 
expressions 
are 
constructed 
inductively 
on 
the 
size 
of 
X. 
For 
the 
basis 
X 
= 
0, 
let 
al, 
... 
, 
aAl 
be 
all 
the 
symbols 
in 
E 
such 
that 
v 
E 
Ä(u, 
ai). 
For 
11. 
i: 
v, 
take 
and 
for 
11. 
= 
v, 
take 
if 
k 
;:: 
1, 
if 
k 
= 
0; 
" 
{ 
al 
+ 
... 
+ 
aAl 
+ 
E 
a
uv 
-
E 
if 
k 
;:: 
1, 
if 
k 
= 
O. 
For 
nonempty 
X, 
we 
can 
choose 
any 
element 
q 
E 
X 
and 
take 
x 
c!!.f 
X-{q} 
+ 
X-{q}( 
X-{q})* 
X-{q} 
a
uv 
-
a
uv 
a
uq 
a
qq 
a
qv 
. 
(9.19) 
To 
justify 
the 
definition 
(9.19), 
note 
that 
any 
path 
from 
11. 
to 
v 
with 
all 
intermediate 
states 
in 
X 
either 
(i) 
never 
visits 
q, 
hence 
the 
expression 
aX-{q} 
IW 
on 
the 
right-hand 
side 
of 
(9.19); 
or 
(ii) 
visits 
q 
for 
the 
first 
time, 
hence 
the 
expression 
aX-{q} 
uq 
, 
followed 
by 
a 
finite 
number 
(possibly 
zero) 
of 
loops 
from 
q 
back 
to 
itself 
withou.t 
visiting 
q 
in 
between 
and 
staying 
in 
X, 
hence 
the 
expression 
(
aX-{q})* 
qq 
, 
followed 
by 
a 
path 
from 
q 
to 
v 
after 
leaving 
q 
for 
the 
last 
time, 
hence 
the 
expression 
",x 
-{q} 
'""qv 
Ł 
The 
sum 
of 
all 
expressions 
of 
the 
form 
Q 
a./, 
where 
s 
is 
astart 
state 
and 
/ 
is 
a 
final 
state, 
represents 
the 
set 
of 
strings 
a.ccepted 
by 
M. 
As 
a 
practical 
rule 
of 
thumb 
when 
doing 
homework 
exercises, 
when 
ing 
the 
q 
E 
X 
to 
drop 
out 
in 
(9.19), 
it 
is 
best 
to 
try 
to 
choose 
one 
that 
disconnects 
the 
automaton 
as 
much 
as 
possible. 

_____________________________________________
Regular 
Expressions 
and 
Finite 
Automata 
53 
. 
Example 
9.3 
Let's 
convert 
the 
automaton 
q 
o 
r 
to 
an 
equivalent 
regular 
expression. 
The 
set 
accepted 
by 
this 
automaton 
will 
be 
represented 
by 
the 
inductively 
defined 
regular 
expression 
a{p,q,r} 
pp 
, 
since 
p 
is 
the 
only 
start 
and 
the 
only 
accept 
state. 
Removing 
the 
state 
q 
(we 
can 
choose 
any 
state 
we 
like 
here), 
we 
can 
take 
a{p,q,r} 
= 
a{p,r} 
+ 
a{p,r}(a{p,r})*a{p,r} 
pp 
pp 
pq 
qq 
qp' 
Looking 
at 
the 
automaton, 
the 
only 
paths 
going 
from 
p 
to 
p 
and 
staying 
in 
the 
states 
{p, 
r} 
are 
paths 
going 
around 
the 
single 
loop 
labeled 
0 
from 
p 
to 
p 
some 
finite 
number 
of 
timesj 
thus 
we 
can 
take 
",{p,r} 
= 
0* 
..... 
pp 
. 
By 
similar 
informal 
reasoning, 
we 
can 
take 
a{p,r} 
= 
0*1 
pq 
, 
a{p,r} 
= 
E 
+ 
01 
+ 
000*1 
qq 
==E+O(E+OO*)1 
== 
E 
+ 
00*1, 
= 
000*. 
Thus 
we 
can 
take 
= 
0* 
+ 
O*I(E 
+ 
00*1)*000*. 
This 
is 
matched 
by 
the 
set 
of 
all 
strings 
accepted 
by 
the 
automaton. 
We 
can 
further 
simplify 
the 
expression 
using 
the 
algebraic 
laws 
(9.1) 
through 
(9.18): 
0* 
+ 
O*l{E 
+ 
00*1)*000* 
== 
0* 
+ 
0*1{00*1)*000* 
== 
f 
+ 
00* 
+ 
0*10(0*10)*00* 
== 
f 
+ 
(E 
+ 
0*10(0*10)*)00* 
== 
E 
+ 
(0*10)*00* 
== 
f 
+ 
(0*10)*0*0 
== 
f 
+ 
(0 
+ 
10)*0 
by 
(9.17) 
by 
(9.10) 
and 
(9.14) 
by 
(9.8) 
by 
(9.10) 
by 
(9.18) 
by 
(9.15). 
o 

_____________________________________________
54 
Lecture 
9 
Historical 
Notes 
Kleene 
[70) 
proved 
that 
deterministic 
finite 
automata 
and 
regular 
sions 
are 
equivalent. 
A 
shorter 
proofwas 
given 
by 
McNaughton 
and 
Yamada 
[85). 
The 
relations 
hip 
betweed 
right-
and 
left-linear 
grammars 
and 
regular 
sets 
(Homework 
5, 
Exercise 
1) 
was 
observed 
by 
Chomsky 
and 
Miller 
[21). 

_____________________________________________
Supplementary 
Lecture 
A 
Kleene 
Algebra 
and 
Regular 
Expressions 
In 
Lecture 
9, 
we 
gave 
a 
combinatorial 
proof 
that 
every 
finite 
automaton 
haz 
an 
equivalent 
regular 
expression. 
Here 
is 
an 
algebraic 
proof 
that 
generalizes 
that 
argument. 
It 
is 
worth 
looking 
at 
because 
it 
intro 
duces 
the 
notion 
of 
Kleene 
algebra· 
and 
the 
use 
of 
matrices. 
We 
will 
show 
how 
to 
use 
matrices 
and 
Kleene 
algebra 
to 
solve 
systems 
of 
linear 
equations 
involving 
sets 
01 
strings. 
Kleene 
algebra 
is 
named 
after 
Stephen 
C. 
Kleene, 
who 
invented 
the 
regulal 
sets 
[70J. 
Kleene 
Algebra 
We 
have 
already 
observed 
in 
Lecture 
9 
that 
the 
set 
operations 
U, 
" 
and 
* 
on 
subsets 
of 
E*, 
along 
with 
the 
distinguished 
subsets 
{(} 
and 
{f}, 
isfy 
certain 
important 
algebraic 
properties. 
These 
were 
listed 
in 
Lecture 
9, 
axioms 
(9.1) 
through 
(9.13). 
Let 
us 
call 
any 
algebraic 
structure 
satisfying 
these 
properties 
a 
Kleene 
algebra. 
In 
general, 
a 
Kleene 
algebra 
1C 
consists 
of 
a 
nonempty 
set 
with 
two 
distinguished 
constants 
0 
and 
1, 
two 
binary 
operations 
+ 
and 
. 
(usually 
omitted 
in 
expressions), 
and 
a 
unary 
operation 
* 
satisfying 
the 
following 
axioms. 
a 
+ 
(b 
+ 
c) 
= 
(a 
+ 
b) 
+ 
c 
associativity 
of 
+ 
a 
+ 
b 
= 
b 
+ 
a 
commutativity 
of 
+ 
(A.1) 
(A.2) 

_____________________________________________
56 
Supplementary 
Lecture 
A 
a+a 
= 
a 
a+O=a 
a{bc) 
= 
(ab)c 
al 
= 
la 
= 
a 
aO 
= 
Oa=O 
a(b 
+ 
c) 
= 
ab 
+ 
ac 
(a+b)c=ac+bc 
1 
+aa* 
= 
a* 
1 
+a*a 
= 
a* 
b+ac 
$ 
c::} 
a*b 
$c 
b 
+ 
ca 
$ 
c 
::} 
00* 
$ 
c 
idempotenee 
of 
+ 
o 
is 
an 
identity 
for 
+ 
associativity 
of 
. 
1 
is 
an 
identity 
for 
. 
o 
is 
an 
annIhilator 
for 
. 
distributivity 
distributivity 
In 
(A.12) 
and 
(A.13), 
$ 
refers 
to 
the 
naturally 
defined 
order 
In 
21::*, 
$ 
is 
just 
set 
inclusion 
S;. 
(A.3) 
(AA) 
(A.5) 
(A.6) 
(A.7) 
(A.8) 
(A.9) 
(A.I0) 
(A.l1) 
(A.12) 
(A.13) 
Axioms 
(A.l) 
through 
(A.9) 
diseuss 
the 
properties 
of 
addition 
and 
plication 
in 
a 
Kleene 
algebra. 
These 
properties 
are 
the 
same 
as 
those 
of 
ordinary 
addition 
and 
multiplieation, 
with 
the 
addition 
of 
the 
idempotenee 
axiom 
(A.3). 
These 
axioms 
ean 
be 
summed 
upbriefly,by 
saying 
that 
Je 
is 
an 
idempotent 
semiring. 
The 
remaining 
axioms 
(A.I0) 
through 
(A.13) 
diseuss 
the 
properties 
of 
the 
operator 
*. 
They 
say 
essentially 
that 
* 
behaves 
like 
the 
asterate 
operator 
on 
sets 
of 
strings 
or 
the 
reflexive 
transitive 
closure 
operator 
on 
binary 
relations. 
It 
follows 
quite 
easily 
from 
the 
axioms 
that 
$ 
is 
a 
partial 
order; 
that 
is, 
it 
is 
reflexive 
(a 
$ 
a), 
transitive 
(a 
$ 
band 
b 
$ 
c 
imply 
a 
$ 
c), 
and 
antisymmetrie 
(a 
$ 
band 
b 
$ 
a 
imply 
a 
= 
b). 
Moreover, 
a 
+ 
b 
is 
the 
least 
upper 
bound 
of 
a 
and 
b 
with 
respeet 
to 
$. 
All 
the 
operators 
are 
monotone 
with 
respeet 
to 
$; 
in 
other 
words, 
if 
a 
$ 
b, 
then 
ac 
$ 
bc, 
ca 
$ 
cb, 
a+c 
$ 
b+c, 
and 
a* 
$ 
b*. 
By 
(A.I0) 
and 
distributivity, 
we 
have 
b 
+ 
aa*b 
$ 
a*b, 
which 
says 
that 
a*b 
satisfies 
the 
inequality 
b 
+ 
ac 
$ c 
when 
substituted 
for 
c. 
The 
implieation 
(A.12) 
says 
that 
a*b 
is 
the 
$-least 
element 
of 
Je 
for 
which 
this 
is 
true. 
It 
follows 
that 
Lemma 
A.l 
In 
any 
Kleene 
algebra, 
a*b 
is 
the 
$-least 
solution 
0/ 
the 
equation 
x 
= 
ax+b. 
ProoJ. 
Miseellaneous 
Exercise 
21. 
o 

_____________________________________________
Kleene 
Algebra 
and 
Regular 
Expressions 
57 
Instead 
of 
(A.12) 
and 
(A.13), 
we 
might 
take 
the 
equivalent 
axioms 
ca 
c 
=? 
ca* 
c 
(see 
Miscellaneous 
Exercise 
22). 
(A.14) 
(A.15) 
Here 
are 
some 
typical 
theorems 
of 
Kleene 
algebra. 
These 
can 
be 
derived 
by 
purely 
equational 
reasoning 
from 
the 
axioms 
above 
(Miscellaneous 
Exercise 
20). 
a*a* 
= 
a* 
a** 
= 
a* 
(a*b)*a* 
= 
(a 
+ 
b)* 
denesting 
rule 
a( 
ba) 
* 
= 
(ab) 
* 
a 
shifting 
rule 
a* 
= 
(aa)* 
+ 
a(aa)* 
(A.16) 
(A.17) 
Equa.tions 
(A.16) 
and 
(A.17), 
the 
denesting 
rule 
and 
the 
shifting 
rule, 
spectively, 
turn 
out 
to 
be 
particularly 
useful 
in 
simplifying 
regular 
sions. 
The 
family 
2
E
* 
of 
all 
subsets 
ofE* 
with 
constants 
0 
and 
{•} 
and 
operations 
U,', 
and 
* 
forms 
a 
Kleene 
algebra, 
as 
does 
the 
family 
of 
all 
regular 
subsets 
of 
E* 
with 
the 
same 
operations. 
As 
mentioned 
in 
Lecture 
9, 
it 
can 
be 
shown 
that 
an 
equation 
a 
= 
ß 
is 
a 
theorem 
of 
Kleene 
algebra, 
that 
is, 
is 
derivable 
from 
axioms 
(A.1) 
through 
(A.13), 
if 
and 
only 
if 
a 
and 
ß 
are 
equivalent 
as 
regular 
expressions 
[73]. 
Another 
example 
of 
a 
Kleene 
algebra 
is 
the 
family 
of 
all 
binary 
relations 
on 
a 
set 
X 
with 
the 
empty 
relation 
for 
0, 
the 
identity 
relation 
L 
{(u,u) 
lu 
E 
X} 
for 
1, 
U 
for 
+, 
relational 
composition 
R 
0 
S 
{(u,w) 
13v 
E 
X 
(u,v) 
E 
Rand 
(v,w) 
E 
S} 
for 
" 
and 
reflexive 
transitive 
closure 
for 
"': 
R* 
U 
Rn, 
where 
R
o 
def 
= 
L, 
R
n
+
1 
Rn 
0 
R. 
Still 
another 
example 
is 
the 
family 
of 
n 
X 
n 
Boolean 
matrices 
with 
the 
zero 
matrix 
for 
0, 
the 
identity 
matrix 
for 
1, 
componentwise 
Boolean 
matrix 

_____________________________________________
58 
Supplementary 
Lecture 
A 
addition 
and 
multiplication 
for 
+ 
and 
" 
respectively, 
and 
reflexive 
transitive 
closure 
for 
*. 
This 
is 
really 
the 
same 
as 
the 
previous 
example, 
where 
the 
set 
X 
has 
n 
elements. 
Matrices 
Given 
an 
arbitrary 
Kleene 
algebra 
K., 
the 
set 
of 
n 
x 
n 
matricesover 
K., 
which 
we 
will 
denote 
by 
M(n, 
K.), 
also 
forms 
a 
Kleene 
algebra. 
In 
M(2, 
K.), 
for 
example, 
the 
identity 
elements 
for 
+ 
and 
. 
are 
respectively, 
and 
the 
operations 
+, 
" 
and 
* 
are 
given 
by 
]
+[e 
9 h 
c+g 
{] 
[ 
ae 
+ 
bg 
-
ce 
+dg 
b+ 
I ] 
d+h 
' 
al 
+ 
bh 
] 
cl 
+dh 
' 
and 
(a 
+ 
bd*c)* 
(d 
+ 
ca*b)*ca* 
(a 
+ 
bd*c)*bd* 
] 
(d 
+ 
ca*b)* 
, 
(A.18) 
respectively. 
In 
general, 
+ 
and 
. 
in 
M(n, 
K.) 
are 
ordinary 
matrix 
addition 
and 
multiplication, 
respectively, 
the 
identity 
for 
+ 
is 
the 
zero 
matrix, 
and 
the 
identity 
for 
. 
is 
the 
identity 
matrix. 
To 
define 
E* 
for 
a 
given 
n 
x 
n 
matrix 
E 
over 
K., 
we 
proceed 
by 
induction 
on 
n. 
If 
n 
= 
1, 
the 
structure 
M(n, 
K.) 
is 
just 
K., 
so 
we 
are 
done. 
For 
n 
> 
1, 
break 
E 
up 
into 
four 
submatrices 
such 
that 
A 
and 
D 
are 
square, 
say 
m x m 
and 
(n 
-
m) 
x 
(n 
-
m), 
spectively. 
By 
the 
induction 
hypothesis, 
M(m, 
K.) 
and 
M(n 
-
m, 
K.) 
are 
Kleene 
algebras, 
so 
it 
makes 
sense 
to 
form 
the 
asterates 
ofany 
m x m 
or 
(n 
-
m) 
x 
(n 
-
m) 
matrix 
over 
K., 
and 
these 
matrices 
will 
satisfy 
all 
the 
axioms 
for 
*. 
This 
allows 
us 
to 
define 
[ 
(A 
+ 
BD*C)* 
E* 
(D 
+ 
CA* 
B)*CA* 
Compare 
this 
definition 
to 
(A.18). 
(A 
+ 
BD*C)* 
BD* 
] . 
(D+CA*B)* 
(A.19) 

_____________________________________________
Kleene 
Algebra 
and 
Regular 
Expressions 
59 
The 
expressions 
on 
the 
right-hand 
sides 
of 
(A.18) 
and 
(A.19) 
may 
look 
like 
they 
were 
pulled 
out 
of 
thin 
air. 
Where 
did 
we 
get 
them 
from? 
The 
answer 
will 
co 
me 
to 
you 
if 
you 
stare 
really 
hard 
at 
the 
following 
mandala: 
b 
aCX_X)d 
c 
It 
can 
be 
shown 
that 
M(n, 
K) 
is 
a 
Kleene 
algebra 
under 
these 
definitions: 
Lemma 
A.2 
1/ 
K 
is 
a 
Kleene 
algebra, 
then 
so 
is 
M( 
n, 
K). 
Proof. 
Miscellaneous 
Exercise 
24. 
We 
must 
verify 
that 
M(n, 
K) 
satisfies 
the 
axioms 
(A.l) 
through 
(A.13) 
of 
Kleene 
algebra 
assuming 
only 
that 
K 
does. 
0 
If 
E 
is 
a 
matrix 
of 
indeterminates, 
and 
if 
the 
inductive 
construction 
of 
E* 
given 
in 
(A.19) 
is 
carried 
out 
symbolically, 
then 
the 
entries 
of 
the 
resulting 
matrix 
E* 
will 
be 
regular 
expressions 
in 
those 
indeterminates. 
This 
struction 
generalizes 
the 
construction 
of 
Lecture 
9, 
which 
corresponds 
to 
the 
case 
m 
= 
1. 
Systems 
of 
Linear 
Equations 
It 
is 
possible 
to 
solve 
systems 
of 
linear 
equations 
over 
a 
Kleene 
algebra 
K. 
Suppose 
we 
are 
given 
a 
set 
of 
n 
variables 
Xl, 
... 
,X
n 
ranging 
over 
K 
and 
a 
system 
of 
n 
equations 
of 
the 
form 
Xi 
= 
ailXI 
+ 
... 
+ 
ainXn 
+ 
bi, 
1 
S 
i 
S 
n, 
where 
the 
aij 
and 
bi 
are 
elements 
of 
K. 
Arranging 
the 
aij 
in 
an 
n 
x 
n 
matrix 
A, 
the 
bi 
in 
a 
vector 
b 
of 
length 
n, 
and 
the 
Xi 
in 
a 
vector 
X 
of 
length 
n, 
we 
obtain 
the 
matrix-vector 
equation 
X 
= 
Ax 
+ 
b. 
(A.20) 
It 
is 
now 
not 
hard 
to 
show 
Theorem 
A.3 
The 
vector 
A*b 
is 
a 
solution 
to 
(A.20); 
moreover, 
it 
is 
the 
S-least 
solution 
in 
Kn. 
Proof. 
Miscellaneous 
Exercise 
25. 
o 
N 
ow 
we 
use 
this 
to 
give 
a 
regular 
expression 
equivalent 
to 
an 
arbitrarily 
given 
deterministic 
finite 
automaton 
M 
= 
(Q, 
0, 
s, 
F). 

_____________________________________________
60 
Supplementary 
Lecture 
A 
Assume 
without 
loss 
of 
generality 
that 
Q 
= 
{I, 
2 
.... 
, 
n}. 
For 
each 
q 
E 
Q, 
let 
X
q 
denote 
the 
set 
of 
strings 
in 
that 
would 
be 
accepted 
by 
M 
if 
q 
were 
the 
start 
statej 
that 
is, 
X
q 
{x 
E 
18(q,x) 
E 
F}. 
The 
X
q 
satisfy 
the 
following 
system 
of 
equations: 
if 
q 
rj. 
F, 
if 
q 
E 
F. 
x 
-{ 
L:aEE 
aX
6
(q,a) 
q 
-
L:aEE 
aX
c5
(q,a) 
+ 
1 
Moreover, 
the 
X
q 
give 
the 
least 
solution 
with 
respect 
to 
As 
above, 
these 
equations 
can 
be 
arranged 
in 
a 
single 
matrix-vector 
equation 
of 
the 
form 
x 
= 
AX 
+b, 
(A.21) 
where 
A 
is 
an 
n 
x 
n 
matrix 
containing 
sums 
Qf 
elements 
of 
b 
is 
a 
0-1 
vector 
of 
length 
n, 
and 
X 
is 
a 
vector 
consisting 
of 
Xl, 
... 
, 
X
n
. 
The 
vector 
Xis 
the 
least 
solution 
of 
(A.21). 
By 
Theorem 
A.3, 
X 
= 
A*b. 
Compute 
the 
matrix 
A* 
symbolically 
according 
to 
(A.I9), 
so 
that 
its 
entries 
are 
regular 
expressions, 
then 
multiply 
by 
b. 
A 
regular 
expression 
for 
L(M) 
can 
then 
be 
read 
off 
from 
the 
8th 
entry 
of 
A*b, 
where 
8 
is 
the 
start 
state. 
of 
M. 
Historical 
Notes 
Salomaa 
[108] 
gave 
the 
first 
complete 
axiomatization 
of 
the 
algebra 
of 
lar 
sets. 
The 
algebraic 
theory 
was 
developed 
extensively 
in 
the 
monograph 
of 
Conway' 
[27]. 
Many 
others 
have 
contributed 
t0 
the 
theory, 
including 
Redko 
[103J, 
Backhouse 
[6], 
Bloom 
and 
Esik 
[10], 
Boffa 
[11, 
12], 
Gecseg 
and 
Peak 
[41], 
Krob 
[74], 
Kuich 
and 
Salomaa 
[76], 
and 
Salomaa 
and 
tola 
[109]. 
The 
definition 
of 
Kleene 
algebra 
and 
the 
complete 
axiomatization 
given 
here 
is 
from 
Kozen 
[73]. 

_____________________________________________
Lecture 
10 
Homomorph 
isms 
A 
homomorpMsm 
is 
a 
map 
h 
: 
l:* 
-
r* 
such 
that 
for 
all 
,x, 
y 
E 
l:*, 
h(xy) 
= 
h(x)h(y), 
h(f) 
= 
f. 
Actually, 
(10.2) 
is 
a 
consequence 
of 
(10.1): 
Ih(f)1 
= 
Ih(ff)1 
= 
Ih(f)h(f)1 
= 
Ih(f)1 
+ 
Ih(f)lj 
(10.1 
) 
(10.2) 
subtracting 
Ih(f)1 
from 
both 
sides, 
we 
have 
Ih(f)1 
= 
0, 
therefore 
h(f) 
= 
f. 
It 
follows 
from 
these 
properties 
that 
any 
homomorphism 
defined 
on 
l:* 
is 
uniquely 
determined 
by 
its 
values 
on 
For 
example, 
if 
h(a) 
= 
cec 
and 
h(b) 
= 
dd, 
then 
h(abaab) 
= 
h(a)h(b)h(a)h(a)h(b) 
= 
cccddceccccdd. 
Moreover, 
any 
map 
h 
: 
-
r* 
extends 
uniquely 
by 
induction 
to 
a 
momorphism 
defined 
on 
all 
of 
Therefore, 
in 
order 
to 
specify 
a 
morphism 
completely, 
we 
need 
only 
say 
what 
values 
it 
takes 
on 
elements 
of 

_____________________________________________
62 
Lecture 
10 
If 
A 
define 
h(A) 
{h(x) 
I 
xE 
A} 
r*, 
and 
if 
B 
r*, 
define 
h-
1
(B) 
{X 
I 
h(x) 
E 
B} 
The 
set 
h(A) 
is 
called 
the 
image 
of 
A 
under 
h, 
and 
the 
set 
h-
1
(B) 
is 
called 
the 
preimage 
of 
B 
under 
h. 
We 
will 
show 
two 
useful 
closure 
properties 
of 
the 
regular 
sets: 
any 
morphic 
image 
or 
homomorphic 
preimage 
of 
a 
regular 
set 
is 
regular. 
Theorem 
10.1 
Let 
h 
: 
--+ 
r* 
be 
a 
homomorphism. 
If 
B 
r* 
is 
regular, 
then 
so 
is 
its 
preimage 
h-
1
(B) 
under 
h. 
Proof. 
Let 
M 
= 
(Q, 
r, 
0, 
s, 
F) 
be 
a 
DFA 
such 
that 
L(M) 
= 
B. 
Create 
a 
new 
DFA 
M' 
= 
(Q, 
0', 
s, 
F) 
for 
h-
1
(B) 
as 
follows. 
The 
set 
of 
states, 
start 
state, 
and 
final 
states 
of 
M' 
are 
the 
same 
as 
in 
M. 
The 
input 
alphabet 
is 
instead 
of 
r. 
The 
transition 
function 
0' 
is 
defined 
by 
O'(q,a) 
d,g 
6(q, 
h(a)). 
Note 
that 
we 
have 
to 
use 
6 
on 
the 
right-hand 
side, 
since 
h(a) 
need 
not 
be 
a 
single 
letter. 
N 
ow 
it 
follows 
by 
induction 
on 
lxi 
that 
for 
all 
x 
E 
6'(q,x) 
= 
6(q,h(x)). 
For 
the 
basis 
x 
= 
f, 
using 
(10.1), 
6'(q,f) 
= 
q 
= 
6(q,f) 
= 
6(q,h(f)) 
For 
the 
induction 
step, 
assume 
that 
6'(q,x) 
= 
6(q,h(x)). 
Then 
6' 
(q, 
xa) 
= 
0' 
(6' 
(q, 
x), 
a) 
definition 
of 
6' 
= 
0'(6(q,h(x)),a) 
induction 
hypothesis 
= 
6(6(q, 
h(x)), 
h(a)) 
definition 
of 
0' 
= 
6(q, 
h(x)h(a)) 
Homework 
1, 
Exercise 
3 
(10.3) 
= 
6(q, 
h(xa)) 
property 
(10.2) 
of 
homomorphisms. 
Now 
we 
can 
use 
(10.3) 
to 
prove 
that 
L(M') 
= 
h-
1
(L(M)). 
For 
any 
x 
E 
xE 
L(M') 
<==> 
6'(s, 
x) 
E 
F 
definition 
of 
acceptance 
<==> 
6(s,h(x)) 
E 
F 
by 
(10.3) 
<==> 
h( 
x) 
E 
L( 
M) 
definition 
of 
acceptance 
<==> 
xE 
h-
1
(L(M)) 
definition 
of 
h-
1
(L(M)). 
0 

_____________________________________________
Homomorphisms 
63 
Theorem 
10.2 
Let 
h 
: 
E* 
-. 
r* 
be 
a 
homomorphism. 
11 
A 
E* 
is 
regular, 
then 
so 
is 
its 
image 
h(A) 
under 
h. 
Proof. 
For 
this 
proof, 
we 
will 
use 
regular 
expressions. 
Let 
0 
be 
a 
regular 
expression 
over 
E 
such 
that 
L(o) 
= 
A. 
Let 
0' 
be 
the 
regular 
expression 
obtained 
by 
replacing 
each 
letter 
a 
E 
E 
appearing 
in 
0 
with 
the 
string 
h(a) 
E 
r*. 
For 
example, 
if 
h(a) 
= 
ccc 
and 
h(b) 
= 
dd, 
then 
«a 
+ 
b)* 
ab)' 
= 
(ccc 
+ 
dd)*cccdd. 
Formally, 
0' 
is 
defined 
by 
induction: 
a' 
= 
h(a), 
a 
E 
E, 
9)' 
= 
9), 
(ß 
+ 
,)' 
= 
ß' 
+ 
,', 
(ß,)' 
= 
ß',', 
ß*' 
= 
ß'* 
We 
claim 
that 
for 
any 
regular 
expression 
ß 
over 
E, 
L(ß') 
= 
h(L(ß)); 
(10.4) 
in 
particular, 
L(o') 
= 
h(A). 
This 
can 
be 
proved 
by 
induction 
on 
the 
ture 
of 
ß. 
To 
do 
this, 
we 
will 
need 
two 
facts 
about 
homomorphisms: 
for 
any 
pair 
of 
subsets 
C, 
D 
E* 
and 
any 
family 
of 
subsets 
Ci 
E*, 
i 
E 
1, 
h(CD) 
= 
h(C)h(D), 
h(U 
Ci) 
= 
U 
h(Ci)' 
iEI 
iEI 
To 
prove 
(10.5), 
h(CD) 
=.{h(w) 
Iw 
E 
CD} 
= 
{h(yz) 
I 
y 
E 
C, 
z 
E 
D} 
= 
{h(y)h(z) 
I 
y 
E 
C, 
z 
E 
D} 
= 
{uv 
I 
U 
E 
h(C), 
v 
E 
h(D)} 
= 
h(C)h(D). 
To 
prove 
(10.6), 
h(UC
i
) 
= 
{h(w) 
Iw 
E 
UC
i
} 
i 
i 
= 
{h( 
w) 
I 
3i 
w 
E 
Ci} 
= 
U{ 
h( 
w) 
I 
w 
E 
Cd 
i 
(10.5) 
( 
10.6) 

_____________________________________________
64 
Lecture 
10 
Now 
we 
prove 
(10.4) 
br 
induction. 
There 
are 
two 
base 
cases: 
L(a') 
= 
= 
{h(a)} 
= 
h({a}) 
= 
h(L(a)) 
and 
L(ß') 
= 
L(ß) 
= 
0 
= 
h(0) 
= 
h(L(ß)). 
The 
case 
of 
E 
is 
covered 
by 
the 
other 
cases, 
since 
E 
= 
0*. 
There 
are 
three 
induction 
cases, 
one 
for 
each 
of 
the 
operators 
+, 
" 
and 
*. 
For 
+, 
L( 
(ß 
+ 
"{ 
)') 
= 
L(ß' 
+ 
"{') 
= 
L(ß') 
U 
L( 
"{') 
= 
h(L(ß)) 
U 
h(L("t)) 
= 
h(L(ß) 
U 
L("t)) 
= 
h(L{ß 
+ 
"{)) 
definition 
of 
' 
definition 
of 
+ 
induction 
hypothesis 
property 
(10.6) 
definition 
of 
+. 
The 
proof 
for 
. 
is 
similar, 
using 
property 
(10.5) 
instead 
of 
(10.6). 
Finally, 
for 
*, 
L(ß*') 
= 
L(ß'*) 
= 
L{ß')* 
= 
h(L(ß))* 
= 
U 
h(L(ß)t 
definition 
of 
' 
definition 
of 
regular 
expression 
operator 
* 
induction 
hypothesis 
definition 
of 
set 
operator 
* 
= 
U 
h{L{ßt) 
property 
(10.5) 
= 
h( 
U 
L(ßt) 
property 
(10.6) 
n2:0 
= 
h(L(ß)*) 
= 
h(L(ß*)) 
definition 
of 
set 
operator 
* 
defillition 
of 
regular 
expression 
operator 
* 
o 
Warning: 
It 
is 
not 
true 
that 
A 
is 
regular 
whenever 
h{A) 
iso 
This 
is 
not 
what 
Theorem 
10.1 
says. 
We 
.will 
show 
later 
that 
the 
set 
{anb
n 
I 
n 
O} 
is 
not 
regular, 
but 
the 
image 
ofthis 
set 
under 
the 
homomorphism 
h{a) 
= 
h(b) 
= 
a 
is 
the 
regular 
set 
{an 
In 
is 
even}. 
The 
preimage 
h-
1
({a
n 
I 
n 
is 
even}) 
is 
not 
{anb
n 
I 
n 
O}, 
but 
{x 
E 
{a,b}* 
Ilxl 
is 
even}, which 
is 
regular. 

_____________________________________________
Homomorphisms 
6:) 
Automata 
with 
•-transitions 
Here 
is 
an 
example 
of 
how 
to 
use 
homomorphisms 
to 
give 
a 
clean 
treatment 
of 
•-transitions. 
Define 
an 
NFA 
with 
•-transitions 
to 
be 
a 
structure 
M 
= 
(Q, 
S, 
F) 
such 
that 
• 
is 
a 
special 
symbol 
not 
in 
and 
M• 
= 
(Q, 
S, 
F) 
is 
an 
ordinary 
NFA 
over 
the 
alphabet 
U 
{•}. 
We 
define 
acceptance 
for 
automata 
with 
•-transitions 
as 
folIows: 
for 
any 
x 
E 
M 
accepis 
x 
if 
there 
exists 
y. 
E 
U 
{•})* 
such 
that 
Ł 
M• 
accepts 
y 
under 
the 
ordinary 
definition 
of 
acceptance 
for 
NFAs, 
and 
Ł 
1: 
is 
obtained 
from 
y 
by 
erasing 
all 
occurrences 
of 
the 
symbol 
•; 
that 
is, 
x 
= 
h(y), 
where 
h: 
{•})* 
-4 
is 
the 
homomorphism 
detined 
by 
( ) 
def 
ha 
= 
a, 
a 
E 
) 
def 
h(e 
= 
•. 
In 
other 
words, 
This 
definition 
and 
the 
definition 
involving 
•-closure 
described 
in 
Lecture 
6 
are 
equivalent 
(Miscellaneous 
Exercise 
10). 
It 
is 
immediate 
from 
this 
definition 
and 
Theorem 
10.2 
that 
the 
set 
accepted 
by 
any 
finite 
automat0:-. 
with 
•-transitions 
is 
regular. 
Hamming 
Distance 
Here 
is 
another 
example 
of 
the 
use 
of 
homomorphisms. 
We 
can 
use 
them 
to 
give 
slick 
solutions 
to 
Exercise 
3 
of 
Homework 
2 
and 
Miscellaneous 
Exercise 
8, 
the 
problems 
involving 
Hamming 
distance. 
Let 
= 
{O, 
I} 
and 
consider 
the 
alphabet 
The 
elements 
of 
x 
are 
ordered 
pairs, 
but 
we 
write 
the 
components 
one 
on 
top 
of 
the 
other. 
Let 
top 
: 
X 
-4 
and 
bottom 
: 
X 
---T 
I; 
be 
the 

_____________________________________________
66 
Lecture 
10 
two 
projections 
These 
maps 
extend 
uniquely 
to 
homomorphisms 
(E 
x 
E)* 
-+ 
E*, 
which 
we 
also 
denote 
by 
top 
and 
bottom. 
For 
example, 
top 
(CililiIQJ) 
= 
0010, 
[ili]IIT] 
bottom 
(CililiIQJ) 
= 
0111. 
Thus 
we 
can 
think 
of 
strings 
in 
(E 
x 
E)* 
as 
consisting 
of 
two 
tracks, 
and 
the 
homomorphisms 
top 
and 
bottom 
give 
the 
contents 
of 
the 
top 
and 
bottom 
track, 
respectively. 
For 
fixed 
k, 
let 
D" 
be 
the 
set 
of 
all 
strings 
in 
(E 
x 
E)* 
containing 
no 
more 
than 
k 
occurrences 
of 
This 
is 
certainly 
a 
regular 
set. 
Note 
also 
that 
D" 
= 
{x 
E 
(E 
x 
E)* 
I 
H(top(x), 
bottom(x)) 
$ 
k}, 
where 
H 
is 
the 
Hamming 
distance 
function. 
Now 
take 
any 
regular 
set 
A 
E*, 
and 
consider 
the 
set 
(10.7) 
Believe 
it 
or 
not, 
this 
set 
is 
exactly 
N,,(A), 
the 
set 
of 
strings 
in 
E* 
of 
Hamming 
distance 
at 
most 
k 
from 
some 
string 
in 
A. 
The 
set 
bottom-1(A) 
is 
the 
set 
of 
strings 
whose 
bottom 
track 
is 
in 
A; 
the 
set 
bottom-
1 
(A) 
n 
D" 
is 
the 
set 
of 
strings 
whose 
bottom 
track 
is 
in 
A 
and 
whose 
top 
track 
is 
of 
Hamming 
distance 
at 
most 
k 
from 
the 
bottom 
track; 
and 
the 
set 
(10.7) 
is 
the 
set 
of 
top 
tracks 
of 
all 
such 
strings. 
Moreover, 
the 
set 
(10.7) 
is 
a 
regular 
set, 
because 
the 
regular 
sets 
are 
closed 
under 
intersection, 
homomorphic 
image, 
and 
homomorphic 
preimage. 

_____________________________________________
Lecture 
11 
Limitations 
of 
Finite 
Automata 
We 
have 
studied 
what 
finite 
automata 
can 
dOj 
let's 
see 
what 
they 
cannot 
do. 
The 
canonical 
example 
of 
a 
nonregular 
set 
(one 
accepted 
by 
no 
finite 
automaton) 
is 
B 
= 
{anb
n 
In?: 
O} 
= 
{f,ab,aabb,aaabbb,aaaabbbb, 
... 
}, 
the 
set 
of 
all 
strings 
of 
the 
form 
a*b* 
with 
equally 
many 
a's 
and 
b's. 
Intuitively, 
in 
order 
to 
accept 
the 
set 
B, 
an 
automaton 
scanning 
astring 
of 
the 
form 
a*b* 
would 
have 
to 
remember 
when 
passing 
the 
center 
point 
between 
the 
a'S 
and 
b's 
how 
many 
a's 
it 
has 
seen, 
since 
it 
would 
have 
to 
compare 
that 
with 
the 
number 
of 
b's 
and 
accept 
iff 
the 
two 
numbers 
are 
the 
same. 
aaaaaaaaaaaaaaaaaaaaaaaaaaaaab 
bbbbbbbb bbbbbbbb 
bb 
bb'bbbb 
bbb 
b 
1 
q 
Moreover, 
it 
would have 
to 
do 
this 
for 
arbitrarily 
long 
strings 
of 
a's 
and 
b's, 
much 
longer 
than 
the 
number 
of 
states. 
This 
is 
an 
unbounded 
amount 
of 
information, 
andthere 
is 
no 
way 
it 
can 
remember 
this 
with 
only 
finite 
memory. 
All 
it 
"knows" 
at 
that 
point 
is 
represented 
in 
the 
state 
q 
it 
is 
in, 
which 
is 
only 
a 
finite 
amount 
of 
information. 
You 
might 
at 
first 
think 
there 
may 
be 
some 
clever 
strategy, 
such 
as 
counting 
mod 
3, 
5, 
and 
7, 
or 
something 
similar. 
But 
any 
such 
attempt 
is 
doomed 
to 
failure: 
you 
cannot 

_____________________________________________
68 
Lecture 
11 
distinguish 
between 
infinitely 
many 
different 
cases 
with 
only 
finitely 
many 
states. 
This 
is 
just 
an 
informal 
argument. 
But 
we 
can 
easily 
give 
a 
formal 
proof 
by 
contradiction 
that 
B 
is 
not 
regular. 
Assuming 
that 
B 
were 
regular, 
there 
would 
be 
a 
DFA 
M 
such 
that 
L(M) 
= 
B. 
Let 
k 
be 
the 
number 
of 
states 
of 
this 
alleged 
M. 
Consider 
the 
action 
of 
M 
on 
input 
anb
n
, 
where 
n 
» 
k. 
It 
starts 
in 
its 
start 
state 
s. 
Since 
the 
string 
anb
n 
is 
in 
B, 
M 
must 
accept 
it, 
thus 
M 
must 
be 
in 
some 
final 
state 
r 
after 
scanning 
anb
n
. 
'Y' 
'V' 
n 
n 
i 
i 
s 
r 
Since 
n 
» 
k, 
by 
the 
pigeonhole 
principle 
there 
must 
be 
some 
state 
p 
that 
the 
automaton 
enters 
more 
than 
on 
ce 
while 
scanning 
the 
initial 
sequence 
of 
a's. 
Break 
up 
the 
string 
anb
n 
into 
three 
pieces 
u, 
v, 
w, 
where 
v 
is 
the 
string 
of 
a's 
scanned 
between 
two 
occurrences 
of 
the 
state 
p, 
as 
illustrated 
in 
the 
following 
picture: 
aaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbb 
, 
v 
'" 
' 
i 
s 
u v 
W 
i 
p 
T 
p 
Let 
j 
= 
lvi> 
O. 
In 
this 
example, 
j 
= 
7. 
Then 
6(s,u) 
=p, 
6(p, 
v) 
= 
p, 
6(p,w) 
= 
rE 
F. 
i 
r 
The 
string 
v 
could 
be 
deleted 
and 
the 
resulting 
string 
would 
be 
erroneously 
accepted: 
8(s,uw) 
= 
8(8(s,u),w) 
= 
8(p, 
w) 
=r 
E 
F. 
i 
s 
y 
y 
u 
w 
T 
p 
r 
It's 
erroneous 
because 
after 
deleting 
v, 
the 
number 
of 
a's 
is 
strictly 
less 
than 
the 
number 
of 
b's: 
uw 
= 
an-ibn 
E 
L(M), 
but 
uw 
f/. 
B. 
This 
contradicts 
our 
assumption 
that 
L(M) 
= 
B. 

_____________________________________________
Limitations 
of 
Finite 
Automata 
69 
We 
could 
also 
insert 
extra 
copies 
of 
v 
and 
the 
resulting 
string 
would 
be 
roneously 
accepted. 
For 
example, 
uv
3
w 
= 
a
n
+
2i
b
n 
is 
erroneously 
accepted: 
6(8, 
uvvvw) 
= 
6(6(6(6(6(8, 
u), 
v), v), v), 
w) 
= 
6(6(6(6(p, 
v), v), v), 
w) 
= 
6(6(6(p, 
v), 
v), 
w) 
= 
6(6(p,v),w) 
= 
6(p,w) 
=r 
E 
F. 
For 
another 
example 
of 
a 
nonregular 
set, 
consider 
C 
= 
{a
2n 
I 
n 
O} 
= 
{x 
E 
{a}* 
Ilxl 
is 
apower 
of 
2} 
= 
{a,a2,a4,a8,a16, 
... 
}. 
This 
set 
is 
also 
nonregular. 
Suppose 
(again 
for 
a 
contradiction) 
that 
L(M) 
= 
C 
for 
some 
DFA 
M. 
Let 
k 
be 
the 
nurnber 
of 
states 
of 
M. 
Let 
n 
:» 
k 
and 
consider 
the 
action 
of 
M 
on 
input 
a
2n 
E 
C. 
Since 
n:» 
k, 
by 
the 
pigeonhole 
principle 
the 
autornaton 
rnust 
repeat 
astate 
p 
while 
scanning 
the 
first 
n 
symbols 
of 
a
2n
Ł 
Thus 
2
n 
= i 
+ 
j 
+m 
for 
sorne 
i,j, 
m 
with 
0 
< 
j 
n 
and 
6(s,a
i
)=p, 
6(p,a
i
) 
= 
p, 
6(p,a
m
) 
= 
rE 
F. 
Y 
T 
1 
8 
1 
p 
j 
m 
T 
p 
T 
r 
As 
above, 
we 
could 
insert 
an 
extra 
a
i 
to 
get 
a
2n
+
i
, 
and 
this 
string 
would 
be 
erroneously 
accepted: 
6(8,a
2n
+
i
) 
= 
6(s,a
i
a
i
a
i
a
m
) 
= 
6(6(6(6(8, 
a
i
), 
a
i
), 
a
i
), 
a
m
) 
= 
6(6(6(p, 
a
i
), 
ai), 
= 
6(8(p,a
i
),a
m
) 
= 
8(p,a
m
) 
=r 
EF. 
This 
is 
erroneous 
because 
2
n 
+ 
j 
is 
not 
apower 
of 
2: 
2
n 
+ 
j 
2
n 
+n 

_____________________________________________
70 
Lecture 
11 
and 
2
n
+
1 
is 
the 
next 
power 
of 
2 
greater 
than 
2
n
Ł 
The 
Pumping 
Lemma 
We 
ean 
eneapsulate 
the 
arguments 
above 
in 
a 
general 
theorem 
ealled 
the 
pumping 
lemma. 
This 
lemma 
is 
very 
useful 
in 
proving 
sets 
nonregular. 
The 
idea 
is 
that 
whenever 
an 
automaton 
seans 
a 
long 
string 
(longer 
than 
the 
number 
of 
states) 
and 
aeeepts, 
there 
must 
be 
a 
repeated 
state, 
and 
extra 
eopies 
of 
the 
segment 
of 
the 
input 
between 
the 
two 
oceurrences 
of 
that 
state 
ean 
be 
inserted 
and 
the 
resulting 
string 
is 
still 
aeeepted. 
Theorem 
11.1 
(Pumping 
lemma) 
Let 
A 
be 
a 
regular 
set. 
Then 
the 
following 
property 
holds 
of 
A: 
(P) 
There 
exists 
k 
0 
such 
that 
for 
any 
strings 
x, 
y, 
z 
with. 
xyz 
E 
A 
and 
lyl 
k, 
there 
exist 
strings 
u,v,w 
such 
that 
y 
= 
uvw, 
V 
f 
•, 
and 
for 
all 
i 
0, 
the 
string 
xuviwz 
E 
A. 
Informally, 
if 
A 
is 
regular, 
then 
for 
any 
string 
in 
A 
and 
any 
sufficiently 
long 
substring 
y 
of 
that 
string, 
y 
has 
a 
non 
null 
substring 
v 
of 
whieh 
you 
can 
pump 
in 
as 
many 
eopies 
as 
you 
like 
and 
the 
resulting 
string 
is 
still 
in 
A. 
We 
have 
essentially 
already 
proved 
this 
theorem. 
Think 
of 
k 
as 
the 
number 
of 
states 
of 
a 
DFA 
aeeepting 
A. 
Sinee 
y 
is 
at 
least 
as 
long 
as 
the 
number 
of 
states, 
there 
must 
be 
a 
repeated 
state 
while 
seanning 
y. 
The 
string 
v 
is 
the 
substring 
between 
the 
two 
oeeurrenees 
of 
that 
state. 
We 
ean 
pump 
in 
as 
many 
eopies 
of 
v 
as 
we 
want 
(or 
delete 
v-this 
would 
be 
the 
case 
i 
= 
0), 
and 
the 
resulting 
string 
is 
still 
aeeepted. 
Games 
with 
the 
Demon 
The 
pumping 
lemma 
is 
often 
used 
to 
show 
that 
certain 
sets 
are 
nonregular. 
For 
this 
purpose 
we 
usually 
use 
it 
in 
its 
eontrapositive 
form: 
Theorem 
11.2 
(Pumping 
lemma, 
contrapositive 
form) 
Let 
A 
be 
a 
set 
of 
strings. 
Suppose 
that 
the 
lollowing 
property 
holds 
01 
A. 
(-,P) 
For 
all 
k 
0 
there 
exist 
strings 
x, 
y, 
z 
such 
that 
xyz 
E 
A, 
lyl 
k, 
and 
for 
all 
11., 
v, 
w 
with 
y 
= 
uvw 
and 
v 
f 
•, 
there 
exists 
an 
i 
0 
such 
that 
xuviwz 
A. 
Then 
A 
is 
not 
regular. 

_____________________________________________
Limitations 
of 
Finite 
Automata 
71 
To 
use 
the 
pumping 
lemma 
to 
prove 
that 
a 
given 
set 
A 
is 
nonregular, 
we 
need 
to 
establish 
that 
(..,P) 
holds 
of 
A. 
BeC'ause 
of 
the 
alternating 
"for 
all/there 
exists" 
form 
of 
(-,P), 
we 
can 
think 
of 
this 
as 
agame 
between 
you 
and 
ademon. 
You 
want 
to 
show 
that 
A 
is 
and 
the 
dem 
on 
wants 
to 
show 
that 
A 
is 
regular. 
The 
game 
proceeds 
as 
follows: 
1. 
The 
dem 
on 
picks 
k. 
(lf 
A 
really 
is 
regular, 
the 
demon's 
best 
strategy 
here 
is 
to 
pick 
k 
to 
be 
the 
number 
of 
states 
of 
a 
DFA 
for 
A.) 
2. 
You 
pick 
x, 
y, 
z 
such 
that 
xyz 
E 
A 
and 
lyl 
k. 
3. 
The 
demon 
picks 
u, 
v, 
w 
such 
that 
y 
= 
uvw 
and 
v 
i-
f. 
4. 
You 
pick 
i 
O. 
You 
win 
if 
xuviwz 
f/. 
A, 
and 
the 
demon 
wins 
if 
xuviwz 
E 
A. 
The 
property 
(..,P) 
for 
A 
is 
equivalent 
to 
saying 
that 
you 
have 
a 
winning 
strategy 
in 
this 
game. 
This 
means 
that 
by 
playing 
optimally, 
you 
can 
always 
win 
no 
matter 
what 
the 
demon 
does 
in 
steps 
1 
and 
3. 
If 
you 
can 
show 
that 
you 
have 
a 
winning 
strategy, 
you 
have 
essentially 
shown 
that 
the 
condition 
(..,P) 
holds 
for 
A, 
therefore 
by 
Theorem 
11.2, 
A 
is 
not 
regular. 
We 
have 
thus 
reduced 
the 
problem 
of 
showing 
that 
a 
given 
set. 
is 
regular 
to 
the 
puzzle 
of 
finding 
a 
winning 
strategy 
in 
the 
corresponding 
demon 
game. 
Each 
nonregular 
set 
gives 
a 
different 
game. 
We'll 
give 
several 
examples 
in 
Lecture 
12. 
Warning: 
Although 
there 
do 
exist 
stronger 
vers 
ions 
that 
give 
necessary 
and 
sufficient 
conditions 
for 
regularity 
(Miscellaneous 
Exercise 
44), 
the 
sion 
of 
the 
pumping 
lemma 
given 
here 
gives 
only 
a 
necessary 
condition; 
there 
exist 
sets 
satisfying 
(P) 
that 
are 
nonregular 
(Miscellaneous 
Exercise 
43). 
You 
cannot 
show 
that 
a 
set 
is 
regular 
by 
showing 
that 
it 
satisfies 
(P). 
To 
show 
a 
given 
set 
is 
regular, 
you 
should 
construct 
a 
finite 
a.utomaton 
or 
regular 
expression 
for 
it. 
Historical 
Notes 
The 
pumping 
lemma 
for 
regular 
sets 
is 
due 
to 
Bar-Hillel, 
Perles, 
and 
Shamir 
[8]. 
This 
version 
gives 
only 
a 
necessary 
condition 
for 
regularity. 
Necessary 
and 
sufficient 
conditions 
are 
given 
by 
Stanat 
and 
Weiss 
[117], 
Jaffe 
[62], 
and 
Ehrenfeucht, 
Parikh, 
and 
Rozenberg 
[33]. 

_____________________________________________
Lecture 
12 
Using 
the 
Pumping 
Lemma 
Example 
12.1 
Let's 
use 
the 
pumping 
lemma 
in 
the 
form 
of 
the 
demon 
game 
to 
show 
that 
the 
set 
A 
= 
{anb
m 
In;::: 
m} 
is 
not 
regular. 
The 
set 
A 
is 
the 
set 
of 
strings 
in 
a*b* 
with 
no 
more 
b's 
than 
a's. 
The 
demon, 
who 
is 
betting 
that 
A 
is 
regular, 
picks 
some 
number 
k. 
A 
good 
response 
for 
you 
is 
to 
pick 
x 
= 
a
k
, 
y 
= 
b
k
, 
and 
z 
= 
E. 
Then 
xyz 
= 
alcb
k 
E 
A 
and 
lyl 
= 
k; 
so 
far 
you 
have 
followed 
the 
rules. 
The 
demon 
must 
now 
pick 
u, 
v, 
w 
such 
that. 
y 
= 
uvw 
and 
v 
# 
f. 
Say 
the 
demon 
picks 
u, 
v, 
w 
of 
length 
j, 
m, 
n, 
respecUvely, 
with 
k 
= 
j 
+ 
m 
+-
n 
and 
m 
> 
o. 
No 
matter 
what 
the 
demon 
picks, 
you 
can 
take 
i 
= 
2 
and 
you 
win: 
xuv
2
wz 
= 
akbibmbmbn 
= 
akbj+2m+n 
= 
akb
k
+
m
, 
which 
is 
not 
in 
A, 
because 
the 
number 
of 
b's 
is 
strictly 
larger 
than 
the 
number 
of 
a's. 
This 
strategy 
always 
leads 
to 
victory 
for 
you 
in 
the 
demon 
game 
associated 
with 
the 
set 
A. 
As 
we 
argued 
in 
Lecture 
11, 
this 
is 
tantamount 
to 
showing 
that 
A 
is 
nonregular. 
0 

_____________________________________________
Example 
12.2 
For 
another 
example, 
take 
the 
set 
C 
= 
{an! 
I 
n 
O}. 
Using 
the 
Pumping 
Lemma 
73 
We 
would 
like 
to 
show 
that 
this 
set 
is 
not 
regular. 
This 
one 
is 
a 
little 
harder. 
It 
is 
an 
example 
of 
a 
nonregular 
set 
over 
,a 
single-letter 
alphabet. 
Intuitively, 
it 
is 
not 
regular 
because 
the 
differences 
in 
the 
lengths 
of 
the 
successive 
elements 
of 
the 
set 
grow 
too 
fast. 
Suppose 
the 
demon 
chooses 
k. 
A 
good 
choice 
for 
you 
is 
x 
= 
Z 
= 
E 
and 
y 
= 
a
k
!. 
Then 
xyz 
= 
a
k
! 
E 
C 
and 
lyl 
= 
k! 
k, 
so 
you 
have 
not 
cheated. 
The 
dem 
on 
must 
now 
choose 
u, 
v, 
w 
such 
that 
y 
= 
uvw 
and 
v 
i= 
E. 
Say 
the 
demon 
chooses 
u, 
v, 
w 
of 
length 
j, 
m, 
n, 
respectively, 
with 
k! 
= 
j 
+ 
m 
+ 
n 
and 
m 
> 
O. 
You 
now 
need 
to 
find 
i 
such 
that 
xuviwz 
(j. 
C; 
in 
other 
words, 
Ixuviwzl 
i= 
p! 
for 
any 
p. 
Note 
that 
for 
any 
i, 
Ixuviwzl 
= 
j 
+ 
im 
+ 
n 
= 
k! 
+ 
(i 
-
1)m, 
so 
you 
will 
win 
if 
you 
can 
choose 
i 
such 
that 
k! 
+ 
(i 
-
l)m 
i= 
p! 
for 
any 
p. 
Take 
i 
= 
(k 
+ 
I)! 
+ 
1. 
Then 
k! 
+ 
(i 
-
l)m 
= 
k! 
+ 
(k 
+ 
l)!m 
= 
k!(l 
+ 
m(k 
+ 
1)), 
and 
we 
want 
to 
show 
that 
this 
cannot 
be 
p! 
for 
any 
p. 
But 
if 
p! 
= 
k!(l 
+ 
m(k 
+ 
1)), 
then 
we 
could 
divide 
both 
sides 
by 
k! 
to 
get 
p(p 
-
l)(p 
-
2) 
.. 
· 
(k 
+ 
2)(k 
+ 
1) 
= 
1 
+ 
m(k 
+ 
1), 
which 
is 
impossible, 
because 
the 
left-hand 
side 
is 
divisible 
by 
k 
+ 
1 
and 
the 
right-hand 
side 
is 
not. 
0 
A 
Trick 
When 
trying 
to 
show 
that 
a 
set 
is 
nonregular, 
one 
can 
often 
simplify 
the 
problem 
by 
using 
one 
of 
the 
closure 
properties 
of 
regula.r 
sets. 
This 
often 
allows 
us 
to 
reduce 
a 
complicated 
set 
to 
a 
simpler 
set 
that 
is 
already 
known 
to 
be 
nonregular, 
thereby 
avoiding 
the 
use 
of 
the 
pumping 
lemma. 
To 
illustrate, 
consider 
the 
set 
D 
= 
{x 
E 
{a,b}* 
I 
#a(x) 
= 
#b(x)}. 
To 
show 
that 
this 
set 
is 
nonregular, 
suppose 
for 
a 
contradiction 
that 
it 
were 
regular. 
Then 
the 
set 
D 
na*b* 

_____________________________________________
74 
Lecture 
12 
would 
also 
be 
regular, 
since 
the 
intersection 
of 
two 
regular 
sets 
is 
always 
regular 
(the 
product 
construction, 
remember?). 
But 
D 
n 
L(a*b*) 
= 
{anbnl 
n 
2: 
O}, 
which 
we 
have 
already 
shown 
to 
be 
nonregular. 
This 
is 
a 
contradiction. 
For 
another 
illustration 
Qf 
this 
trick, 
consider 
the 
set 
A 
of 
Example 
12.1 
above: 
A 
= 
{anb
m 
In 
2: 
m}, 
the 
set 
of 
strings 
x 
E 
L(a*b*) 
with 
no 
more 
b's 
than 
a's. 
By 
Exercise 
2 
of 
Homework 
2, 
if 
A 
were 
regular, 
then 
so 
would 
be 
the 
set 
rev 
A 
= 
{bma
n 
In 
2: 
m}, 
and 
by 
interchanging 
a 
and 
b, 
we 
would 
get 
that 
the 
set 
A' 
= 
{amb
n 
I 
n 
2: 
m} 
is 
also 
regular. 
Formally, 
"interchanging 
a 
and 
b" 
means 
applying 
the 
momorphism 
a 
....... 
b, 
b 
....... 
a. 
But 
then 
the 
intersection 
would 
be 
regular. 
But 
we 
have 
already 
shown 
using 
the 
pumping 
lemma 
that 
this 
set 
is 
nonregular. 
This 
is 
a 
contradiction. 
Ultimate 
Periodicity 
Let 
U 
be 
a 
subset 
of 
N 
= 
{O, 
1,2,3, 
... 
}, 
the 
natural 
numbers. 
The 
set 
U 
is 
said 
to 
be 
ultimately 
periodic 
if 
there 
exist 
numbers 
n 
2: 
0 
and 
p 
> 
0 
such 
that 
for 
all 
m 
2: 
n, 
m E 
U 
if 
and 
only 
if 
m 
+ 
p 
EU. 
The 
number 
p 
is 
called 
aperiod 
of 
U. 
In 
other 
words, 
except 
for 
a 
finite 
initial 
part 
(the 
numbers 
less 
than 
n), 
numbers 
are 
in 
or 
out 
of 
the 
set 
U 
according 
to 
a 
pattern. 
For 
example, 
consider 
the 
set 
{0,3,7,II,I9,20,23;26,29,32,35,38,4I,44,47,50, 
... 
}. 
Starting 
at 
20, 
every 
third 
element 
is 
in 
the 
set, 
t.herefore 
this 
set 
is 
mately 
periodic 
with 
n 
= 
20 
and 
p 
= 
3. 
Note 
that 
neither 
n 
nor 
pis 
uniquej 
for 
example, 
for 
this 
set 
we 
could 
also 
have 
taken 
n 
= 
21 
and 
p 
= 
6, 
or 
n 
= 
100 
and 
p 
= 
33. 
Regular 
sets 
over 
a 
single-letter 
alphabet 
{al 
and 
ultimately 
periodic 
sets 
of 
N 
are 
strongly 
related: 

_____________________________________________
Using 
the 
Pumping 
Lemma 
75 
Theorem 
12.3 
Let 
A 
{a}*. 
Then 
Aisregular 
i/ 
and 
only 
i/ 
the 
set 
{m 
I 
a
m 
E 
A}, 
the 
set 
o/lengths 
0/ 
strings 
in 
A, 
is 
ultimately 
periodic. 
Proo/. 
If 
A 
is 
regular, 
then. 
any 
DFA 
for 
it 
consists 
of 
a 
finite 
tail 
of 
so 
me 
length, 
say 
n 
0, 
followed 
by 
a 
loop 
of 
length 
p 
> 
° 
(plus 
possibly 
some 
inaccessible 
states, 
which 
can 
be 
thrown 
out) 
. 
.. 
n 
To 
see 
this, 
consider 
any 
DFA 
for 
A. 
Since 
the 
alphabet 
is 
{a} 
and 
the 
machine 
is 
deterministic, 
there 
is 
exactly 
one 
edge 
out 
of 
each 
state, 
and 
it 
has 
label 
a. 
Thus 
there 
is 
a 
unique 
path 
through 
the 
automaton 
starting 
at 
the 
start 
state. 
Follow 
this 
path 
until 
the 
first 
time 
you 
see 
astate 
that 
you 
have 
seen 
before. 
Since 
the 
collection 
of 
states 
is 
finite, 
eventually 
this 
must 
happen. 
The 
first 
time 
this 
happens, 
we 
have 
discovered 
a 
loop. 
Let 
p 
be 
the 
length 
of 
the 
loop, 
and 
let 
n 
be 
the 
length 
of 
the 
initial 
tail 
preceding 
the 
first 
time 
we 
enter 
the 
loop. 
For 
all 
strings 
a
m 
with 
m 
n, 
the 
automaton 
is 
in 
the 
loop 
part 
after 
scanning 
a
m
. 
Then 
a
m 
is 
accepted 
iff 
a
m
+
p 
is, 
since 
the 
automaton 
moves 
around 
the 
loop 
once 
under 
the 
last 
p 
a's 
of 
a
m
+
p
Ł 
Thus 
it 
is 
in 
the 
same 
state 
after 
scanning 
both 
strings. 
Therefore, 
the 
set 
of 
lengths 
of 
accepted 
strings 
is 
ultimately 
periodic. 
Conversely, 
given 
any 
ultimately 
periodic 
set 
U, 
let 
p 
be 
the 
period 
and 
let 
n 
be 
the 
starting 
point 
of 
the 
periodic 
behavior. 
Then 
one 
can 
build 
an 
automaton 
with 
a 
tail 
of 
length 
n 
and 
loop 
of 
length 
p 
accepting 
exactly 
the 
set 
of 
strings 
in 
{a} 
* 
whose 
lengths 
are 
in 
U. 
For 
example, 
for 
the 
ultimately 
periodic 
set 
{0,3,7,11,19,20,23,26,29,32,35,38,41,44,47,50, 
... 
} 
mentioned 
above, 
the 
automaton 
would 
be 
.@ 
.... 
@ 
...... 
@ 
...... 
@ 
.............. 
Corollary 
12.4 
Let 
A 
be 
any 
regular 
set 
over 
any 
finite 
alphabet 
not 
necessarily 
consisting 
0/ 
a 
single 
letter. 
Then 
the 
set 

_____________________________________________
76 
Lecture 
12 
lengths 
A 
= 
{lxii 
x 
E 
A} 
0/ 
lengths 
0/ 
strings 
in 
A 
is 
ultimately 
periodic. 
Proof. 
Define 
the 
homomorphism 
h 
: 
E 
-+ 
{al 
by 
h(b) 
= 
a 
for 
all 
b 
E 
E. 
Then 
h(x) 
= 
a
1zl
. 
Since 
h 
preserves 
length, 
we 
have 
that 
lengths 
A 
= 
lengths 
h(A). 
But 
h(A) 
is 
a 
regular 
subset 
of 
{a}*, 
since 
the 
regular 
sets 
are 
closed 
under 
homomorphic 
image; 
therefore, 
by 
Theorem 
12.3, 
lengths 
h(A) 
is 
ultimately 
periodic. 
0 
Historical 
Notes 
A 
general 
treatment 
of 
ultimate 
periodicity 
and 
regularity-preserving 
tions 
is 
given 
in 
Seiferas 
and 
McNaughton 
[113]; 
see 
Miscellaneous 
Exercise 
34. 

_____________________________________________
Lecture 
13 
DFA 
State 
Minimization 
By 
now 
you 
have 
probably 
come 
across 
several 
situations 
in 
which 
you 
have 
observed 
that 
some 
automaton 
could 
be 
simplified 
either 
by'deleting 
states 
inaccessible 
from 
the 
start 
state 
or 
by 
collapsing 
states 
that 
were 
equivalent 
in 
some 
sense. 
For 
example, 
if 
you 
were 
to 
apply 
the 
subset 
construction 
to 
the 
NFA 
Ka 
.. 
b 
s 
t 
u 
v 
accepting 
the 
set 
of 
all 
strings 
containing 
the 
substring 
aba, 
you 
would 
obtain 
a 
DFA 
with 
2
4 
= 
16 
states. 
However, 
all 
except 
six 
of 
these 
states 
are 
inaccessible. 
Deleting 
them, 
you 
would 
obtain 
the 
DFA 
b 
a 
From 
left 
to 
right, 
the 
states 
of 
this 
DFA 
correspond 
to 
the 
subsets 
{s}, 
{s,t}, 
{s,u}, 
{s,t,v}, 
{s,u,v}, 
{s,v}. 

_____________________________________________
78 
Lecture 
13 
Example 
13.1 
Now, 
note 
that 
the 
rightmost 
three 
states 
of 
this 
DFA 
might 
as 
well 
be 
collapsed 
into 
a 
single 
state, 
since 
they 
are 
all 
accept 
states, 
and 
once 
the 
machine 
enters 
one 
of 
them 
it 
cannot 
escape. 
Thus 
this 
DFA 
is 
equivalent 
to 
b 
7 
b 
This 
is 
a 
simple 
example 
in 
which 
the 
equivalence 
of 
states 
is 
obvious, 
but 
sometimes 
it 
is 
not 
so 
obvious. 
In 
this 
and 
the 
next 
lecture 
we 
will 
develop 
a 
mechanical 
method 
to 
find 
all 
equivalent 
states 
of 
any 
given 
DFA 
and 
collapse 
them. 
This 
will 
give 
a 
DFA 
for 
any 
given 
regular 
set 
A 
that 
has 
as 
few 
states 
as 
possible. 
An 
amazing 
fact 
is 
that 
every 
regular 
set 
has 
a 
minimal 
DFA 
that 
is 
unique 
up 
to 
isomorphism, 
and 
there 
is 
a 
purely 
mechanical 
method 
for 
constructing 
it 
from 
any 
given 
DFA 
for 
A. 
Say 
we 
are 
given 
a 
DFA 
M 
= 
(Q, 
E, 
8, 
s, 
F) 
for 
A. 
The 
minimization 
process 
consists 
of 
two 
stages: 
1. 
Get 
rid 
of 
inaccessible 
states; 
that 
is, 
states 
q 
for 
wh 
ich 
there 
exists 
no 
string 
x 
E 
E* 
such 
that 
6( 
s, 
x) 
= 
q. 
2. 
Collapse 
"equivalent" 
states. 
Removing 
inaccessible 
states 
surely 
does 
not 
change 
the 
set 
accepted. 
It 
is 
quite 
straight 
forward 
to 
see 
how 
to 
do 
this 
mechanically 
using 
first 
search 
on 
the 
transition 
graph. 
Let 
us 
then 
assurne 
that 
this 
has 
been 
done. 
For 
stage 
2, 
we 
need 
to 
say 
what 
we 
mean 
by 
"equivalent" 
and 
how 
we 
do 
the 
collapsing. 
Let's 
look 
at 
some 
examples 
before 
giving 
a 
formal 
definition. 
a,b 
u 
a,b 
ab 
ab 
0 
These 
automata 
both 
accept 
the 
set 
{a, 
b}. 
The 
automaton 
with 
four 
states 
goes 
to 
different 
states 
depending 
on 
the 
first 
input 
symbol, 
but 
there's 
rea1'!y 
no 
reason 
for 
the 
states 
to 
be 
separate. 
They 
are 
equivalent 
and 
can 
be 
collapsed 
into 
one 
state, 
giving 
the 
automaton 
with 
three 
states. 
0 

_____________________________________________
Example 
13.2 
Example 
13.3 
Example 
13.4 
DFA 
State 
Minimization 
79 
1 3 
a,b 
2 4 
a,b. 
t::\ 
a,b 
a,b 
Ü 
b 
.. 
Ł 
.. 
Ł 
1---
a, 
6 7 8 9 
This 
example 
is 
a 
little 
more 
eomplieated. 
The 
automata 
both 
aeeept 
the 
set 
{a, 
b 
}U{strings 
of 
length 
3 
or 
greater}. 
In 
the 
first 
automaton, 
states 
3 
and 
4 
are 
equivalent, 
sinee 
they 
both 
go 
to 
state 
5 
under 
both 
input 
symbols, 
so 
there's 
no 
reason 
to 
keep 
them 
separate. 
Onee 
we 
eollapse 
them, 
we 
ean 
eollapse 
1 
and 
2 
for 
the 
same 
reason, 
giving 
the 
seeond 
automaton. 
State 
0 
beeomes 
state 
6; 
states 
1 
and 
2 
eollapse 
to 
beeome 
state 
7; 
states 
3 
and 
4 
eollapse 
to 
beeome 
state 
8; 
and 
state 
5 
beeomes 
state 
9. 
0 
1 3 
a,b 
2 4 
a,b 
a,b 
Ü 
b 
... ... 
a, 
6 7 8 
Here 
we 
have 
modified 
the 
first 
automaton 
by 
making 
states 
3, 
4 
aeeept 
states 
instead 
of 
1,2. 
Now 
states 
3, 
4, 
5 
are 
equivalent 
and 
can 
be 
eollapsed. 
These 
beeome 
state 
8 
of 
the 
seeond 
automaton. 
The 
set 
accepted 
is 
the 
set 
of 
all 
strings 
of 
length 
at 
least 
two. 
0 
These 
automata 
both 
aeeept 
the 
set 
{am 
I 
m 
== 
1 
mod 
3} 
(edge 
labels 
are 
omitted). 
In 
the 
left 
automaton, 
diametrieally 
opposed 
states 
are 
equivalent 
. 
and 
ean 
be 
eollapsed, 
giving 
the 
automaton 
on 
the 
right. 
0 

_____________________________________________
80 
Lecture 
13 
The 
Quotient 
Construction 
How 
do 
we 
know 
in 
general 
when 
two 
states 
can 
be 
collapsed 
safely 
without 
changing 
the 
set 
accepted? 
How 
do 
we 
do 
the 
collapsing 
formally? 
Is 
there 
a 
fast 
algorithm 
.for 
doing 
it? 
How 
can 
we 
determine 
whether 
any 
further 
collapsing 
is 
possible? 
Surely 
we 
never 
want 
to 
collapse 
an 
accept 
state 
p 
and 
a 
reject 
state 
q, 
because 
if 
p 
= 
6( 
s, 
x) 
E 
Fand 
q 
= 
6( 
s, 
y) 
t/. 
F, 
then 
x 
must 
be 
accepted 
and 
y 
must 
be 
rejected 
even 
after 
collapsing, 
so 
there 
is 
no 
way 
to 
declare 
the 
collapsed 
state 
to 
be 
an 
accept 
or 
reje'Ct 
state 
without 
error. 
Also, 
if 
we 
collapse 
p 
and 
q, 
then 
we 
had 
better 
also 
collapse 
c5(p,a) 
and 
c5(q,a) 
to 
maintain 
determinism. 
These 
two 
observations 
together 
imply 
inductively 
that 
we 
cannot 
collapse 
p 
and 
q 
if 
6(p,x) 
E 
Fand 
6(q;x) 
t/. 
F 
for 
some 
string 
x. 
It 
turns 
out 
that 
this 
criterion 
is 
necessary 
and 
sufficient 
for 
deciding 
whether 
a 
"pair 
of 
states 
call 
be 
collapsed. 
Tliat 
is, 
if 
there 
exists 
astring 
x 
such 
that 
c5(p,x) 
E 
Fand 
c5(q,x) 
t/. 
F 
or 
vice 
versa, 
then 
p 
and 
q 
cannot 
be 
safely 
collapsed; 
and 
if 
no 
such 
x 
exists, 
then 
they 
can. 
Here's 
how 
we 
show 
this 
formally. 
We 
first 
define 
an 
equivalence 
relation 
on 
Q 
by 
def 
* 
p 
q 
{:::=> 
\Ix 
E 
E 
(t5(p,x) 
E 
F 
{:::=> 
t5(q,x) 
E 
F). 
This 
definition 
is 
just 
a 
formal 
restatement 
of 
the 
collapsing 
criterion. 
It 
is 
not 
hard 
to 
argue 
that 
the 
relation 
is 
indeed 
an 
equivalence 
relation: 
it 
lS 
Ł 
reflexive: 
p 
p 
for 
all 
p; 
Ł 
symmetrie: 
if 
p 
q, 
then 
q 
p; 
and 
Ł 
transitive: 
if 
p 
q 
and 
q 
T, 
then 
p 
T. 
As 
with 
all 
equivalence 
relations, 
partitions 
the 
set 
on 
which 
it 
is 
defined 
into 
disioint 
equivalenee 
classes: 
[PI 
{q 
I 
q 
p}. 
Every 
element 
pE 
Q 
is 
contained 
in 
exactly 
one 
equivalence 
class 
[PI, 
and 
p 
q 
[P) 
= 
[q). 
We 
now 
define 
a 
DFA 
M 
/ 
called 
the 
quotient 
automaton, 
whose 
states 
correspond 
to 
the 
equivalence 
classes 
of 
This 
constructlon.is 
called 
a 
quotient 
con.druction 
and 
is 
quite 
comDion 
in 
algebra. 
We 
will 
see 
a 
more 
general 
account 
of 
it 
in 
Supplementary 
Lectures 
C 
and 
D. 

_____________________________________________
DFA 
State 
Minimization 
81 
There 
is 
one 
state 
of 
M 
/ 
for 
each 
dass. 
In 
fact, 
formally, 
the 
states 
of 
M 
are 
the 
equivalence 
dasses; 
this 
is 
the 
mathematical 
way 
of 
"collapsing" 
equivalent 
states. 
Define 
(QI, 
0
1
, 
SI, 
F
I
), 
where 
QI 
{[P]I 
p 
E 
Q}, 
ol([p],a) 
[o(p,a)], 
I 
def 
[ ] 
S 
= 
S, 
F
I 
{[P]I 
p 
E 
F}. 
(13.1) 
There 
is 
a 
subtle 
but 
important 
point 
involving 
the 
definition 
of 
0
1 
in 
(13.1): 
we 
need 
to 
show 
that 
it 
1S 
well-defined. 
Note 
that 
the 
action 
of 
0
1 
on 
the 
equivalence 
dass 
[P] 
is 
defined 
in 
terms 
of 
p. 
It 
is 
conceivable 
that 
a 
different 
choice 
of 
representative 
of 
the 
dass 
[P] 
(Le., 
some 
q 
such 
that 
q 
p) 
might 
lead 
to 
a 
different 
right-hand 
side 
in 
(13.1). 
Lemma 
13.5 
says 
exactly 
that 
this 
does 
not 
happen. 
Lemma 
13.5 
If 
p 
q, 
then 
o(p,a) 
o(q,a). 
Equivalently, 
if 
[P] 
= 
[q], 
then 
[o(p,a)] 
= 
[o(q, 
a)]. 
. 
Proof. 
Suppose 
p 
q. 
Let 
a 
E 
and 
y 
E 
6(o(p,a),y) 
E 
F 
{:=} 
6(p,ay) 
E 
F 
{::::::> 
6(q,ay) 
E 
F 
since 
p 
q 
{::::::> 
6(0(q,a),y) 
E 
F. 
Since 
y 
was 
arbitrary, 
o(p,a) 
o(q,a) 
by 
definition 
Lemma 
13.6 
p 
E 
F 
{::::::> 
[P] 
E 
F
I
Ł 
o 
Proof. 
The 
direction 
:::} 
is 
immediate 
from 
the 
definition 
of 
F
I
Ł 
For 
the 
direction 
<=, 
we 
need 
to 
show 
that 
if 
p 
q 
and 
pE 
F, 
then 
q 
E 
F. 
In 
other 
words, 
every 
dass 
is 
either 
a 
subset 
of 
F 
or 
disjoint 
from 
F. 
This 
follows 
immediately 
by 
taking 
x 
= 
• 
in 
the 
definition 
of 
p 
q. 
0 
Lemma 
13.7 
For 
all 
x 
E 
6
1
([P],x) 
= 
[6(p,x)]. 
Proof. 
By 
induction 
on 
lxi. 

_____________________________________________
82 
Lecture 
13 
Basis 
For 
x 
= 
1:, 
8' 
([P], 
1:) 
= 
[PI 
definition 
of 
b' 
= 
[b(p, 
1:)] 
definition 
of 
O. 
Induction 
step 
Assume 
b'([P],x) 
= 
[b(p,x)], 
and 
let 
a 
E 
b'([P], 
xa) 
= 
o'(b'([P], 
x), 
a) 
definition 
of 
6' 
= 
o'([b(p, 
x)], 
a) 
induetion 
hypothesis 
= 
[0 
(b(p, 
x), 
a)] 
definition 
of 
0' 
= 
[b(p,xa)] 
definition 
of 
b. 
o 
Theorem 
13.8 
L(MI-;:;:,) 
= 
L(M). 
Prooj. 
For 
x 
E 
x 
E 
L 
(M 
I 
-;:;:,) 
{:::=> 
8' 
(s', 
x) 
E 
F' 
definition 
of 
aeeeptanee 
{:::=> 
b'([s],x) 
E 
F' 
definition 
of 
s' 
{:::=> 
[b(s, 
x)] 
E 
F' 
Lemma 
13.7 
{:::=> 
b( 
s, 
x) 
E 
F 
Lemma 
13.6 
{:::=> 
x 
E 
L(M) 
definition 
of 
aeceptanee. 
o 
MI-;:;:, 
Cannot 
Be 
Collapsed 
Further 
It 
is 
eoneeivable 
that 
after 
doing 
the 
quotient 
construetion 
onee, 
we 
might 
be 
able 
to 
coll.apse 
even 
further 
by 
doing 
it 
again. 
It 
turns 
out 
that 
onee 
is 
t'nough. 
To 
see 
this, 
let's 
do 
the 
quotient 
eonstruetion 
a 
seeond 
time. 
Define 
[PI 
'" 
[q] 
Vx 
E 
(b'([P],x) 
E 
F' 
{:::=> 
b'([q],x) 
E 
F'). 
This 
is 
exactly 
the 
same 
definition 
a.s 
above, 
only 
applied 
to 
the 
quotient 
automaton 
M 
1-;:;:,. 
We 
use 
the 
notation"" 
for 
the 
equivalence 
relation 
on 
Q' 
to 
distinguish 
it 
from 
the 
relation 
on 
Q. 
Now 
[P] 
'" 
[q] 
:::} 
Vx 
(b'([P),x) 
E 
F' 
{:::=> 
b'([q),x) 
E 
F') 
definition 
of 
"" 
:::} 
Vx 
([b(p, 
x)] 
E 
F' 
{:::::} 
[b(q,x)] 
E 
F') 
Lemma 
13.7 

_____________________________________________
::} 
'Ix 
(6(p,x) 
E 
F 
-<==> 
6(q,x) 
E 
F) 
::} 
[P] 
= 
[q]. 
DFA 
State 
Minimization 
83 
Lemma 
13.6 
definition 
of 
Thus 
any 
two 
equivalent 
states 
of 
M 
are 
in 
fact 
equal, 
and 
the 
collapsing 
relation", 
on 
Q' 
is 
just 
the 
identity 
relation 
=. 

_____________________________________________
Lecture 
14 
A 
Minimization 
Aigorithm 
Here 
is 
an 
algorithm 
for 
computing 
the 
collapsing 
relation:::::: 
for 
a 
given 
DFA 
M 
with 
no 
inaccessible 
states. 
Our 
algorithm 
will 
mark 
(unordered) 
pairs 
of 
states 
{p, 
q}. 
A 
pair 
{p, 
q} 
will 
be 
marked 
as 
soon 
as 
a 
reason 
is 
discovered 
why 
p 
and 
q 
are 
not 
equivalent. 
1. 
Write 
down 
a 
t.able 
of 
all 
pairs 
{p, 
q}, 
initially 
unmarked. 
2. 
Mark 
{p, 
q} 
if 
p 
t: 
Fand 
q 
f/. 
F 
or 
vice 
versa. 
3. 
Repeat 
the 
foliuwing 
until 
no 
more 
changes 
occur: 
if 
there 
exists 
an 
unmarkcd 
pair 
{p,q} 
such 
that 
{b(p,a),b(q,a)} 
is 
marked 
for 
some 
a 
E 
L:, 
then 
mark 
{p, 
q}. 
4. 
When 
done, 
p 
:::::: 
q 
iff 
{p, 
q} 
is 
not 
marked. 
Here 
are 
some 
things 
to 
note 
about 
this 
algorithm: 
Ł 
If 
{p, 
q} 
is 
marked 
in 
step 
2, 
then 
p 
and 
q 
are 
surely 
not 
equivalent: 
take 
x 
= 
E 
in 
the 
definition 
of 
:::::: 
. 
Ł 
We 
may 
have 
to 
look 
at 
the 
same 
pair 
{p, 
q} 
many 
times 
in 
step 
3, 
since 
any 
change 
in 
the 
table 
may 
suddenly 
allow 
{p, 
q} 
to 
be 
marked. 
We 
stop 
only 
after 
we 
make 
an 
entire 
pass 
through 
the 
table 
with 
no 
new 
marks. 

_____________________________________________
A 
Minimization 
Aigorithm 
85 
Ł 
The 
algorithm 
runs 
for 
only 
a 
finite 
number 
of 
steps, 
since 
there 
are 
only 
possible 
marks 
that 
can 
be 
made,! 
and 
we 
have 
to 
make 
at 
least 
one 
new 
mark 
in 
each 
pass 
to 
keep 
going 
. 
Ł . 
Step 
4 
is 
really 
a 
statement 
of 
the 
theorem 
that 
the 
algorithm 
correctly 
computes 
This 
requires 
proof, 
wh 
ich 
we 
defer 
untillater. 
Example 
14.1 
Let's 
minimize 
the 
automaton 
of 
Example 
13.2 
of 
Lecture 
13. 
a 
b 
-+ 
0 1 2 
1F 
3 4 
2F 
4 3 
3 
5 5 
4 5 5 
5F 
5 5 
Here 
is 
the 
table 
built 
in 
step 
1. 
Initially 
all 
pairs 
are 
unmarked. 
o 
1 
2 
3 
4 
5 
After 
step 
2, 
all 
pairs 
consisting 
of 
one 
accept 
state 
and 
one 
nonaccept 
state 
have 
been 
marked. 
° 
,( 
1 
,( 
2 
,( 
,( 
3 
,(,( 
4 
,( 
,(,(5 
Now 
look 
at 
an 
unmarked 
pair, 
say 
{O,3}. 
Underinput 
a, 
° 
and 
3 
go 
to 
1 
and 
5, 
respectively 
(write: 
{O,3} 
-+ 
{1,5}). 
The 
pair 
{1,5} 
is 
not 
marked, 
so 
we 
don't 
mark 
{O, 
3}, 
at 
least 
not 
yet. 
Under 
input 
b, 
{O,3} 
-+ 
{2, 
5}, 
which 
is 
not 
marked, 
so 
we 
still 
don't 
mark 
{O, 
3}. 
We 
then 
look 
at 
unmarked 
pairs 
{O, 
4} 
and 
{1, 
2} 
and 
find 
out 
we 
cannot 
mark 
them 
yet 
for 
the 
same 
reasons. 
But 
for 
{1,5}, 
under 
input 
a, 
{1,5} 
-+ 
{3,5}, 
and 
{3,5} 
is 
marked, 
so 
we 
mark 
{1,5}. 
Similarly, 
under 
input 
a, 
{2,5} 
-+ 
{4,5} 
which 
is 
marked, 
so 
we 
mark 
{2,5}. 
Under 
both 
inputs 
a 
and 
b, 
{3,4} 
-+ 
{5,5}, 
which 
is 
never 
marked 
(it 
'5 
not 
even 
in 
the 
table), 
so 
we 
do 
not 
mark 
{3, 
4}. 
After 
the 
first 
1(k) 
the 
number 
of 
subsets 
of 
size 
k 
in 
a 
set 
of 
size 
n. 

_____________________________________________
86 
Lecture 
14 
pass 
of 
step 
3, 
the 
table 
looks 
like 
° 
./ 
1 
./ 
2 
./ 
./ 
3 
./ 
./ 
4 
./ 
./ 
./ 
./ 
./ 
5 
Now 
we 
make 
another 
pass 
through 
the 
table. 
As 
before, 
{O,3} 
-+ 
{1,5} 
under 
input 
a, 
but 
this 
time 
{1, 
5} 
is 
marked, 
so 
we 
mark 
{O, 
3}. 
Similarly, 
{O,4} 
-+ 
{2, 
5} 
nnder 
input 
b, 
and 
{2, 
5} 
is 
marked, 
so 
we 
mark 
{O, 
4}. 
This 
gives 
° 
./ 
1 
./ 
2 
./ 
./ 
./ 
3 
./ 
./ 
./ 
4 
./ 
./ 
./ 
./ 
./ 
5 
Now 
we 
check 
the 
remaining 
unmarked 
pairs 
and 
find 
out 
that 
{1,2} 
-+ 
{3,4} 
and 
{3,4} 
-+ 
{5,5} 
under 
both 
a 
and 
b, 
and 
neither 
{3,4} 
nor 
{5,5} 
is 
marked, 
so 
there 
are 
no 
new 
marks. 
We 
are 
left 
with 
unmarked 
pairs 
{1,2} 
and 
{3,4}, 
indicating 
that 
1 
2 
and 
3 
4. 
0 
Example 
14.2 
Now 
let's 
do 
Example 
13.4 
of 
Lecture 
13. 
a 
-+ 
° 
1 
1F 
2 
2 3 
3 4 
4F 
5 
5 
° 
Here 
is 
the 
table 
after 
step 
2. 
° 
./ 
1 
./ 
2 
./ 
3 
./ 
././ 
4 
./ ./ 
5 
Then: 
Ł 
{O,2} 
-+ 
{1,3}, 
which 
is 
marked, 
so 
mark 
{O,2}. 

_____________________________________________
A 
Minimization 
Aigorithm 
87 
Ł 
{O,3} 
-+ 
{1,4}, 
which 
is 
not 
marked, 
so 
do 
not 
mark 
{O,3}. 
Ł 
{O,5} 
-+ 
{O, 
l}, 
which 
is 
marked, 
so 
mark 
{O,5}. 
Ł 
{1,4} 
-+ 
{2,5}, 
which 
is 
not 
marked, 
so 
do 
not 
mark 
{1,4}. 
Ł 
{2,3} 
-+ 
{3,4}, 
wh 
ich 
is 
marked, 
so 
mark 
{2,3}. 
Ł 
{2,5} 
-+ 
{O, 
3}, 
wh 
ich 
is 
not 
marked, 
so 
do 
not 
mark 
{2, 
5}. 
Ł 
{3,5} 
-+ 
{O,4}, 
which 
is 
marked, 
so 
mark 
{3,5}. 
After 
the 
first 
pass, 
the 
table 
looks 
like 
this: 
° 
.( 
.( 
.( 
.( 
1 
.( 
2 
.( .( 
3 
.( 
.( 
.( .( 
4 
.( 
5 
Now 
do 
another 
pass. 
We 
discover 
that 
{O,3} 
-+ 
{l,4} 
-+ 
{2,5} 
-+ 
{O,3} 
and 
none 
of 
these 
are 
marked, 
so 
we 
are 
done. 
Thus 
° 
3, 
1 
4, 
and 
2 
5. 
0 
Correctness 
of 
the 
Collapsing 
Aigorithrn 
Theorem 
14.3 
The 
pair 
{p, 
q} 
is 
marked 
by 
the 
above 
algorithm 
if 
and 
only 
if 
there 
exists 
x 
E 
E* 
such 
that 
8(p, 
x) 
E 
Fand 
8( 
q, 
x) 
F 
or 
vi 
ce 
versa; 
i. 
e., 
if 
and 
only 
if 
p 
'*' 
q. 
Proof. 
This 
is 
easily 
proved 
by 
induction. 
We 
leave 
the 
proof 
as 
an 
exercise 
(Miscellaneous 
Exercise 
49). 
0 
A 
nice 
way 
to 
look 
at 
the 
algorithm 
is 
as 
a 
finite 
automn.ton 
itself. 
Let 
Q 
= 
{{p,q} 
I 
p,q 
E 
Q, 
p 
f. 
q}. 
There 
are 
(2) 
elements 
of 
Q, 
where 
n 
is 
the 
size 
of 
Q. 
Define 
a 
ministic 
"transition 
function" 
ß: 
Q 
-+ 
2
Q 
on 
Q 
as 
follows: 
ß ( 
{p, 
q}, 
a) 
= 
{{p', 
q'} 
I 
p 
= 
8 
(p' 
, 
a), 
Q 
= 
b 
(q', 
a)}. 
Define 
a 
set 
of 
"start 
states" 
S 
Q 
as 
folIows: 
S 
= 
{{p, 
q} 
I 
p 
E 
F, 
q 
F}. 

_____________________________________________
88 
Lecture 
14 
(We 
don't 
need 
to 
write 
CI 
ŁŁŁ 
or 
vice 
versa" 
because 
{p,q} 
is 
an 
unordered 
pair.) 
Step 
2 
of 
the 
algorithm 
marks 
the 
elements 
of 
S, 
and 
step 
3 
marks 
pairs 
in 
ß({p,q},a) 
when 
{p,q} 
is 
marked 
for 
any 
a 
E 
E. 
In 
these 
terms, 
Theorem 
14.3 
says 
that 
p 
*' 
q 
iff 
{p, 
q} 
is 
accessible 
in 
this 
automaton. 

_____________________________________________
Lecture 
15 
Myhill-Nerode 
Relations 
Two 
deterministie 
finite 
automata 
M 
= 
(QM, 
E, 
OM, 
SM, 
FM), 
N 
= 
(QN, 
E, 
ON, 
SN, 
F
N
) 
are 
said 
to 
be 
isomorphie 
(Greek 
for 
"same 
form") 
if 
there 
is 
a 
one-to-one 
and 
onto 
mapping 
! : 
Q 
M 
-+ 
Q 
N 
such 
that 
Ł 
!(SM) 
= 
SN, 
Ł 
!(oM(p,a)) 
= 
oN(f(p),a) 
for 
all 
p 
E 
QM, 
a 
E 
E, 
and 
Ł 
pE 
FM 
iff 
!(p) 
E 
F
N
. 
That 
is, 
they 
are 
essentially 
the 
same 
automaton 
up 
to 
renaming 
of 
states. 
It 
is 
easily 
argued 
that 
isomorphie 
automata 
aeeept 
the 
same 
set. 
In 
this 
leeture 
and 
the 
next 
we 
will 
show 
that 
if 
M 
and 
N 
are 
any 
two 
tomata 
with 
no 
inaeeessible 
states 
aeeepting 
the 
same 
set, 
then 
the 
quotient 
automata 
and 
obtained 
by 
the 
eollapsing 
algorithm 
of 
Leeture 
14 
are 
isomorphie. 
Thus 
the 
DFA 
obtained 
by 
the 
eollapsing 
algorithm 
is 
the 
minimal 
DFA 
for 
the 
set 
it 
aeeepts, 
and 
this 
automaton 
is 
unique 
up 
to 
isomorphism. 
We 
will 
do 
this 
by 
exploiting 
a 
profound 
and 
beautiful 
eorrespondenee 
between 
finite 
automata 
with 
input 
alphabet 
E 
and 
eertain 
equivalenee 

_____________________________________________
90 
Lecture 
15 
relations 
on 
E*. 
We 
will 
show 
that 
the 
unique 
minimal 
DFA 
fora 
regular 
set 
R 
can 
be 
defined 
in 
a 
natural 
way 
directly 
from 
R, 
and 
that 
any 
minimal 
automaton 
for 
R 
is 
isomorphie 
to 
this 
automaton. 
Myhill-Nerode 
Relations 
Let 
R 
E* 
be 
a 
regular set, 
and 
let 
M 
= 
(Q, 
E, 
6, 
s, 
F) 
be 
a 
DFA 
for 
R 
with 
no 
inaceessible 
states. 
The 
automaton 
M 
induees 
an 
equivalenee 
relation 
=M 
on 
E* 
defined 
by 
def 
X 
=M 
Y 
{:::=:} 
6(s,x) 
= 
6(s,y). 
(Don't 
confuse 
this 
relation 
with 
the 
collapsing 
relation 
of 
Leeture 
13-
that 
relation 
was 
defined 
Oll. 
Q, 
whereas 
=M 
is 
defined 
on 
E*.) 
One 
ean 
easily 
show 
that 
the 
relation 
=M 
is 
an 
equivalenee 
relation; 
that 
is, 
that 
it 
is 
reflexive, 
symmetrie, 
and 
transitive. 
In 
addition, 
=M 
satisfies 
a 
few 
other 
useful 
properties: 
(i) 
It 
is 
a 
right 
congruence: 
for 
any 
x, 
y 
E 
E* 
and 
a 
E 
E, 
X 
=M 
Y::::} 
xa 
=M 
ya. 
To 
see 
this, 
assume 
that 
x 
=M 
y. 
Then 
6(s,xa) 
= 
6(6(s,x),a) 
= 
6(6(s,y),a) 
byassumption 
=·b{s,ya). 
(ii) 
It 
refines 
R: 
föt 
any 
x, 
y 
E 
E*, 
X 
=M 
Y 
.::::} 
(x 
ER{:::=:} 
Y 
ER). 
This 
is 
because 
6(s, 
x) 
= 
6(s, 
y), 
and 
this 
is 
either 
an 
aeeept 
or 
a 
reject 
state, 
so 
either 
both 
x 
and 
y 
are 
accepted 
or 
both 
are 
rejected. 
Another 
way' 
to 
say 
this 
is 
that 
every 
=M-class 
has 
either 
all 
its 
elements 
in 
R 
or 
none 
of 
its 
elements 
in 
R; 
in 
other 
words, 
R 
is 
a 
union 
of 
=M-classes. 
(iii) 
It 
is 
of 
finite 
inder, 
that 
is, 
it 
has 
only 
finitely 
many 
equivalenee 
classes. 
This 
is 
beeause 
there 
is 
exactly 
one 
equivalenee 
class 
{x 
E 
E* 
16(s,x) 
::;: 
q} 
eorresponding 
to 
eaeh 
state 
q 
of 
M. 
Let 
us 
caU 
an 
equivalence 
relation 
= 
on 
E* 
a 
Myhill-Nerode 
relation 
for 
R 
if 
it 
satisfies 
properties 
(i), 
(ii), 
and 
(iii)j 
that 
is, 
if 
it 
is 
a 
right 
eongruenee 
of 
finite 
index 
refining 
R. 

_____________________________________________
Myhill-Nerode 
Relations 
91 
The 
interesting 
thing 
about 
this 
definition 
is 
that 
it 
characterizes 
exactly 
the 
relations 
on 
E* 
that 
are 
==M 
for 
some 
automaton 
M. 
In 
other 
words, 
we 
can 
reconstruct 
M 
from 
==M 
using 
only 
the 
fact 
that 
==M 
is 
Myhill-Nerode. 
To 
see 
this, 
we 
will 
show 
how 
to 
construct 
an 
automaton 
M=. 
for 
R 
from 
any 
given 
Myhill-Nerode 
relation 
== 
for 
R. 
We 
will 
show 
later 
that 
the 
two 
constructions 
are 
inverses 
up 
to 
isomorphism 
of 
automata. 
Let 
R 
E*, 
and 
let 
== 
be 
an 
arbitrary 
Myhill-Nerode 
relation 
for 
R. 
Right 
now 
we're 
not 
assuming 
that 
R 
is 
regular, 
only 
that 
the 
relation 
== 
satisfies 
(i), 
(ii), 
and 
(iii). 
The 
==-dass 
ofthe 
string 
x 
is 
[x) 
{y 
I 
y 
== 
x}. 
Although 
there 
are 
infinitely 
many 
strings, 
there 
are 
only 
finitely 
many 
==-dasses, 
by 
property 
(iii). 
Now 
define 
the 
DFA 
M=. 
= 
(Q, 
E, 
ö, 
s, 
F), 
where 
Q 
{[x] 
I 
x 
E 
E*}, 
def 
[ ) 
S 
= 
f, 
F 
([x) 
I 
x 
ER}, 
ö([x],a) 
[xa]. 
It 
follows 
from 
property 
(i) 
of 
Myhill-Nerode 
relations 
that 
ö 
is 
weIl 
defined. 
In 
other 
words, 
we 
have 
defined 
the 
action 
of 
{j 
on 
an 
equivalence 
dass 
[x] 
in 
terms 
of 
an 
element 
z 
chosen 
from 
that 
dass, 
and 
it 
is 
conceivable 
that 
we 
could 
have 
gotten 
something 
different 
had 
we 
chosen 
another 
y 
E 
[x] 
such 
that 
[xa] 
# 
[ya]. 
The 
property 
of 
right 
congruence 
says 
exactly 
that 
tl'iis 
cannot 
happen. 
Finally, 
observe 
that 
x 
E 
R 
{:::::> 
[x] 
E 
F. 
(15.1) 
The 
implication 
is 
from 
the 
definition 
of 
F, 
and 
({:::) 
foIlows 
from 
the 
definition 
of 
Fand 
property 
(ii) 
of 
Myhill-Nerode 
relations. 
Now 
we 
are 
ready 
to 
prove 
that 
L(M=.) 
= 
R. 
Lemma 
15.1 
6([x],y) 
= 
[xy]. 
Proof. 
Induction 
on 
lyl. 

_____________________________________________
92 
Lecture 
15 
Basis 
6([x], 
f) 
= 
[x] 
= 
[XE]. 
Induetion 
step 
6([x],ya) 
= 
t5(6([x],y),a) 
= 
t5([xy],a) 
= 
[xya] 
definition 
of 
6 
induction 
hypothesis 
definition 
of 
15. 
o 
Theorem 
15.2 
L(M=) 
= 
R. 
Lemma 
15.3 
Proof. 
x 
E 
<=> 
6([f], 
x) 
E 
F 
<=> 
[x] 
E 
F 
<=>xER 
definition 
of 
acceptance 
Lemma 
15.1 
property 
(15.1). 
M 
t-+ 
=M 
and 
= 
t-+ 
M= 
Are 
Inverses 
o 
We 
have 
described 
two 
natural 
constructions, 
one 
taking 
a 
given 
ton 
M 
for 
R 
with 
no 
inaccessible 
states 
to 
a 
corresponding 
Myhill-Nerode 
relation 
=M 
for 
R, 
and 
one 
taking 
a 
given 
Myhill-Nerode 
relation 
= 
for 
R 
to 
a 
DFA 
M= 
for 
R. 
We 
now 
wish 
to 
show 
that 
these 
two 
operations 
are 
inverses 
up 
to 
isomorphism. 
(i) 
If 
= 
is 
a 
Myhill-Nerode 
relation 
for 
R, 
and 
if 
we 
apply 
the 
eonstruction 
= 
t-+ 
M= 
and 
then 
apply 
the 
eonstruetion 
M 
t-+ 
=M 
to 
the 
result, 
the 
resulting 
relation 
=M= 
is 
identieal 
to 
=. 
(ii) 
If 
M 
is 
a 
DFA 
for 
R 
with 
no 
inaeeessible 
states, 
and 
if 
we 
apply 
the 
eonstruetion 
M 
t-+ 
=M 
and 
then 
apply 
the 
eonstruction 
= 
t-+ 
M= 
to 
the 
result, 
the 
resulting 
DFA 
M=M 
is 
isomorphie 
to 
M. 
Proof. 
(i) 
Let 
M= 
= 
(Q, 
E, 
15, 
s, 
F) 
be 
the 
automaton 
constructed 
from 
= 
as 
described 
above. 
Then 
for 
any 
x, 
y 
E 
E*, 
X 
=M= 
Y 
<=> 
6(s, 
x) 
= 
6(s,y) 
<=> 
6([f] 
, 
x) 
= 
6([f],y) 
<=> 
[x] 
= 
[y] 
<=> 
x 
= 
y. 
definition 
of 
=.\1= 
definition 
of 
s 
Lemma 
15.1 

_____________________________________________
Myhill-Nerode 
Relations 
93 
(ii) 
Let 
M 
= 
(Q, 
E, 
6, 
s, 
F) 
and 
let 
M=.u 
= 
(Q', 
E, 
6', 
s', 
F'). 
RecaH 
from 
the 
construction 
that 
[x] 
= 
{y 
I 
Y 
=M 
x} 
= 
{y 
16(s,y) 
= 
6(s,x)}, 
Q' 
= 
{[x] 
I 
x 
E 
E*}, 
s' 
= 
[E], 
F' 
= 
{[x] 
I 
x 
ER}, 
6'([x], 
a) 
= 
[xa]. 
We 
will 
show 
that 
M='M 
and 
M 
are 
isomorphie 
under 
the 
map 
I: 
Q' 
--
Q, 
I[X]) 
= 
6(s,x). 
By 
the 
definition 
of 
=M, 
[x] 
= 
[y] 
ift" 
6( 
s, 
x) 
= 
6( 
s, 
y), 
so 
the 
map 
I 
is 
weH 
rlefined 
on 
=M-classes 
and 
is 
one-to-one. 
Since 
M 
has 
no 
inaccessible 
states, 
f-
is 
onto. 
To 
show 
that 
I 
is 
an 
isomorphism 
of 
automata, 
we 
need 
to 
show 
that 
I 
serves 
a11 
automata-theoretie 
structure: 
the 
start 
state, 
transition 
function, 
and 
final 
states. 
That 
is, 
we 
need 
to 
show 
Ł 
I(s') 
= 
s, 
Ł 
1(6'([x],a)) 
= 
6(f([x]),a), 
Ł 
[x] 
E 
F' 
I([x]) 
E 
F. 
These 
are 
argued 
as 
fo11ows: 
/(5') 
= 
f([E]) 
definition 
of 
s' 
= 
6( 
s) 
() 
definition 
of 
I 
= 
s 
definition 
of 
6; 
1(6'([x],a)) 
= 
1([xaJ) 
=6(s,xa) 
= 
6(6(s,x),a) 
= 
6(f([x]),a) 
[x] 
E 
F' 
x 
E 
R 
6(s,x) 
E 
F 
I([x]) 
E 
F 
definition 
of 
6' 
definition 
of 
I 
definition 
of 
6 
definition 
of 
I; 
definition 
of 
Fand 
property 
(ii) 
since 
L(M) 
= 
R 
definition 
of 
I 

_____________________________________________
94 
Lecture 
15 
We 
have 
shown: 
Theorem 
15.4 
Let 
E 
be 
a 
finite 
alphabet. 
Up 
to 
isomorphism 
of 
automata, 
there 
is 
a 
to-one 
correspondence 
between 
deterministic 
finite 
automata 
over 
E 
.with 
no 
inaccessible 
states 
accepting 
Rand 
Myhill-Nerode 
relations 
for 
R 
on 
E* 

_____________________________________________
Lecture 
16 
The 
Myhill-Nerode 
Theorem 
Let 
R 
1::* 
be 
a 
regular 
set. 
Recall 
from 
Lecture 
15 
that 
a 
Myhill-Nerode 
relation 
for 
R 
is 
an 
equivalence 
relation 
== 
on 
1::* 
1latisfying 
the 
following 
three 
properties: 
(i) 
== 
is 
a 
right 
congruence: 
for 
any 
x, 
y 
E 
1::+ 
and 
a 
E 
1::, 
x 
== 
11 
=? 
xa 
== 
yaj 
(ii) 
== 
refines 
R: 
for 
any 
x, 
y 
E 
1::*, 
x 
== 
Y 
=? 
(x 
ER<=> 
Y 
E 
R)j 
(iii) 
== 
is 
of 
finite 
index; 
that 
is, 
== 
has 
only 
finitely 
many 
equivalence 
classes. 
We 
showed 
that 
there 
was 
a 
natural 
one-to-one 
correspondence 
(up 
to 
isomorphism 
of 
automata) 
between 
Ł 
deterministic 
finite 
automat-a 
for 
R 
with 
input 
alphabet 
1:: 
and 
with 
no 
inaccessible 
states, 
and 
Ł 
Myhill-Nerode 
relations 
for 
R 
on 
1::* 
This 
is 
interesting, 
because 
it 
says 
we 
can 
deal 
with 
regular 
sets 
and 
finite 
automata 
in 
terms 
of 
a 
few 
simple, 
purely 
algebraic 
properties. 

_____________________________________________
96 
lecture 
16 
In 
this 
lecture 
we 
will 
show 
that 
there 
exists 
a 
coarsest 
Myhill-Nerode 
relation 
=R 
for 
any 
given 
regular 
set 
Rj 
that 
is, 
one 
that 
every 
other 
Nerode 
relation 
for 
R 
refines. 
The 
notions 
of 
coarsest 
and 
refinement 
will 
be 
defined 
below. 
The 
relation 
=R 
to 
the 
unique 
minimal 
DFA 
for 
R. 
Recall 
from 
Lecture 
15 
the 
two 
constructions 
Ł M 
1-+ 
=M, 
which 
takes 
an 
arbitrary 
DFA 
M 
= 
(Q, 
E, 
6, 
s, 
F) 
with 
no 
inaccessible 
states 
accepting 
Rand 
produces 
a 
Myhill-Nerode 
relation 
=M 
for 
R: 
def 
-. -. 
X 
=M 
Y 
<==> 
6{s,x) 
= 
6(s,Y)j 
Ł = 
1-+ 
M=, 
which 
takes 
an 
arbitrary 
Myhill-Nerode 
relation 
= 
on 
E* 
for 
Rand 
produces 
a 
DFA 
M= 
= 
(Q, 
E, 
6, 
s, 
F) 
accepting 
R: 
[x] 
{y 
I 
Y 
= 
x}, 
{[x] 
I 
x 
E 
E*}, 
def 
I] 
S 
= 
f, 
6([x], 
a) 
[xa], 
F 
{[x] 
I 
x 
ER}. 
We 
showed 
that 
these 
two 
constructions 
are 
inverses 
up 
to 
isomorphism. 
Definition 
16.1 
A 
relation 
=1 
is 
said 
to 
refine 
another 
relation 
=2 
if 
=1 
=2, 
considered 
as 
sets 
of 
ordered 
pairs. 
In 
other 
words, 
=1 
refines 
=2 
if 
for 
a11 
x 
and 
y, 
x 
=1 
Y 
implies 
x 
=2 
y. 
For 
equivalence 
relations 
=1 
and 
= 
2, 
this 
is 
the 
same 
as 
saying 
that 
for 
every 
x, 
the 
=1-dass 
of 
x 
is 
inc1uded 
in 
the 
=2-class 
of 
x. 
0 
For 
example, 
the 
equivalence 
relation 
x 
= 
y 
mod 
6 
on 
the 
integers 
refines 
the 
equivalence 
relation 
x 
= 
y 
mod 
3. 
For 
another 
example, 
dause 
(ii) 
of 
thf 
definition 
of 
Myhill-Nerode 
relations 
says 
that 
a 
Myhill-Nerode 
relation 
=, 
for 
R 
refines 
the 
equivalence 
relation 
with 
equivalence 
dass 
es 
Rand 
E* 
-
R. 
The 
relation 
of 
refinement 
between 
equivalence 
relations 
is 
a 
partial 
ordf·r 
it 
ia 
reflexive 
(every 
relation 
refines 
itself). 
transitive 
(if 
=1 
refines 
=2 
allJ 
=2 
re 
fines 
=3, 
then 
=1 
refines 
=3), 
and 
antisymmetric 
(if 
=1 
refines 
=2 
and 
=2 
refines 
=1, 
then 
=1 
and 
=2 
are 
the 
same relation). 
If 
=1 
refines 
=2, 
then 
=1 
is 
the 
finer 
and 
is 
the 
coarser 
of 
the 
two 
relations. 
There 
is 
always 
a 
finest 
and 
a 
coarsest 
equivalence 
relation 
on 
any 
set 
U, 
namely 
the 
identity 
relation 
{(x,x) 
I 
x 
E 
U} 
and 
the 
univer8al 
relation 
{(x,y) 
I 
x,y 
EU}, 
respectively. 

_____________________________________________
The 
Myhill-Nerode 
Theorem 
97 
Now 
let 
R 
S;;; 
regular 
or 
not. 
We 
define 
an 
equivalence 
relation 
=R 
on 
in 
terms 
of 
R 
as 
follows: 
x 
=R 
y 
'</z 
E 
(xz 
E 
R 
{::::::::> 
yz 
ER). 
(16.1) 
In 
other 
words, 
two 
strings 
are 
equivalent 
under 
=R 
if, 
whenever 
you 
append 
the 
same 
string 
to 
both 
of 
them, 
the 
resulting 
two 
strings 
are 
either 
both 
in 
R 
or 
both 
not 
in 
R. 
It 
is 
not 
hard 
to 
show 
t:lat 
this 
is 
an 
equivalence. 
relation 
for 
any 
R. 
We 
show 
that 
for 
any 
set 
R, 
regular 
or 
not, 
the 
relation 
=R 
satisfies 
the 
first 
two 
properties 
(i) 
and 
(ii) 
of 
Myhill-Nerode 
relations 
and 
is 
the 
coarsest 
such 
relation 
on 
In 
case 
R 
is 
regular, 
this 
relation 
is 
also 
offinite 
index, 
therefore 
a 
Myhill-Nerode 
relation 
for 
R. 
In 
fact, 
it 
is 
the 
coarsest 
possible 
Myhill-Nerode 
relation 
for 
Rand 
corresponds 
to 
the 
unique 
minimal 
finite 
automaton 
for 
R. 
Lemma 
16.2 
Let 
R 
S;;; 
regular 
or 
not. 
The 
relation 
=R 
defined 
by 
(16.1) 
is 
a 
fight 
congruence 
refining 
Rand 
is 
the 
coarsest 
such 
relation 
on 
Proof. 
To 
show 
that 
=R 
is 
a 
right congruence, 
take 
z 
= 
aw 
in 
the 
definition 
of 
=R: 
x 
=R 
y 
:::} 
'</a 
E 
'</w 
E 
E 
R 
{::::::::> 
yaw 
E 
R) 
:::} 
'</a 
E 
(xa 
=R 
ya). 
To 
show 
that 
=R 
refines 
R, 
take 
z 
= 
E 
in 
the 
definition 
of 
=R: 
x 
=R 
Y 
:::} 
(x 
E 
R 
{::::::::> 
Y 
ER). 
Moreover, 
=R 
is 
the 
coarsest 
such 
relation, 
because 
any 
other 
equivalence 
relation 
= 
satisfying 
(i) 
and 
(ii) 
re 
fines 
=R: 
X=Y 
:::} 
'</z 
(xz 
= 
yz) 
:::} 
'</z 
(xz 
E 
R 
{::::::::> 
yz 
E 
R) 
:::} 
X 
=R 
Y 
by 
induction 
on 
Izl, 
using 
property 
(i) 
property 
(ii) 
of 
=R. 
o 
At 
this 
point 
all 
the 
hard 
work 
is 
done. 
We 
can 
now 
state 
and 
prove 
the 
Myhill-Nerode 
theorem: 
Theorem 
16.3 
(Myhill-Nerode 
theorem) 
Let 
R 
S;;; 
The 
following 
statements 
are 
equivalent: 
(a) 
R 
is 
regular; 
(b) 
there 
exisl8 
a 
Myhill-NrTodf 
rduti,1n 
Jor 
R: 
(c) 
lht 
rtlation 
=R 
is 
01 
jinitf 
iTd/u. 

_____________________________________________
98 
Lecture 
16 
Proof. 
(a) 
'* 
(b) 
Given 
a 
DFA 
M 
for 
R, 
the 
construction 
M 
t-+ 
=M 
produces 
a 
Myhill-Nerode 
relation 
for 
R. 
(b) 
'* 
(c) 
By 
Lemma 
16.2, 
any 
Myhill-Nerode 
relation 
for 
R 
is 
of 
finite 
index 
and 
refines 
=R; 
therefore 
=R 
is 
of 
finite 
index. 
(c) 
'* 
(a) 
If 
=R 
is 
of 
finite 
index, 
then 
it 
is 
a 
Myhill-Nerode 
relation 
for 
R, 
and 
the 
construction 
= 
t-+ 
M=. 
produces 
a 
DFA 
for 
R. 
0 
Since 
=R 
is 
the 
unique 
coarsest 
Myhill-Nerode 
relation 
for 
a 
regular 
set 
R, 
it 
corresponds 
to 
the 
DFA 
for 
R 
with 
the 
fewest 
states 
among 
all 
DFAs 
for 
R. 
The 
collapsing 
algorithm 
of 
Lecture 
14 
actually 
gives 
this 
automaton. 
pose 
M 
= 
(Q, 
E, 
li, 
s, 
F) 
is 
a 
DFA 
for 
R 
that 
is 
already 
collapsed; 
that 
is, 
there 
are 
no 
inaccessible 
states, 
and 
the 
collapsing 
relation 
d.,f 
q 
-$==} 
'Ix 
E 
E* 
(li(p,x) 
E 
F 
-$==} 
li(q,x) 
E 
F) 
is 
the 
identity 
relation 
on 
Q. 
Then 
the 
Myhill-Nerode 
relation 
=M 
sponding 
to 
M 
is 
exactly 
=R: 
X=RY 
-$==} 
Vz 
E 
E* 
(xz 
ER-$==} 
YZ 
E 
R) 
definition 
of 
=R 
* 
-$==} 
Vz 
E 
E 
(li(s,xz) 
E 
F 
-$==} 
t5(s,yz) 
E 
F) 
definition 
of 
acceptance 
{:::=:> 
Vz 
E 
E* 
(6(6(s,x),z) 
E 
F 
{:::=:> 
6(6(s,y),z) 
E 
F) 
6(s, 
x) 
6(s,y) 
-$==} 
b(s,x) 
= 
b(s,y) 
An 
Application 
Homework 
1, 
Exercise 
3 
definition of 
since 
M 
is 
collapsed 
definition 
of 
=M. 
The 
Myhill-Nerode 
theorem 
can 
be 
used 
to 
determine 
whether 
a 
set 
R 
is 
regular 
or 
nonregular 
by 
deterrnining 
the 
number 
of 
=R-classes. 
For 
example, 
consider 
the 
set 
A 
= 
{anb
n 
In 
2: 
O}. 
If 
k 
=I 
m, 
then 
a
k 
a
m
, 
since 
akb
k 
E 
A 
but 
amb
k 
fi. 
A. 
Therefore, 
there 
are 
infinitely 
many 
=A-classes, 
at 
least 
one 
for 
each 
a
k
, 
k 
2: 
0. 
By 
the 
Myhill-Nerode 
theorem, 
A 
is 
not 
regular. 
In 
fact, 
one 
can 
show 
that 
the 
=Kclasses 
are 
exactly 
Gk 
= 
{a
k
}, 
k 
2: 
0, 

_____________________________________________
The 
Myhill-Nerode 
Theorem 
99 
Hk 
= 
{anHb
n 
11 
:5 
n}, 
k 
0, 
E 
= 
r;* 
-
U 
Gk 
U 
Hk 
= 
r;* 
-
{amb
n 
10:5 
n 
:5 
m}. 
For 
strings 
in 
Gk, 
all 
and 
only 
strings 
in 
{anb
n
+
k 
I 
n 
O} 
can 
be 
appended 
to 
obtain 
astring 
in 
Ai 
for 
strings 
in 
Hk, 
only 
the 
sbring 
b
k 
can 
be 
appended 
to 
obtain 
astring 
in 
Ai 
and 
no 
string 
can 
be 
appended 
to 
astring 
in 
E 
to 
obtain 
astring 
in 
A. 
We 
will 
see 
another 
application 
of 
the 
Myhill-Nerode 
theorem 
involving 
two-way 
finite 
automata 
in 
Lectures 
17 
and 
18. 
Historical 
Notes 
Minimization 
of 
DFAs 
was 
studied 
by 
Huffman 
[61], 
Moore 
[90], 
Nerode 
[94], 
and 
Hopcroft 
[59], 
among 
others. 
The 
Myhill-Nerode 
theorem 
is 
due 
independently 
to 
Myhill 
[91] 
and 
Nerode 
[94] 
in 
slightly 
different 
forms. 

_____________________________________________
Supplementary 
Lecture 
B 
Collapsing 
Nondeterministic 
Automata 
With 
respect 
to 
minimization, 
the 
situation 
for 
nondeterministlc 
automata 
is 
not 
as 
satisfactory 
as 
that 
for 
deterministic 
automata. 
For 
example, 
mal 
NFAs 
are 
not 
necessarily 
unique 
up 
to 
isomorphism 
(Miscellaneous 
ercise 
60). 
However, 
t>art 
of 
the 
Myhill-N 
erode 
theory 
developed 
in 
Lectures 
13 
through 
16 
does 
ger.eralize 
to 
NFAs.The 
generalization 
is 
based 
on 
the 
notion 
of 
bisimulation, 
an 
important 
concept 
in 
the 
theory 
of 
concurrency 
[87]. 
In 
this 
lecture 
we 
brießy 
investigate 
this 
connection. 
The 
version 
of 
bisimulation 
we 
consider 
here 
is 
called 
strong 
bisimulation 
in 
the 
concurrency 
literature. 
There 
are 
weaker 
forms 
that 
apply 
too. 
We 
show 
that 
bisimulation 
relations 
between 
nondeterministic 
automata 
and 
ing 
relations 
on 
deterministic 
automata 
are 
strongly 
related. 
The 
former 
generalize 
the 
latter 
in 
two 
significant 
ways: 
they 
work 
for 
nondeterministic 
automata, 
and 
they 
can 
relate 
two 
different 
automata. 
Bisi 
m u 
lation 
Let 
M 
= 
(Q",. 
6.
M
, 
SM, 
FM), 
X 
= 
(QN. 
!J.
N
Ł 
SN, 
F
N
) 
be 
two 
NFAs. 
Recall 
that 
for 
NFAs, 
6.(p,a) 
is 
a 
set 
of 
states. 

_____________________________________________
-
Collapsing 
Nondeterministic 
Automata 
101 
Let 
be 
a 
binary 
relation 
relating 
states 
of 
M 
with 
states 
of 
Ni 
that 
is, 
is 
a 
subset 
of 
QM 
x 
QN. 
For 
B 
QN, 
define 
C::::l(B) 
{p 
E 
QM 
13q 
E 
B 
p 
q}, 
the 
set 
of 
all 
states 
of 
M 
that 
are 
related 
via 
to 
some 
state 
in 
B. 
Similarly, 
for 
A 
QM, 
define 
C::::l(A) 
{q 
E 
QN 
13p 
E 
A 
p 
q}. 
The 
relation 
can 
be 
extended 
in 
a 
natural 
way 
to 
subsets 
of 
QM 
and 
QN: 
for 
A 
Q 
M 
and 
B 
Q 
N, 
A 
B 
A 
C::::l(B) 
and 
B 
C::::l(A) 
(B.l) 
<==} 
Vp 
E 
A 
3q 
E 
B' 
p 
q 
and 
Vq 
E 
B 
3p 
E 
A 
p 
q. 
Note 
that 
{p} 
{q} 
iff 
p 
q 
and 
that 
B 
B' 
implies 
C::::l(B'). 
Definition 
8.1 
The 
relation 
is 
called 
abisimulation 
if 
the 
following 
three 
conditions 
are 
met: 
Lemma 
B 2 
(ii) 
if 
P 
q, 
then 
for 
all 
a 
E 
ßM(p,a) 
ßN(q,a)i 
and 
(iii) 
if 
P 
q, 
then 
pE 
FM 
iff 
q 
E 
F
N
. 
o 
Note 
the 
similarity 
of 
these 
conditions 
to.the 
defining 
conditions 
of 
ing 
relations 
on 
DFAs 
from 
Lecture 
13. 
We 
say 
that 
M 
and 
N 
are 
bisimilar 
if 
there 
exists 
abisimulation 
between 
them. 
The 
bisimilarity 
dass 
of 
M 
is 
the 
family 
of 
all 
NFAs 
that 
are 
bisimilar 
to 
M. 
We 
will 
show 
that 
bisimilar 
automata 
accept 
the 
same 
set 
and 
that 
e.J{ery 
bisimilarity 
dass 
contains 
a 
unique 
minimal 
NFA 
that 
ra.n 
be 
obtained 
by 
a 
collapsing 
construction. 
First 
let's 
establish 
some 
basic 
consequences 
of 
Definition 
B.l. 
(i) 
Bisimulation 
is 
symmetrie: 
if::::; 
is 
abisimulation 
between 
M 
and 
N, 
then 
its 
revusc 
{(q,p) 
I 
p::::; 
q} 
is 
abisimulation 
betwctn 
N 
and 
M. 
(ii) 
Bisimulation 
is 
transitive: 
if 
is 
abisimulation 
between 
M 
and 
N 
and 
is 
abisimulation 
between 
IV 
and 
P, 
then 
their 
composition 

_____________________________________________
102 
Supplementary 
Lecture 
B 
0 

{(p, 
r) 
I 
3q 
P 
q 
anrl 
q 
r} 
is 
abisimulation 
between 
M 
and 
P. 
(iii) 
The 
union 
of 
any 
nonempty 
family 
of 
bisimulations 
between 
M 
and 
N 
is 
a 
bisimulation 
between 
M 
and 
N. 
Proof. 
All 
three 
properties 
follow 
quite 
easily 
from 
the 
definition 
of 
ulation. 
We 
argue 
(iii) 
explicitly. 
Let 
I 
i 
E 
I} 
be 
a 
nonem 
pty 
indexed 
set 
of 
bisimulations 
between 
M 
and 
N. 
Define 
<!!f 
U 
...... 
-
"....". 
iEI 
Thus 
p 
q 
<=> 
3i 
E 
I 
p 
q. 
Sinee 
I 
is 
nonempty, 
SM 
SN 
for 
some 
i 
E 
I, 
therefore 
SM 
SN. 
If 
p 
q, 
then 
for 
some 
i 
E 
I, 
P 
q. 
ß(p,a) 
ß(q,a) 
and 
ß(p,a) 
ß(q,a). 
Finally, 
if 
p 
q, 
then 
p 
q 
for 
some 
i 
E 
I, 
whenee 
pE 
FM 
iff 
q 
E 
FN. 
0 
Lemma 
8.3 
Let 
be 
abisimulation 
between 
M 
and 
N. 
1f 
A 
B, 
then 
for 
all 
x 
E 
E* 
I 
LiM(A,x) 
LiN(B,x). 
Proof. 
Suppose 
A 
B. 
For 
x 
= 
•, 
LiM(A, 
c:) 
= 
A 
:::::: 
B 
= 
LiM(B, 
c:). 
For 
x 
= 
a 
E 
E, 
sinee 
A 
if 
p 
E 
Athen 
there 
exists 
q 
E 
B 
such 
that 
p 
q. 
By 
Definition 
B.1(ii), 
ßM(p,a) 
Therefore, 
LiM(A,a) 
= 
U 
ßM(p,a) 
pEA 
By 
a 
symmetrie 
argument, 
LiN(B,a) 
Therefore, 
LiM(A,a) 
LiN(B,a). 
(B.2) 
Proeeeding 
by 
induetion, 
suppose 
that 
LiM(A,x) 
LiN(B,x). 
By 
(B.2) 
and 
Lemma 
6.1, 
LiM(A,xa) 
= 
LiM(LiM(A,x),a) 
LiN(LiN(B, 
x), 
a) 
= 
LiN(B, 
xa). 
o 

_____________________________________________
Collapsing 
Nondeterministic 
Automata 
103 
Theorem 
B.4 
Bisimilar 
automata 
accept 
the 
same 
set. 
Proof. 
Suppose 
:::::: 
is 
abisimulation 
between 
M 
and 
N. 
By 
Definition 
B.l(i) 
and 
Lemma 
B.3,Jor 
any 
x 
E 
L\M(SM,X):::::: 
L\N(SN,X). 
By 
Definition 
B.l(iii), 
L\M(SM, 
x) 
n 
FM 
=I 
0 
Hf 
L\N(SN, 
x) 
n 
FN 
=I 
0. 
By 
definition 
of 
acceptance 
for 
nondeterministic 
automata, 
x 
E 
L(M) 
iff 
x 
E 
L(N). 
Since 
x 
is 
arbitrary, 
L(M) 
= 
L(N). 
0 
In 
fact, 
one 
can 
show 
that 
if 
M 
and 
N 
are 
bisimilar, 
then 
(B.l) 
is 
a 
lation 
between 
the 
deterministic 
automata 
obtained 
from 
M 
and 
N 
by 
the 
subset 
construction 
(Miscellaneous 
Exercise 
64). 
As 
with 
the 
deterministic 
theory, 
minimization 
involves 
elimination 
of 
accessible 
states 
and 
collapsing. 
Here's 
how 
we 
deal 
with 
accessibility. 
Let 
:::::: 
be 
abisimulation 
between 
M 
and 
N. 
The 
support 
of:::::: 
in 
M 
is 
the 
set 
C-;::::(QN), 
the 
set 
of 
states 
of 
M 
that 
are 
related 
by 
:::::: 
to 
some 
state 
of 
N. 
Lemma 
B.5 
Astate 
of 
M 
is 
in 
the 
support 
of 
all 
bisimulations 
involving 
M 
if 
and 
onlv 
if 
it 
is 
accessible. 
Proof. 
Let:::::: 
be 
an 
arbitrary 
bisimulation 
between 
M 
and 
another 
ton. 
By 
Definition 
B.l(i), 
every 
start 
state 
of 
M 
is 
in 
the 
support 
of 
::::::j 
and 
by 
Definition 
B.l(ii), 
if 
pis 
in 
the 
support 
of 
::::::, 
then 
every 
element 
of 
Ll(p, 
a) 
is 
in 
the 
support 
of 
:::::: 
for 
every 
a 
E 
It 
follows 
inductively 
that 
every 
accessible 
state 
of 
M 
is 
in 
the 
support 
of 
::::::. 
Conversely, 
it 
is 
not 
difficult 
to 
check 
that 
the 
relation 
{(p, 
p) 
I 
p 
is 
acceSSl 
ble 
} 
(B.3) 
is 
abisimulation 
between 
M 
and 
itself. 
If 
astate 
is 
in 
the 
support 
of 
all 
bisimulations, 
then 
it 
must 
be 
in 
the 
support 
of 
(B.3), 
therefore 
ble. 
0 
Autobisi 
m u 
lation 
Definition 
B.6 
An 
autobisimulation 
is 
abisimulation 
between 
an 
automaton 
and 
itself. 
0 
Theorem 
B.7 
Any 
nondeterministic 
automaton 
M 
has 
a 
coarsest 
autobisimulation 
=M. 
The 
relation 
=M 
is 
an 
equivalence 
relation. 
Proof. 
Let 
B 
be 
the 
set 
of 
all 
autobisimulations 
on 
M. 
The 
set 
B 
is 
nonempty, 
since 
it 
contains 
the 
identity 
relation 
at 
least. 
Let 
=M 
be 
the 
union 
of 
all 
the 
relations 
in 
B. 
By 
Lemma 
B.2(iii), 
=M 
is 
itself 
in 
Band 
is 
refined 
by 
every 
element 
of 
B. 
The 
relation 
=M 
is 
reflexive, 
since 
the 
identity 
relation 
is 
in 
B, 
and 
is 
symmetrie 
and 
transitive 
by 
Lemma 
B.2(i) 
and 
(ii). 
0 

_____________________________________________
104 
Supplementary 
Lecture 
B 
We 
can 
now 
remove 
inaccessible 
states 
and 
collapse 
by 
the 
maximal 
bisimulation 
to 
get 
a 
minimal 
NFA 
bisimilar 
to 
the 
original 
NFA. 
Let 
M= 
(Q, 
E, 
ß, 
S, 
F). 
We 
have 
already 
observed 
that 
the 
accessible 
subautomaton 
of 
M 
is 
ilar 
to 
M 
under 
the 
bisimulation 
(B.3), 
so 
we 
can 
assume 
without 
ioss 
of 
generality 
that 
M 
has 
no 
inaccessible 
states. 
Let 
== 
be 
==M, 
the 
maximal 
autobisimulation 
on 
M. 
For 
p 
E 
Q, 
let 
[PI 
denote 
the 
==-equivalence 
dass 
of 
p, 
and 
let 
be 
the 
relation 
relating 
p 
to 
its 
==-equivalence 
class: 
d,g 
{(p, 
[PD 
I 
p 
E 
Q}. 
For 
any 
A 
Q, 
define 
AI 
{[P]I 
p 
E 
A}. 
Lemma 
B.8 
For 
all 
A,B 
Q, 
(i) 
A 
C=(B) 
AI 
BI, 
(ii) 
A 
== 
B 
AI 
= 
BI, 
and 
(
00') 
A> 
A' 
tU 
"" 
Ł 
(B.4) 
These 
properties 
are 
straightforward 
consequences 
of 
the 
definitions 
and 
are 
left 
as 
exercises 
(Miscellaneous 
Exercise 
62). 
Now 
define 
the 
quotient 
automaton 
MI 
d,g 
(QI, 
E, 
ß/, 
SI, 
F
I
), 
where 
QI, 
SI, 
and 
F
I 
refer 
to 
(B.4) 
and 
ß/([p],a) 
ß(p,a)'. 
The 
function 
ß' 
is 
weIl 
defined, 
because 
[PI 
= 
[q] 
=? 
P 
== 
q 
=? 
ß(p,a) 
== 
ß(q,a) 
=? 
ß(p,a)' 
= 
ß(q,a)' 
Definition 
B.l(ii) 
Lemma 
B.8(ii). 
Lemma 
B.9 
The 
relation 
is 
abisimulation 
between 
M 
and 
MI. 
Proof. 
By 
Lemma 
B.8(iii), 
we 
have 
S 
S', 
and 
if 
p 
[q], 
then 
p 
== 
q. 
Therefore, 
ß(p,a) 
ß(p,a)' 
= 
ß'([p],a) 
= 
ß'([q],a). 

_____________________________________________
Collapsing 
Nondeterministic 
Automata 
105 
This 
takes 
eare 
of 
start 
states 
and 
transitions. 
For 
the 
final 
states, 
if 
p 
E 
F, 
then 
[P] 
E 
F
'
. 
Conversely, 
if 
[P] 
E 
F
'
, 
there 
exists 
q 
E 
[P] 
such 
that 
q 
E 
F; 
then 
p 
== 
q, 
therefore 
pE 
F. 
0 
By 
Theorem 
BA, 
M 
and 
Mt 
aeeept 
the 
same 
set. 
Lemma 
B.10 
The 
only 
autobisimulation 
on 
M' 
is 
the 
identity 
relation 
=. 
Proof. 
Let", 
be 
an 
autobisimulation 
on 
M'. 
If 
rv 
related 
two 
distinet 
states, 
then 
the 
eomposition 
(B.5) 
where 
.:S 
is 
the 
reverse 
of 
;::;, 
would 
relate 
two 
non-==M-equivalent 
states 
of 
M, 
eontradieting 
the 
maximality 
of 
==M. 
Thus 
rv 
is 
a 
subset 
of 
the 
identity 
relation. 
On 
the 
other 
hand, 
if 
there 
is 
astate 
lP] 
of 
M' 
that 
is 
not 
related 
to 
itself 
by 
"', 
then 
the 
state 
p 
of 
M 
is 
not 
related 
to 
any 
state 
of 
M 
und 
er 
(B.5), 
eontradieting 
Lemma 
B.5 
and 
the 
assumption 
that 
all 
states 
of 
M 
are 
aeeessibie. 
0 
Theorem 
B.11 
Let 
M 
be 
an 
NFA 
with 
no 
inaccessible 
states 
and 
let 
==M 
be 
the 
mal 
autobisimulation 
on 
M. 
The 
quotient 
automaton 
M' 
is 
the 
minimal 
automaton 
bisimilar 
to 
M 
and 
is 
unique 
up 
to 
isomorphism. 
Proof. 
To 
show 
this, 
it 
will 
suffiee 
to 
show 
that 
for 
any 
automaton 
N 
ilar 
to 
M, 
if 
we 
remove 
inaeeessible 
states 
and 
then 
eollapse 
the 
resulting 
NFA 
by 
its 
maximal 
autobisimulation, 
we 
obtain 
an 
automaton 
isomorphie 
to 
M'. 
Using 
(B.3), 
we 
can 
assume 
without 
lass 
of 
generality 
that 
N 
has 
no 
eessible 
states. 
Let 
==N 
be 
the 
maximal 
autobisimulation 
on 
N, 
and 
let 
N' 
be 
the 
quotient 
automaton. 
By 
Lemmas 
B.2 
and 
B.9, 
M' 
and 
N' 
are 
bisimilar. 
We 
will 
show 
that 
any 
bisimulation 
between 
M' 
and 
N' 
gives 
a 
one-to-one 
eorrespondenee 
between 
the 
states 
of 
M' 
and 
N'. 
This 
establishes 
the 
result, 
since 
a 
ulation 
that 
is 
a 
one-to-one 
correspondenee 
eonstitutes 
an 
isomorphism 
(Miscellaneous 
Exercise 
63). 
Let;::;;; 
be 
abisimulation 
between 
M' 
and 
N'. 
U 
nder 
;::;;;, 
every 
state 
of 
M' 
is 
related 
to 
at 
least 
one 
state 
uf 
N', 
and 
every 
state 
of 
N' 
is 
related 
to 
at 
most 
one 
state 
of 
M'; 
otherwise 
the 
eornposition 
of 
;::;;; 
with 
its 
reverse 
would 
not 
be 
the 
identity 
on 
M', 
contradicting 
Lemma 
B.lO. 
Therefore, 
;::;;; 
embeds 
M' 
into 
N' 
injeetively 
(i.e., 
in 
a 
one-to-one 
fashion). 
By 
asymmetrie 
argument, 
the 
reverse 
of 
;::;;; 
embeds 
N' 
into 
M' 
injectively. 
Therefore, 
;::;;; 
gives 
a 
one-to-one 
correspondence 
between 
the 
states 
of 
M' 
and 
N'. 
0 

_____________________________________________
106 
Supplementary 
Lecture 
B 
Ł 
An 
Aigorithm 
Here 
is 
an 
algorithm 
for 
computing 
the 
maximal 
bisimulation 
between 
any 
given 
pair 
of 
NFAs 
M 
and 
N. 
There 
may 
exist 
no 
bisimulation 
between 
M 
and 
N, 
in 
which 
case 
the 
algorithm 
halts. 
and 
reports 
failure. 
For 
the 
case 
M 
= 
N, 
the 
algorithm 
computes 
the 
maximal 
autobisimulation. 
The 
algorithm 
is 
a 
direct 
generalization 
of 
the 
algorithm 
of 
Lecture 
14. 
As 
in 
Lecture 
14, 
the 
algorithm 
will 
mark 
pairs 
of 
states 
(p, 
q), 
where 
p 
E 
Q 
M 
and 
q 
E 
Q 
N. 
A 
pair 
(p, 
q) 
will 
be 
marked 
when 
a 
proof 
is 
discovered 
that 
p 
and 
q 
cannot 
be 
related 
by 
any 
bisimulation. 
1. 
Write 
down 
a 
table 
of 
all 
pairs 
(p, 
q), 
initially 
unmarked. 
2. 
Mark 
(p, 
q) 
if 
p 
E 
FM 
and 
q 
fJ. 
F
N 
or 
vice 
versa. 
3. 
Repeat 
the 
following 
until 
no 
more 
changes 
occur: 
if 
(p, 
q) 
is 
unmarked, 
and 
if 
for 
some 
a 
E 
either 
Ł 
there 
exists 
p' 
E 
AM 
(p, 
a) 
such 
that 
for 
all 
q' 
E 
AN 
(q, 
a), 
(p', 
q') 
is 
marked, 
or 
Ł 
there 
exists 
q' 
E 
AN(q,a) 
such 
that 
for 
all 
p' 
E 
AM(p,a), 
(p',q') 
is 
marked, 
then 
mark 
(p,q). 
4. 
Define 
p 
== 
q 
Hf 
(p, 
q) 
is 
never 
marked. 
Check 
whether 
SM 
== 
SN. 
If 
so, 
then 
== 
is 
the 
maximal 
bisimulation 
between 
M 
and 
N. 
If 
not, 
then 
no 
bi 
simulation 
between 
M 
and 
N 
exists. 
One 
can 
easily 
prove 
by 
induction 
on 
the 
stages 
of 
this 
algorithm 
that 
if 
the 
pair 
(p, 
q) 
is 
ever 
marked, 
then 
p 
q 
for 
any 
bisimulation 
between 
M 
and 
N, 
because 
we 
only 
mark 
pairs 
that 
violate 
some 
condition 
in 
the 
definition 
of 
bisimulation. 
Therefore, 
any 
bisimulation 
is 
a 
refinement 
of 
==. 
In 
particular, 
the 
maximal,bisimulation 
between 
M 
and 
N, 
if 
it 
exists, 
is 
a 
refinement 
of 
==. 
If 
SM 
t 
SN, 
then 
the 
same 
is 
true 
for 
any 
reflnement 
of 
==; 
in 
this 
case, 
no 
bisimulation 
exists. 
On 
the 
other 
hand, 
suppose 
SM 
== 
SN. 
To 
show 
that 
the 
algorithm 
is 
correct, 
we 
need 
only 
show 
that 
== 
is 
abisimulation; 
then 
it 
must 
be 
the 
maximal 
one. 
We 
have 
SM 
:= 
SN 
by 
assumption. 
Also, 
== 
respects 
the 
transition 
functions 
of 
M 
and 
N 
because 
of 
step 
3 
of 
the 
algorithm 
and 
respects 
final 
states 
of 
M 
and 
N 
because 
of 
step 
2 
of 
the 
algorithm. 
We 
have 
shown: 

_____________________________________________
Collapsing 
Nondeterministic 
Automata 
107 
Theorem 
8.12 
The 
algorithm 
above 
correctly 
computes 
the 
maximal 
bisimulation 
between 
two 
NFAs 
i/ 
abisimulation 
exists. 
I/ 
no 
bisimulation 
exists, 
the 
algorithm 
halts 
and 
reports 
/ailure. 
I/ 
both 
automata 
are 
the 
same, 
the 
algorithm 
computes 
the 
maximal 
autobisimulation. 

_____________________________________________
Supplementary 
Lecture 
C 
Automata 
on 
Terms 
The 
theory 
of 
finite 
automata 
has 
many 
interesting 
and 
useful 
tions 
that 
allow 
more 
general 
types 
of 
inputs, 
such 
as 
infinite 
strings 
and 
finite 
and 
infinite 
trees 
In 
this 
lecture 
and 
the 
next 
we 
will 
study 
one 
such 
generalization: 
finite 
automata 
on 
terms, 
also 
known 
as 
finite 
labeled 
trees. 
This 
generalization 
is 
quite 
natural 
and 
has 
a 
decidedly 
algebraic 
flavor. 
In 
particular, 
we 
will 
show 
that 
the 
entire 
Myhill-Nerode 
theory 
developed 
in 
Lectures 
13 
through 
16 
is 
really 
a 
consequence 
of 
basic 
results 
in 
sal 
algebra, 
a 
branch 
of 
algebra 
that 
deals 
with 
general 
algebraic 
concepts 
such 
as 
direct 
product, 
homomorphism, 
homomorphic 
image, 
and 
quotient 
algebra. 
Signatures 
and 
Terms 
A 
signature 
is 
an 
alphabet 
:E 
consisting 
of 
various 
function 
and 
relation 
symbols 
in 
which 
each 
symbol 
is 
assigned 
a 
natural 
number, 
called 
its 
arity. 
An 
element 
of 
:E 
is 
called 
constant, 
unary, 
binary, 
ternary, 
or 
n-ary 
if 
its 
arity 
is 
0, 
1, 
2, 
3, 
or 
n, 
respectively. 
We 
regard 
an 
n-ary 
function 
or 
relation 
symbol 
as 
denoting 
some 
(as 
yet 
unspecified) 
function 
or 
relation 
of 
n 
inputs 
on 
some 
(as 
yet 
unspecified) 
domain. 
For 
example, 
the 
signature 
of 
monoids 
consists 
of 
two 
function 
symbols, 
a 
binary 
multiplication 
symbol 
. 
and 
a 
constant 
1 
for 
the 
multiplicative 

_____________________________________________
Automata 
on 
Terms 
109 
identity. 
The 
signature 
of 
groups 
consists 
of 
the 
symbols 
for 
monoids 
plus 
a 
unary 
function 
symbol 
-1 
for 
multiplicative 
inverse. 
The 
signature 
of 
Kleene 
algebra 
consists 
of 
two 
binary 
function 
symbols 
+ 
and 
., 
one 
unary 
function 
symbol 
*, 
and 
two 
constants 
0 
and 
l. 
Informally, 
a 
ground 
term 
over 
is 
an 
expression' 
built 
from 
the 
function 
symbols 
of 
that 
respects 
the 
arities 
of 
all 
the 
symbols. 
The 
set 
of 
ground 
terms 
over 
is 
denoted 
TE. 
Formally, 
(i) 
any 
constant 
function 
symbol 
c 
E 
is 
in 
TE; 
and 
(ii) 
if 
t1, 
.
.. 
, 
t
n 
E 
TE 
and 
I 
is 
an 
n-ary 
function 
symbol 
then 
Itl 
... 
t
n 
E 
TE' 
We 
can 
picture 
the 
term 
Itl 
... 
t
n 
as 
a 
labeled 
tree 
I 
t
1 
t2 
t
n 
Actually, 
(i) 
is 
a 
special 
case 
of 
(ii): 
the 
precondition 
"if 
tI, 
... 
, 
t
n 
E 
TE" 
is 
vacuously 
true 
when 
n 
= 
O. 
For 
example, 
if 
I 
is 
binary, 
9 
is 
unary, 
and 
a, 
bare 
constants, 
then 
the 
following 
are 
examples 
of 
terms: 
a 
lab 
Igblaa 
or 
pictorially, 
a 
I 
I 
/\ 
a 
b 
9 
1 
/\ 
b 
a 
a 
The 
term 
Itl 
... 
t
n 
is 
an 
expression 
representing 
the 
result 
of 
applying 
an 
n-ary 
function 
denoted 
by 
1 
to 
n 
inputs 
denoted 
by 
t
1
, 
... 
, 
tn, 
although 
we 
have 
not 
yet 
said 
what 
the 
function 
denoted 
by 
1 
iso 
So 
far, 
1 
is 
just 
an 
uninterpreted 
symbol, 
and 
a 
term 
is 
just 
a 
syntactic 
expression 
with 
no 
further 
meaning. 
Even 
though 
we 
don't 
use 
parentheses, 
the 
terms 
t
l
, 
... 
, 
t
n 
are 
uniquely 
termined 
by 
the 
term 
Itt 
... 
t
n 
and 
the 
fact 
that 
I 
is 
n-ary. 
In 
other 
words, 
there 
is 
one 
and 
only 
one 
way 
to 
parse 
the 
string 
It
1 
... 
t
n 
as 
a 
ground 
term. 
A 
formal 
proof 
of 
this 
fact 
is 
given 
as 
an 
exercise 
(Miscellaneous 
Exercise 
94), 
but 
is 
better 
left 
until 
after 
the 
study 
of 
context-free 
languages. 

_____________________________________________
110 
Supplementary 
Lecture 
C 
Note 
that 
if 
there 
are 
no 
constants 
in 
E, 
then 
TE 
is 
empty. 
If 
there 
are 
only 
finitely 
many 
and 
no 
other 
function 
symbols, 
then 
TE 
is 
finite. 
In 
all 
other 
cases, 
TE 
is 
infinite. 
E-algebras 
A 
E-algebra 
is 
a 
structure 
A 
consisting 
of 
a 
set 
A, 
called 
the 
carrier 
of 
A, 
along 
with 
a 
map 
that 
assigns 
a 
function 
JA 
or 
relation 
RA 
of 
the 
appropriate 
arity 
to 
each 
function 
symbol 
J 
E 
E 
or 
relation 
symbol 
R 
E 
E. 
If 
J 
is 
an 
n-ary 
function 
symbol, 
then 
the 
function 
associated 
with 
J 
must 
be 
an 
n-ary 
function 
JA 
: 
An 
-> 
A. 
If 
R 
is 
an 
n-ary 
relation 
symbol, 
then 
the 
relation 
RA 
must 
be 
an 
n-ary 
relation 
RA 
An. 
Constant 
function 
symbols 
c 
are 
interpreted 
as 
O-ary 
functions 
(functions 
with 
no 
inputs), 
which 
are 
just 
elements 
cA 
of 
A. 
A 
unary 
relation 
is 
just 
a 
subset 
of 
A. 
This 
interpretation 
of 
symbols 
of 
E 
extends 
in 
a 
natural 
way 
by 
induction 
to 
all 
ground 
terms. 
Each 
ground 
term 
t 
is 
naturally 
associated 
with 
an 
element 
t
A 
E 
A, 
defined 
inductively 
as 
follows: 
j
A 
def 
jA( 
A 
A) 
h 
... 
t
n 
= 
t
1 
, 
Ł.Ł 
, 
t
n 
. 
This 
includes 
the 
base 
case: 
the 
interpretation 
of 
a 
constant 
c 
as 
an 
element 
cA 
E 
A 
is 
part 
of 
the 
specification 
of 
A. 
Example 
C.1 
Let 
r 
be 
a 
finite 
alphabet. 
The 
monoid 
r* 
is 
an 
algebra 
of 
signature 
.,1. 
The 
carrier 
of 
this. 
algebra 
is 
the 
set 
r*, 
the 
binary 
function 
symbol· 
is 
interpreted 
as 
string 
concatenation, 
:lIld 
the 
constant 
1 
is 
interpreted 
as 
the 
null 
string 
f. 
0 
Example 
C.2 
The 
family 
of 
regular 
sets 
over 
an 
alphabet 
r 
is 
a 
Kleene 
algebra 
in 
which 
+ 
is 
interpreted 
as 
set 
union, 
. 
as 
set 
concatenation, 
* 
as 
asterate, 
0 
as 
the 
null 
set, 
and 
1 
as 
the 
set 
{f}. 
0 
Example 
C.3 
The 
family 
of 
binary 
relations 
on 
a 
set 
X 
is 
also 
a 
Kleene 
algebra 
in 
which 
+ 
is 
interpreted 
as 
set 
union, 
. 
as 
relational 
composition, 
* 
as 
reflexive 
transitive 
closure, 
0 
as 
the 
null 
relation, 
and 
1 
as 
the 
identity 
relation. 
0 
Term 
Aigebras 
Example 
C.4 
Let 
E 
be 
an 
arbitrary 
signature. 
The 
set 
TE 
of 
groundterms 
over 
E 
gives 
a 
family 
of 
E-algebras 
under 
the 
following 
natural 
interpretation: 
for 
n-ary 
j, 

_____________________________________________
Automata 
on 
Terms 
111 
This 
definition 
includes 
constants 
TE 
def 
C 
= 
c. 
The 
particular 
algebra 
depends 
on 
the 
interpretation 
of 
the 
relation 
symbols 
of 
E 
as 
relations 
on 
TI;. 
In 
such 
algebras, 
each 
grouJ,ld 
term 
t 
denotes 
itself: 
t
TE 
= 
t. 
These 
algebras 
are 
called 
syntactic 
or 
term 
algebras. 
0 
Automata 
on 
Terms 
Now 
here's 
something 
interesting. 
Definition 
C.S 
Let 
E 
be 
a 
signature 
consisting 
of 
finitely 
many 
function 
symbols 
and 
a 
single 
unary 
relation 
symbol 
R. 
A 
(deterministic) 
term 
automaton 
over 
E 
is 
a 
finite 
E-algebra. 
0 
Let 
A 
be 
a 
term 
automaton 
over 
E 
with 
carrier 
A. 
We'll 
call 
elements 
of 
Astates. 
The 
states 
satisfying 
the 
unary 
relation 
RA 
will 
be 
called 
final 
or 
accept 
states. 
Since 
a 
unary 
relation 
on 
A 
is 
just 
a 
subset 
of 
A, 
we 
can 
write 
RA(q) 
or 
q 
E 
RA 
interchangeably. 
Inputs 
to 
Aare 
ground 
terms 
over 
Ej 
that 
is, 
elements 
of 
TI;. 
Definition 
C.6 
A 
ground 
term 
t 
is 
said 
to 
be 
accepted 
by 
A 
if 
t
A 
E 
RA. 
The 
set 
of 
terms 
accepted 
by 
Ais 
denoted 
L(A). 
A 
set 
of 
terms 
is 
called 
regular 
if 
it 
is 
L(A) 
for 
so 
me 
A. 
0 
To 
understand 
what 
is 
going 
on 
here, 
think 
of 
a 
ground 
term 
t 
as 
a 
labeled 
tree. 
The 
automaton 
A, 
given 
t 
as 
input, 
starts 
at 
the 
leaves 
of 
t 
and 
works 
upward, 
associating 
astate 
with 
each 
subterm 
inductively. 
If 
there 
is 
a 
stant 
c 
E 
E 
labeling 
a 
particular 
leaf 
of 
t, 
the 
state 
that 
is 
associated 
with 
that 
leaf 
is 
cA. 
If 
the 
immediate 
subterms 
tl, 
... 
, 
t
n 
of 
the 
term 
jtl 
... 
t
n 
are 
labeled 
with 
states 
ql,'" 
,qn, 
respectively, 
then 
the 
term 
jtl 
... 
t
n 
is 
labeled 
with 
the 
state 
jA( 
ql, 
... 
,qn)' 
A 
term 
is 
accepted 
if 
the 
state 
beling 
the 
root 
is 
in 
RA
j 
that 
is, 
if 
it 
is 
an 
accept 
state. 
There 
is 
no 
need 
for 
astart 
statej 
this 
role 
is 
played 
by 
the 
elements 
cA 
associated 
with 
the 
constants 
c 
E 
E. 
Now 
let's 
describe 
the 
relationship 
of 
this 
new 
definition 
of 
automata 
to 
our 
previous 
definition 
and 
explain 
how 
the 
old 
one 
is 
a 
special 
case 
of 
the 
new 
one. 
Given 
an 
ordinary 
DFA 
over 
strings 
M 
= 
(Q, 
E', 
0, 
s, 
F), 
where 
E' 
is 
a 
finite 
alphabet, 
let 
E 
E' 
U 
{O, 
R}, 
where 
0, 
R 
f/. 
E'. 
We 
make 
E 
into 
a 
signature 
by 
declaring 
all 
elements 
of 
E' 
to 
be 
unary 
function 
symbols, 
° 
a 
constant, 
and 
R 
a 
unary 
relation 

_____________________________________________
112 
Supplementary 
Lecture 
( 
symbol. 
There 
is 
a 
one-to-one 
correspondence 
between 
ground 
terms 
over 
E 
and 
strings 
in 
E
/
*: 
the 
string 
ala2'" 
an-la
n 
E 
E
/
* 
corresponds 
to 
the 
ground 
term 
anan-l 
... 
a2al 
D E 
TI;' 
In 
particular, 
the 
empty 
string 
f 
E 
E
/
* 
corresponds 
to 
the 
ground 
term 
D E 
Tn. 
Now 
we 
make 
a 
E-algebra 
out 
of 
M, 
wh 
ich 
we 
will 
denote 
by 
M. 
The 
carrier 
of 
M 
is 
Q. 
The 
symbols 
of 
E 
are 
interpreted 
as 
follows: 
DM 
s, 
aM(q) 
ö(q,a), 
R
M 
F. 
In 
other 
words, 
the 
constant 
D 
is 
interpreted 
as 
the 
start 
state 
of 
M 
j 
the 
symbol 
a 
E 
EI 
is 
interpreted 
as 
the 
unary 
function 
q 
H 
ö(q,a)j 
and 
the 
relation 
symbol 
R 
is 
interpreted 
as 
the 
set 
of 
final 
states 
F. 
It 
is 
not 
difficult 
to 
show 
by 
induction 
that 
M 
ö(s,ala2···an-lan) 
=a
n
a
n
-l···a2
a
l
D 
. 
Therefore, 
ala2'" 
an-la
n 
E 
L(M) 
{:::::} 
b(s,ala2'" 
an-la
n
) 
E 
F 
{:::::} 
anan-l 
... 
a2al 
DM 
E 
R
M 
{:::::} 
anan-l 
... 
a2al 
D E 
L(M). 
It 
should 
be 
pretty 
apparent 
by 
now.that 
much 
of 
automata 
theory 
is 
just 
algebra. 
What 
is 
the 
value 
of 
this 
alternative 
point 
of 
view? 
Let's 
develop 
the 
connection 
a 
little 
further 
to 
find 
out. 
H 
omomorphisms 
A 
central 
concept 
of 
universal 
algebra 
is 
the 
notion 
of 
homomorphism. 
Ine 
tuitively, 
a 
E-algebra 
homomorphism 
is 
a 
map 
between 
two 
E-algebras 
that 
preserves 
all 
algebraic 
structure 
as 
specified 
by 
E. 
Formally, 
Definition 
(,7 
Let 
A 
and 
ß 
be 
two 
E-algebras 
with 
carriers 
A 
and 
B, 
respectively. 
A 
E-algebra 
homomorphism 
from 
A 
to 
ß 
is 
a 
map 
(1 
: 
A 
-+ 
B 
such 
that 
(i) 
for 
all 
n-ary 
function 
symbols 
J 
E 
E 
and 
all 
al, 
... 
,an 
E 
A, 
(1(fA(al'''' 
,an)) 
= 
JB((1(ad, 
... 
,(1(a
n
))j 
(ii) 
for 
all 
n-ary 
relation 
symbols 
R 
E 
E 
and 
all 
ab 
.
.. 
,an 
E 
A, 
AB' 
R 
(ab 
... 
,an)<=?R 
((1(ad, 
... 
,(1(a
n
)). 
0 
Condition 
(i) 
of 
Definition 
C.7 
says 
that 
for 
any 
function 
symbol 
f 
E 
we 
can 
apply 
JA 
to 
al, 
.
.. 
,an 
in 
A 
and 
then 
apply 
the 
homomorphism 
(1 
to 

_____________________________________________
Automata 
on 
Terms 
113 
the 
result 
to 
get 
an 
element 
of 
B, 
or 
we 
can 
apply 
(J 
to 
each 
of 
al, 
... 
,an, 
then 
apply 
fB 
to 
the 
resulting 
elements 
of 
B, 
and 
we 
get 
to 
the 
same 
place. 
Condition 
(ii) 
says 
that 
the 
distinguished 
relation 
R 
holds 
before 
applying 
the 
homomorphism 
if 
and 
only 
If 
it 
holds 
after. 
Example 
(.a 
The 
homomorphisms 
described 
in 
Lecture 
10 
are 
monoid 
homomorphisms. 
Conditions 
(10.1) 
and 
(10.2) 
are 
exactly 
Definition 
C.7(i) 
for 
the 
signature 
., 
1 
of 
monoids. 
0 
Example 
(,9 
Let 
A 
be 
any 
1;-algebra. 
The 
function 
t 
........ 
t
A 
mapping 
a 
ground 
term 
t 
E 
Tr. 
to 
its 
interpretation 
t
A 
in 
A 
satisfies 
Definition 
C.7(i), 
because 
for 
all 
n-ary 
f 
E 
1; 
and 
tl, 
... 
,tn 
E 
'fr., 
pr;(lI, 
... 
,tn)A 
= 
ftl··· 
t
n
A 
= 
jA(tf, 
... 
, 
Moreover, 
it 
is 
the 
only 
function 
Tr. 
-. 
A 
that 
does 
so. 
o 
For 
a 
term 
automaton 
M, 
whether 
or 
not 
the 
map 
t 
........ 
t
M 
satisfies 
tion 
C.7(ii) 
depends 
on 
the 
interpretation 
of 
the 
unary 
relation 
symbol 
R 
in 
the 
term 
algebra 
TE. 
There 
is 
only 
one 
interpretation 
that 
works: 
L(M). 
Thus 
we 
might 
have 
defined 
L(M) 
to 
be 
the 
unique 
interpretation 
of 
R 
in 
Tr. 
making 
the 
map 
t 
........ 
t
M 
a 
homomorphism. 
Definition 
(.10 
A 
homomorphism 
(J 
: 
A 
-. 
B 
that 
is 
onto 
(for 
all 
bEB, 
there 
exists 
a 
E 
A 
such 
that 
(J(a) 
= 
b) 
is 
called 
an 
epimorphism. 
A 
homomorphism 
(J 
: 
A 
-. 
B 
that 
is 
one-to-one 
(for 
all 
a,b 
E 
A, 
if 
(J(a} 
= 
(J(b), 
then 
a 
= 
b) 
is 
called 
a 
monomorphism. 
A 
homomorphism 
that 
is 
both 
an 
epimorphism 
and 
a 
monomorphism 
is 
called 
an 
isomorphism. 
If 
(J 
: 
A 
-. 
B 
is 
an 
epimorphism, 
then 
the 
algebra 
B 
is 
called 
a 
homomorphic 
image 
of 
A. 
0 
Let 
1; 
be 
a 
signature 
consisting 
of 
finitely 
many 
function 
symbols 
and 
a 
single 
unary 
relation 
symbol 
R. 
Let 
A 
<; 
Tr. 
be 
an 
arbitrary 
set 
of 
ground 
terms, 
and 
let 
Tr.(A) 
denote 
the 
term 
algebra 
obtained 
by 
interpreting 
R 
as 
the 
set 
A; 
that 
is, 
RTd
A
) 
= 
A. 
Lemma 
(.11 
The 
set 
Ais 
regular 
if 
and 
only 
if 
the 
algebra 
Tr.(A) 
has 
a 
finite 
phic 
image. 
Proof. 
Once 
we 
have 
stated 
this, 
it's 
easy 
to 
prove. 
A 
finite 
homomorphic 
image 
A 
of 
Tr.(A) 
is 
just 
a 
term 
automaton 
for 
A. 
The 
homomorphism 
is 
the 
interpretation 
map 
t 
........ 
tAo 
The 
inductive 
definition 
of 
this 
map 
corresponds 
to 
a 
run 
of 
the 
automaton. 
We 
leave 
the 
details 
as 
an 
exercise 
(Miscellaneous 
Exercise 
66). 
0 
In 
the 
next 
lecture, 
Supplementary 
Lecture 
D, 
we 
will 
give 
an 
account 
of 
the 
Myhill-N 
erode 
theorem 
in 
this 
more 
general 
setting. 

_____________________________________________
Supplementary 
Lecture 
0 
The 
Myhill-Nerode 
Theorem 
for 
Term 
Automcita 
In 
the 
last 
lecture 
we 
generalized 
DFAs 
on 
sttings 
to 
term 
automata 
over 
a 
signature 
and 
demonstrated 
that 
automata-theoretic 
concepts 
such 
as 
"final 
states" 
and 
"run" 
were 
really 
more 
general 
algebraic 
concepts 
in 
disguise. 
In 
this 
lecture 
we 
continue 
to 
develop 
this 
correspondence, 
leading 
finally 
to 
a 
fuller 
understanding 
of 
the 
Myhill-Nerode 
theorem. 
Congruence 
First 
we 
need 
to 
introduce 
the 
important 
algebraic 
concept 
of 
ence. 
Congruences 
and 
homomorphisms 
go 
hand-in-hand. 
Recall 
from 
plementary 
Lecture 
C 
that 
a 
homomorphism 
between 
two 
is 
a 
map 
that 
preserves 
all 
algebraic 
structure 
(Definition 
C.7). 
Every 
momorphism 
a 
: 
A 
Binduces 
a 
certain 
natural 
binary 
relation 
on 
A: 
U 
=0' 
V 
a(u) 
= 
a(v). 
The 
relation 
=0' 
is 
called 
the 
kerneIl 
of 
a. 
IIf 
you 
have 
taken 
algebra, 
you 
may 
have 
seen 
the 
word 
kernel 
used 
differently: 
normal 
subgroups 
of 
groups, 
ideals 
of 
rings, 
null 
spaces 
of 
linear 
transformations. 
These 
concepts 
are 
closely 
allied 
and 
serve 
tbe 
same 
purpose. 
Tbe 
definition 
of 
kernel 
as 
a 
binary 
relation 
is 
more 
broadly 
applicable. 

_____________________________________________
The 
Myhill-Nerode 
Theorem 
for 
Term 
Automata 
115 
The 
kernel 
of 
any 
homomorphism 
defined 
on 
Ais 
an 
equivalence 
relation 
on 
A 
(reflexive, 
symmetrie, 
transitive). 
It 
also 
respects 
all 
algebraic 
structure 
in 
the 
following 
sense: 
(i) 
for 
all 
n-ary 
function 
symbols 
fEE, 
if 
Ui 
==.,. 
Vi, 
1 
::; 
i 
::; 
n, 
then 
rA.(Ul"'" 
u
n
) 
==.,. 
fA(Vl"'" 
v
n
); 
(ii) 
for 
all 
n-ary 
relation 
symbols 
R 
E 
E, 
if 
Ui 
==.,. 
Vi, 
1 
::; 
i 
::; 
n, 
then 
RA(ul, 
... 
,u
n
) 
{:::::} 
RA(vl, 
... 
,v
n
). 
These 
properties 
follow 
immediately 
from 
the 
properties 
of 
homomorphisms 
and 
the 
definition 
of 
==.,.. 
In 
general, 
a 
congruence 
on 
A 
is 
any 
equivalence 
relation 
on 
A 
satisfying 
properties 
(i) 
and 
(ii): 
Definition 
0.1 
Let 
A 
be 
a 
E-algebra 
with 
carrier 
A. 
A 
congruence 
on 
Ais 
an 
equivalence 
relation 
== 
on 
A 
such 
that 
Lemma 
0.2 
(i) 
for 
all 
n-ary 
function 
symbols 
fEE, 
if 
Ui 
== 
Vi, 
1 
::; 
i 
::; 
n, 
then 
fA(Ul!'" 
,u
n
) 
== 
f-!-(Vl"" 
,v
n
); 
(ii) 
for 
aIl 
n-ary 
relation 
symbols 
R 
E 
E, 
if 
Ui 
== 
Vi, 
1 
::; 
i 
::; 
n, 
then 
RA(Ul"" 
,u
n
) 
<=-=> 
RA(Vl"'" 
v
n
). 
0 
Thus 
the 
kerneI. 
of 
every 
homomorphism 
IS 
a 
congruence. 
Now, 
the 
esting 
thing 
about 
this 
definition 
is 
that 
it 
goes 
the 
other 
way 
as 
weIl: 
every 
congruence 
is 
the 
kernel 
of 
so 
me 
homomorphism. 
In 
other 
words, 
given 
an 
arbitrary 
congruence 
==, 
we 
can 
construct 
a 
homomorphism 
a 
such 
that 
==.,. 
is 
==. 
In 
fact, 
we 
can 
make 
the 
homomorphism 
a 
an 
epimorphism. 
We 
will 
prove 
this 
using 
a 
general 
algebraic 
construction 
called 
the 
quotient 
construction. 
We 
saw 
an 
example 
of 
the 
quotient 
construction 
in 
Lecture. 
13, 
where 
we 
used 
it 
to 
collapse 
a 
DFA. 
We 
saw 
it 
again 
in 
Lecture 
15, 
where 
we 
structed 
an 
automaton 
for 
a 
set 
A 
from 
a 
given 
Myhill-Nerode 
relation 
for 
A. 
By 
now 
you 
have 
probably 
figured 
out 
where 
we 
are 
going 
with 
this: 
the 
coIlapsing 
relations 
of 
Lecture 
13, 
the 
Myhill-Nerode 
relations 
== 
of 
Lecture 
15, 
and 
the 
maximal 
Myhill-Nerode 
relation 
==R 
of 
Lecture 
16 
are 
all 
congruences 
on 
E-algebras! 
We 
warned 
you 
not 
to 
confuse 
these 
ent 
kinds 
of 
relations, 
because 
some 
were 
defined 
on 
automata 
and 
others 
on 
stringsj 
but 
now 
we 
can 
roll 
them 
all 
into 
a 
single 
concept. 
This 
is 
the 
power 
and 
beauty 
of 
abstraction. 
(i) 
The 
kernel 
01 
any 
homomorphism 
is 
a 
congruence. 

_____________________________________________
116 
Supplementary 
Lecture 
D 
(ii) 
Any 
congruence 
is 
the 
kernel 
of 
an 
epimorphism. 
Proof. 
For 
(ii), 
build 
a 
quotient 
algebra 
whose 
elements 
are 
the 
congruence 
classes 
[u]. 
There 
is 
a 
unique 
interpretation 
of 
the 
function 
and 
relation 
symbols 
in 
the 
quotient 
making 
the 
map 
u 
H 
[u] 
an 
epimorphisni. 
We'll 
leave 
the 
details 
as 
an 
exercise 
(Miscellaneous 
Exercise 
67). 
0 
In 
fact, 
if 
the 
map 
t 
1-+ 
t
A 
is 
onto 
(i.e., 
if 
each 
element 
of 
A 
is 
named 
by 
a 
term), 
then 
the 
congruences 
on 
A 
and 
homomorphic 
images 
of 
A 
are 
in 
one-to-one 
correspondence 
up 
to 
isomorphism. 
This 
is 
true 
of 
term 
algebras, 
since 
the 
map 
t 
1-+ 
tTr. 
is 
the 
identity. 
This 
is 
completely 
analogous 
to 
Lemma 
15.3, 
which 
for 
the 
case 
of 
automata 
over 
strings 
gives 
a 
one-to-one 
correspondence 
up 
to 
isomorphism 
between 
the 
Myhill-Nerode 
reiatioI.ls 
for 
A 
(i.e., 
the 
right 
congruences 
of 
finite 
index 
refining 
A) 
and 
the 
DFAs 
with 
no 
inaccessible 
states 
accepting 
A. 
Here 
"no 
inaccessible 
states" 
just 
means 
that 
the 
map 
t 
1-+ 
t
A 
is 
onto. 
The 
Myhill-Nerode 
Theorem 
Recall 
from 
Lecture 
16 
the 
statement 
of 
the 
Myhill-Nerode 
theorem: 
for 
a 
set 
A 
of 
strings 
over 
a 
finite 
alphabet 
the 
following 
three 
conditions 
are 
equivalent: 
(i) 
A 
is 
regular; 
(ii) 
there 
exists 
a 
Myhill-
N 
erode 
relation 
for 
A; 
(iii) 
the 
relation 
=A 
is 
of 
finite 
index, 
where 
x 
=A 
y 
Vz 
E 
(xz 
E 
A 
yz 
E 
A). 
Recall 
that 
finite 
index 
means 
finitely 
many 
equivalence 
classes, 
and 
a 
tion 
is 
Myhill-Nerode 
for 
A 
if 
it 
is 
a 
right 
congruence 
of 
finite 
index 
refining 
A. 
This 
theorem 
generalizes 
in 
a 
natural 
way 
to 
automata 
over 
terms. 
Define 
a 
context 
to 
be 
a 
term 
in 
TEU{:C}, 
where 
xis 
a 
new 
symbol 
of 
arity 
O. 
For 
a 
context 
u 
and 
ground 
term 
tE 
TE, 
denote 
by 
sf(u) 
the 
term 
in 
TE 
obtained 
by 
substituting 
t 
for 
all 
occurrences 
of 
x 
in 
u. 
Formally, 
:c 
( ) 
def 
t 
St 
x 
= , 
... 
t
n
) 
As 
usual, 
the 
last 
line 
includes 
the 
case 
of 
constants: 
:c 
( ) 
def 
St 
C 
= 
c. 

_____________________________________________
The 
Myhill-Nerode 
Theorem 
for 
Term 
Automata 
117 
Let 
L; 
be 
a 
signature 
consisting 
of 
finitely 
many 
function 
symbols 
and 
a 
single 
unary 
relation 
symbol 
R. 
For 
a 
given 
A 
TE 
and 
ground 
terms 
s, 
tE 
TE, 
define 
S 
==A 
t 
for 
an 
contexts 
u, 
E 
A 
{=} 
sf(u) 
E 
A. 
It 
is 
not 
difficult 
to 
argue 
that 
==A 
is 
a 
congruence 
on 
TE(A) 
(Miscellane011s 
Exercise 
68). 
Theorem 
0.3 
(Myhill-Nerode 
theorem 
for 
term 
automata) 
Let 
A 
TE. 
TE(A) 
denote 
the 
term 
algebra 
over 
L; 
in 
which 
R 
is 
interpreted 
as 
the 
unary 
relation 
A. 
The 
following 
statements 
are 
equivalent: 
(i) 
A 
is 
regular; 
(i') 
TE(A) 
has 
a 
finite 
homomorphic 
image; 
(ii) 
there 
exists 
a 
congruence 
of 
finite 
index 
on 
TE(A); 
(iii) 
the 
relation 
==A 
is 
of 
finite 
index. 
Proof. 
We 
have 
already 
observed 
(Lemma 
C.11) 
that 
to 
say 
A 
is 
regular 
(i.e., 
accepted 
by 
a 
term 
automaton) 
is 
just 
another 
way 
of 
saying 
that 
TE(A) 
has 
a 
finite 
homomorphic 
image. 
Thus 
(i) 
and 
(i') 
are 
equivalent. 
(i') 
::} 
(ii) 
If 
TE(A) 
has 
a 
finite 
homomorphic 
image 
under 
epimorphism 
a, 
then 
the 
kernel 
of 
a 
is 
of 
finite 
index, 
since 
its 
congruence 
classes 
are 
in 
one-to-one 
correspondence 
with 
the 
elements 
of 
the 
homomorphic 
image. 
(ii) 
::} 
(iii) 
Let 
== 
be 
any 
congruence 
on 
TE(A). 
We 
show 
that 
== 
refines 
==A; 
therefore, 
==A 
is 
of 
finite 
index 
if 
== 
iso 
Suppose 
s 
== 
t. 
It 
follows 
by 
a 
straightforward 
inductive 
argument 
using 
Definition 
D.l(i) 
that 
for 
any 
context 
u, 
== 
s:(u); 
then 
by 
Definition 
D.l(ii), 
s;(u) 
E 
A 
iffs:(u) 
E 
A. 
Since 
the 
context 
u 
was 
arbitrary, 
s 
==A 
t. 
(iii) 
::} 
(i') 
Since 
==A 
is 
a 
congruence, 
it 
is 
the 
kernel 
of 
an 
epimorphism 
tained 
by 
the 
quotient 
construction. 
Since 
==A 
is 
of 
finite 
index, 
the 
quotient 
algebra 
is 
finite 
and 
therefore 
a 
finite 
homomorphic 
image 
of 
TE(A). 
0 
As 
in 
Lecture 
16, 
the 
quotient 
of 
TE(A) 
by 
==A 
gives 
the 
minimal 
morphic 
image 
of 
TE(A); 
and 
for 
any 
other 
homomorphic 
image 
B, 
there 
is 
a 
unique 
homomorphism 
B 
---> 
TE(A)/==A. 
Historical 
Notes 
Thatcher 
and 
Wright 
[119] 
generalized 
finite 
automata 
on 
strings 
to 
finite 
automata 
on 
terms 
and 
developed 
the 
algebraic connection. 
The 
more 
eral 
version 
of 
the 
Myhill-Nerode 
theorem 
(Theorem 
D.3) 
is 
in 
some 
sense 

_____________________________________________
118 
Supplementary 
Lecture 
D 
an 
inevitable 
consequence 
of 
Myhill 
and 
Nerode's 
work 
[91, 
94] 
since 
cording 
to 
Thatcher 
and 
Wright, 
"conventional 
finite 
automata 
theory 
goes 
through 
for 
the 
generalization-and 
it 
goes 
through 
quite 
neatly" 
[119]. 
The 
first 
explicit 
mention 
of 
the 
equivalence 
of 
the 
term 
analogs 
of 
(i) 
and 
(ii) 
in 
the 
statement 
ofthe 
Myhill-Nerode 
theorem 
seems 
to 
be 
by 
Brainerd 
[13, 
14] 
and 
Eilenberg 
and 
Wright 
[34], 
although 
the 
latter 
claim 
that 
their 
paper 
"contains 
nothing 
that 
is 
essentially 
new, 
except 
perhaps 
for 
a 
point 
of 
view" 
[34]. 
A 
relation 
on 
terms 
analogous 
to 
=A 
was 
defined 
and 
clause 
(iii) 
added 
explicitly 
by 
Arbib 
and 
Give'on 
[5, 
Definition 
2.13], 
although 
it 
is 
also 
essentially 
implicit 
in 
work 
of 
Brainerd 
[13, 
14]. 
Good 
general 
references 
are 
Gecseg 
and 
Steinby 
[42] 
and 
Englefriet 
[35]. 

_____________________________________________
Lecture 
17 
Two-Way 
Finite 
Automata 
Two-way 
finite 
automataare 
similar 
to 
the 
machines 
we 
have 
been 
studying, 
except 
that 
they 
can 
read 
tlie 
input 
string 
in 
either 
direction. 
We 
think 
o( 
them 
as 
having 
a 
read 
head, 
which 
can 
move 
left 
or 
right. 
over 
the 
input 
string. 
Like 
ordinary 
finite 
automata, 
they 
have 
a 
finite 
set 
Q 
of 
states 
and 
can 
be 
either 
deterministic 
(2DFA) 
or 
nondeterministic 
(2NFA). 
Although 
these 
automata 
appear 
much 
more 
powerful 
than 
olle-way 
finite 
automata, 
in 
reality 
they 
are 
equivalent 
in 
the 
sense 
that 
they 
only 
acccpt 
regular 
sets. 
We 
will 
prove 
this 
result 
using 
the 
Myhill-Nerode 
theorem. 
We 
think 
of 
the 
symbols 
of 
the 
input 
string 
as 
occupying 
cells 
of 
a 
finite 
tape, 
one 
symbol 
per 
cello 
The 
input 
string 
is 
enclosed 
in 
left 
and 
right 
endmarkers 
I-
and 
-1, 
which 
are 
not 
elements 
of 
the 
input 
alphabet 
E. 
The 
read 
head 
may 
not 
move 
outside 
of 
the 
endmarkers. 
Q 
Informally, 
the 
machine 
starts 
in 
its 
start 
state 
s 
with 
its 
read 
head 
pointing 
to 
the 
left 
endmarker. 
At 
any 
point 
in 
time, 
the 
machine 
is 
in 
some 
state 
q 
with 
its 
read 
head 
scanning 
some 
tape 
cell 
containing 
an 
input 
symbol 
ai 
or 

_____________________________________________
120 
Lecture 
17 
one 
of 
the 
endmarkers. 
Based 
on 
its 
current 
state 
and 
the 
symbol 
occupying 
the 
tape 
cell 
it 
is 
eurrently 
seanning, 
it 
moves 
its 
read 
head 
either 
left 
or 
right 
one 
eell 
and 
enters 
a 
new 
state. 
It 
accepts 
by 
entering 
a 
special 
aecept 
state 
t 
and 
rejects 
by 
entering 
a 
special 
rejeet.state 
r. 
The 
maehine's 
action 
on 
a 
partieular 
state 
and 
symbol 
is 
determined 
by 
a 
transition 
function 
{) 
that 
is 
part 
of 
the 
speeifieation 
of 
the 
maehine. 
Example 
17.1 
Here 
is 
an 
informal 
deseription 
of 
a 
2DFA 
aceepting 
the 
set 
A 
= 
{x 
E 
{a,b}* 
I 
#a(x) 
is 
a 
multiple 
of 
3 
and 
#b(x) 
is 
even}. 
The 
maehine 
starts 
in 
its 
start 
state 
seanning 
the 
left 
endmarker. 
It 
seans 
left 
to 
right 
over 
the 
input, 
eounting 
the 
number 
of 
a's 
mod 
3 
and 
ignoring 
the 
b's. 
When 
it 
reaehes 
the 
right 
endmarker 
-i, 
if 
the 
number 
of 
a's 
it 
has 
seen 
is 
not 
a 
multiple 
of 
3, 
it 
enters 
its 
rejeet 
state, 
thereby 
rejecting 
the 
input-the 
input 
string 
x 
is 
not 
in 
the 
set 
A, 
sinee 
the 
first 
condition 
is 
not 
satisfied. 
Otherwise 
it 
seans 
right 
to 
left 
over 
the 
input, 
counting 
the 
number 
of 
b's 
mod 
2 
and 
ignoring 
the 
a's. 
When 
it 
reaches 
the 
left 
endmarker 
f-
again, 
if 
the 
number 
of 
b's 
it 
has 
seen 
is 
odd, 
it 
enters 
its 
rejeet 
statej 
otherwise, 
it 
enters 
its 
accept 
state. 
0 
Unlike 
ordinary 
finite 
automata, 
a 
2DFA 
needs 
only 
a 
single 
accept 
state 
and 
a 
single 
reject 
state. 
We 
ean 
think 
of 
it 
as 
halting 
immediately 
when 
it 
enters 
one 
of 
these 
two 
states, 
although 
formally 
it 
keeps 
running 
but 
remains 
in 
the 
accept 
or 
reject 
state. 
The 
machine 
need 
not 
read 
the 
entire 
input 
before 
accepting 
or 
rejecting. 
Indeed, 
it 
need 
not 
ever 
accept 
or 
reject 
at 
all, 
but 
may 
loop 
infinitely 
without 
ever 
entering 
its 
accept 
or 
reject 
state. 
Formal 
Definition 
of 
2DFA 
FormaIly, 
a 
2DFA 
is 
an 
octuple 
M 
= 
(Q, 
f-, 
-i, 
6, 
s, 
t, 
r), 
where 
Ł Q 
is 
a 
finite 
set 
(the 
states), 
Ł 
is 
a 
finite 
set 
(the 
input 
alphabet), 
Ł 
f-
is 
the 
left 
endmarker, 
f-
rt 
Ł 
-i 
is 
the 
right 
endmo,rker, 
-l 
rt 
Ł 
{) 
: 
Q 
x 
U 
{f-, 
-i}) 
-> 
(Q 
x 
{L, 
R}) 
is 
the 
transition 
function 
(L. 
R 
stand 
for 
left 
and 
right, 
respectively), 
Ł s 
E 
Q 
is 
the 
start 
state, 

_____________________________________________
Two-Way 
Finite 
Automata 
121 
Ł t 
E 
Q 
is 
the 
aeeept 
state, 
and 
Ł T 
E 
Q 
is 
the 
rejeet 
state, 
T 
i-
t, 
such 
that 
für 
all 
states 
q, 
6(q,l-) 
6(q,-I) 
= 
(u,R) 
(v, 
L) 
for 
some 
u 
E 
Q, 
for 
some 
v 
E 
Q, 
and 
for 
all 
symbols 
b 
E 
1: 
U 
{I-}, 
c5(t,b) 
= 
(t,R), 
c5(t, 
-I) 
= 
(t, 
L), 
c5(r, 
b) 
= 
(r, 
R), 
6(r, 
-I) 
= 
(r, 
L). 
(17.1) 
(17.2) 
Intuitively, 
the 
function 
c5 
takes 
astate 
and 
a 
symbol 
as 
arguments 
and 
returns 
a 
new 
state 
and 
a 
direction 
to 
move 
the 
head. 
If 
c5(p,b) 
= 
(q,d), 
then 
whenever 
the 
machine 
is 
in 
state 
P 
and 
scanning 
a 
tape 
ceIl 
containing 
symbol 
b, 
it 
moves 
its 
head 
one 
cell 
in 
the 
direction 
d 
and 
enters 
state 
q. 
The 
restrictions 
(17.1) 
prevent 
the 
machine 
from 
ever 
moving 
outside 
the 
input 
area. 
The 
restrictions 
(17.2) 
say 
that 
on 
ce 
the 
machine 
enters 
its 
accept 
or 
reject 
state, 
it 
stays 
in 
that 
state 
and 
moves 
its 
head 
all 
the 
way 
to 
the 
right 
of 
the 
tape. 
The 
octuple 
is 
not 
a 
legal 
2DFA 
if 
its 
transition 
function 
6 
does 
not 
satisfy 
these 
conditions. 
Example 
17.2 
Here 
is 
a 
formal 
description 
of 
the 
2DFA 
described 
informally 
in 
Example 
17.1 
abüve. 
Q 
= 
{qo,ql,q2,PO,Pl,t,r}, 
1: 
= 
{a,b}. 
The 
start, 
accept, 
and 
reject 
states 
are 
qo, 
t, 
and 
r, 
respectively. 
The 
transition 
function 
c5 
is 
given 
by 
the 
following 
table: 
I-
qo 
ql 
q2 
Po 
(t, 
R) 
Pl 
(T, 
R) 
t 
(t, 
R) 
T 
(T, 
R) 
a 
ql,R 
(q2, 
R) 
(qo,R) 
(Po, 
L) 
(Pl, 
L) 
(t, 
R) 
(T, 
R) 
b 
qo,R 
(ql,R) 
(q2,R) 
(Pl, 
L) 
(Po, 
L) 
(t, 
R) 
(T, 
R) 
(t, 
L) 
(T, 
L) 
The 
entries 
marked 
-
will 
never 
occur 
in 
any 
computation, 
so 
it 
doesn't 
matter 
what 
we 
put 
here. 
The 
machine 
is 
in 
states 
qo, 
ql, 
or 
q2 
on 
the 
first 
pass 
over 
the 
input 
from 
left 
to 
right; 
it 
is 
in 
state 
qi 
if 
the 
number 
of 
a's 
it 
has 
seen 
so 
far 
is 
i 
mod 
3. 
The 
machine 
is 
in 
state 
Po 
or 
Pl 
on 
the 
second 
pass 
over 
the 
input 
from 
right 
to 
left, 
the 
index 
indicating 
the 
parity 
of 
the 
number 
of 
b's 
it 
has 
seen 
so 
far. 
0 

_____________________________________________
122 
Lecture 
17 
Configu 
rations 
a 
nd 
Accepta 
nce 
Fix 
an 
input 
x 
E 
say 
x 
= 
ala2'" 
an. 
Let 
ao 
= 
I-
and 
an+! 
= 
-1. 
Then 
A 
conjiguration 
of 
the 
machine 
on 
input 
x 
is 
a 
pair 
(q, 
i) 
such 
that 
q 
E 
Q 
and 
0 
i 
n 
+ 
1. 
Informally, 
the 
pair 
(q, 
i) 
gives 
a 
current 
state 
and 
current 
position 
ofthe 
read 
head. 
The 
start 
conjiguration 
is 
(8,0), 
meaping 
that 
the 
machine 
is 
in 
its 
start 
state 
sand 
scanning 
the 
left 
endmarker. 
A 
binary 
relation 
2...., 
the 
next 
configuration 
relation, 
is 
defined 
on 
con-
z 
figurations 
as 
folIows: 
6(p,ai) 
= 
(q,L) 
(p,i) 
+ 
(q,i 
-1), 
6(p,ai) 
= 
(q,R) 
(p,i) 
+ 
(q,i 
+ 
1). 
The 
relation 
describes 
one 
step 
of 
the 
machine 
on 
input 
x. 
We 
define 
z 
the 
relations 
+ 
inductively, 
n 
0: 
Ł 
(p, 
i) 
+ 
(p, 
i); 
and 
Ł 
if 
(p,i) 
+ 
(q,j) 
and 
(q,j) 
+ 
(u,k), 
then 
(p,i) 
(u,k). 
The 
relation 
is 
just 
the 
n-fold 
composition 
of 
2..... 
The 
relations 
z 
z 
are 
functions; 
that 
is, 
for 
any 
configuration 
(p, 
i), 
there 
is 
exactly 
one 
z 
configuration 
(q,j) 
such 
that 
(p,i) 
+ 
(q,j). 
Now 
define 
(p,i) 
7 
(q,j) 
3n 
0 
(p,i) 
+ 
(q,j). 
Note 
that 
the 
definitions 
of 
these 
relations 
depend 
on 
the 
input 
x. 
The 
machine 
is 
said 
to 
accept 
the 
input 
x 
if 
(8,0) 
7 
(t,i) 
for 
some 
i. 
In 
other 
words, 
the 
machine 
enters 
its 
accept 
state 
at 
some 
point. 
The 
machine 
is 
said 
to 
reject 
the 
input 
x 
if 
(s,O) 
7 
(r,i) 
for 
some 
i. 
In 
other 
words, 
the 
machine 
enters 
its 
reject 
state 
at 
some 
point. 
It 
·cannot 
both 
accept 
and 
reject 
input 
x 
by 
our 
assumption 
that 
t 
=I 
rand 
by 
erties 
(17.2). 
The 
machine 
is 
said 
to 
halt 
on 
input 
x 
if 
it 
either 
accepts 
x 
or 
rejects 
x. 
Note 
that 
this 
is 
a 
purely 
mathematical 
definition-the 
machine 
doesn't 
really 
grind 
to 
abalt! 
It 
is 
possible 
that 
the 
machine 
neither 
accepts 
nor 
rejects 
x, 
in 
which case 
it 
is 
said 
to 
loop 
on 
x. 
The 
set 
L(M) 
is 
defined 
to 
be 
the 
set 
of 
strings 
accepted 
by 
M. 

_____________________________________________
Two-Way 
Finite 
Automata 
123 
Example 
17.3 
The 
2DFA 
described 
in 
Example 
17.2 
goes 
through 
the 
following 
sequence 
of 
configurations 
on 
input 
aababbb, 
leading 
to 
acceptance: 
(qO,O), 
(qO, 
1), 
(ql, 
2), 
(q2,3), 
(q2,4), 
(qo,5), 
(qo,6), 
(qo, 
7), 
(qo,8), 
(Po, 
7), 
(pI,6), 
(Po, 
5), 
(pI,4), 
(pI,3), 
(po,2), 
(Po, 
1), 
(Po,O), 
(t,l). 
It 
go 
es 
through 
the 
following 
sequence 
of 
configurations 
on 
input 
aababa, 
leading 
to 
rejection: 
(qo,O), 
(qo,l), 
(ql,2), 
(q2,3), (q2,4), 
(qo,5), (qo,6), 
(ql,7), 
(r,6). 
It 
go 
es 
through 
the 
following 
sequence 
of 
configurations 
on 
input 
aababb, 
leading 
to 
rejection: 
(qo, 
0), 
(qO, 
1), 
(ql, 
2), 
(q2, 
3), 
(q2, 
4), 
(qo, 
5), 
(qO, 
6), 
(qo, 
7), 
(Po, 
6), 
(PI, 
5), 
(po, 
4), 
(po, 
3), 
(PI, 
2), 
(PI, 
1), 
(PI, 
0), 
(r, 
1). 
0 

_____________________________________________
Lecture 
18 
2DFAs 
and 
Regular 
Sets 
In 
this 
lecture 
we 
show 
that 
2DFAs 
are 
no 
more 
powerful 
than 
ordinary 
DFAs. 
Here 
is 
the 
idea. 
Consider 
a 
long 
input 
string 
broken 
up 
in 
an 
bitrary 
place 
into 
two 
substrings 
xz. 
How 
much 
information 
about 
x 
can 
the 
machine 
carry 
ac 
ross 
the 
boundary 
from 
x 
into 
z? 
Since 
the 
machine 
is 
two-way; 
it 
can 
cross 
the 
boundary 
between 
x 
and 
z 
several 
times. 
Each 
time 
it 
crosses 
the 
boundary 
moving 
from 
right 
to 
left, 
that 
is, 
from 
z 
into 
x, 
it 
does 
so 
in 
some 
state 
q. 
When 
it 
crosses 
the 
boundary 
again 
moving 
from 
left 
to 
right 
(if 
ever), 
it 
comes 
out 
of 
x 
in 
some 
state, 
say 
p. 
Now 
if 
it 
ever 
goes 
into 
x 
in 
the 
future 
in 
state 
q 
again, 
it 
wIll 
emerge 
again 
in 
state 
p, 
because 
its 
future 
action 
is 
completely 
determined 
by 
its 
current 
configuration 
(state 
and 
head 
position). 
Moreover, 
the 
state 
p 
depends 
only 
on 
q 
and 
x. 
We 
will 
write 
Tz 
(q) 
= 
P 
to 
denote 
this 
relationship. 
We 
can 
keep 
track 
of 
all 
such 
information 
by 
means 
of 
a 
finite 
table 
Tz 
: 
(Q 
U { Ł 
}) 
-> 
(Q 
U 
{1-} 
), 
where 
Q 
is 
the 
set 
of 
states 
of 
the 
2DFA 
M, 
and 
Ł 
and 
1-
are 
two 
other 
objects 
not 
in 
Q 
whose 
purpose 
is 
described 
below. 
On 
input 
xz, 
the 
machine 
M 
starts 
in 
its 
start 
state 
scanning 
the 
left 
endmarker. 
As 
it 
computes, 
it 
moves 
its 
read 
head. 
The 
head 
may 
eventually 
cross 
the 
boundary 
moving 
left 
to 
right 
from 
x 
into 
z. 
The 
first 
time 
it 
does 
so 
(if 
ever), 
it 
is 
in 
so 
me 
state, 
which 
we 
will 
call 
Tz 
( 
.) 
(this 
is 
the 
purpose 
of 
.). 
The 
machine 
may 
never 
emerge 
from 
x; 
in 
this 
case 
we 
write 
Tz 
(.) 
= 
1. 

_____________________________________________
2DFAs 
and 
Regular 
Sets 
125 
(this 
is 
the 
purpose 
of 
.1). 
The 
state 
T., 
( 
.) 
gives 
some 
information 
abou 
t 
x, 
but 
only 
a 
finite 
amount 
of 
information, 
since 
there 
are 
only 
finitely 
many 
possibilities 
for 
T., 
(. 
). 
Note 
also 
that 
T., 
( 
.) 
depends 
only 
on 
x 
and 
not 
on 
z: 
if 
the 
input 
were 
xw 
instead 
of 
XZ, 
the 
first 
time 
the 
machine 
passed 
the 
boundary 
from 
x 
into 
w, 
it 
would 
also 
be 
in 
state 
T., 
( Ł 
), 
because 
its 
action 
up 
to 
that 
point 
is 
determined 
only 
by 
Xi 
it 
hasn't 
seen 
anything 
to 
the 
right 
of 
the 
boundary 
yet. 
If 
T., 
( 
.) 
= 
.1, 
M 
must 
be 
in 
an 
infinite 
loop 
inside 
x 
and 
will 
never 
accept 
or 
reject, 
by 
our 
assumption 
about 
moving 
all 
the 
way 
to 
the 
right 
endmarker 
whenever 
it 
accepts 
or 
rejects. 
Suppose 
that 
the 
machine 
does 
emerge 
from 
x 
into 
z. 
It 
may 
wander 
around 
in 
z 
for 
a 
while, 
then 
later 
may 
move 
back 
into 
x 
from 
right 
to 
left 
in 
state 
q. 
If 
this 
happens, 
then 
it 
will 
either 
Ł 
eventually 
emerge 
from 
x 
again 
in 
some 
state 
p, 
in 
which 
case 
we 
define 
TOl(q) 
= 
Pi 
or 
Ł 
never 
emerge, 
in 
which 
case 
we 
define 
T., 
(q) 
= 
.1. 
Again, 
note 
that 
TOl(q) 
depends 
only 
on 
x 
and 
q 
and 
not 
on 
z. 
Ifthe 
machine 
entered 
x 
from 
the 
right 
on 
input 
xw 
in 
state 
q, 
then 
it 
would 
emerge 
again 
in 
state 
T.,(q) 
(or 
never 
emerge, 
if 
TOl(q) 
= 
.1), 
because 
M 
is 
deterministic, 
and 
its 
behavior 
while 
inside 
x 
is 
completely 
determined 
by. 
x 
and 
the 
state 
it 
entered 
x 
in. 
If 
we 
write 
down 
T., 
(q) 
for 
every 
state 
q 
along 
with 
T., 
( 
.), 
this 
gives 
all 
the 
information 
about 
x 
one 
could 
ever 
hope 
to 
carry 
across 
the 
boundary 
from 
x 
to 
z. 
One 
can 
imagine 
an 
agent 
sitting 
to 
the 
right 
of 
the 
boundary 
between 
x 
and 
z, 
trying 
to 
obtain 
information 
about 
x. 
All 
it 
is 
allowed 
to 
do 
is 
observe 
the 
state 
T., 
(.) 
the 
first 
time 
the 
machine 
emerges 
from 
x 
(if 
ever) 
and 
later 
send 
prob 
es 
into 
x 
in 
various 
states 
q 
to 
see 
what 
state 
TOl(q) 
the 
machine 
comes 
out 
in 
(if 
at 
all). 
If 
y 
is 
another 
string 
such 
that 
T
y 
= 
TOl' 
then 
x 
and 
y 
will 
be 
indistinguishable 
from 
the 
agent's 
point 
of 
view. 
Now 
note 
that 
there 
are 
only 
finitely 
many 
possible 
tables 
T: 
(Q 
U 
{.}) 
-+ 
(QU 
{.1}), 
namely 
(k 
+ 
l)k+l, 
where 
k 
is 
the 
size 
of 
Q. 
Thus 
there 
is 
only 
a 
finite 
amount 
of 
information 
about 
x 
that 
can 
be 
passed 
across 
the 
boundary 
to 
the 
right 
of 
x, 
and 
it 
is 
all 
encoded 
in 
the 
table 
T.,. 
Note 
also 
that 
if 
T., 
= 
T
lI 
and 
M 
accepts 
XZ, 
then 
M 
accepts 
yz. 
This 
is 
because 
the 
sequence 
of 
states 
the 
machine 
is 
in 
as 
it 
passes 
the 
boundary 
between 
x 
and 
z 
(or 
between 
y 
and 
z) 
in 
either 
direction 
is 
com 
pletely 

_____________________________________________
126 
Lecture 
18 
determilled 
by 
the 
table 
T", 
= 
T
y 
and 
z. 
To 
accept 
xz, 
the 
machine 
must 
at 
some 
point 
be 
scanning 
the 
right 
endmarker 
in 
itS' 
accept 
state 
t. 
Since 
the 
sequence 
of 
states 
along 
the 
boundary 
is 
the 
same 
and 
the 
action 
when 
the 
machine 
is 
scanning 
z 
is 
the 
same, 
this 
also 
must 
happen 
on 
input 
yz. 
Now 
we 
can 
use 
the 
Myhill-Nerode 
theorem 
to 
show 
that 
L(M) 
is 
regular. 
We 
have 
just 
argued 
that 
T", 
= 
T
y 
'r/z 
(M 
accepts 
xz 
M 
accepts 
yz) 
'r/z 
(xz 
E 
L(M) 
yz 
E 
L(M)) 
x 
==L(M) 
y, 
where 
==L(M) 
is 
the 
relation 
first 
defined 
in 
Bq. 
(16.1) 
of 
Lecture 
16. 
Thus 
if 
two 
strings 
have 
the 
same 
table, 
then they 
are 
equivalent 
under 
==L(M)' 
Since 
there 
are 
only 
finitely 
many 
tables, 
the 
relation 
==L(M) 
has 
only 
finitely 
many 
equivalence 
classes, 
at 
most 
one 
for 
each 
table; 
therefore, 
==L(M) 
is 
of 
finite 
index. 
By 
the 
Myhill-Nerode 
theorem, 
L(M) 
is 
a 
regular 
set. 
Constructing 
a 
DFA 
The 
argument 
above 
may 
be 
a 
bit 
unsatisfying, 
since 
it 
does 
not 
explicitly 
construct 
a 
DFA 
equivalent 
to 
a 
given 
2DFA 
M. 
We 
can 
easily 
do 
so, 
however. 
Intuitively, 
we 
can 
build 
a 
DFA 
whose 
states 
correspond 
to 
the 
tables. 
Formally, 
define 
def 
T 
X==y 
'", 
=T
y
Ł 
That 
is, 
call 
two 
strings 
in 
equivalent 
if 
they 
have 
the 
same 
table. 
There 
are 
only 
finitely 
many 
equivalence 
classes, 
at 
most 
one 
for 
each 
table; 
thus 
== 
is 
of 
finite 
index. 
We 
can 
also 
show 
the 
following: 
(i) 
The 
table 
T",a 
is 
uniquely 
determined 
by 
T", 
and 
a; 
that 
is, 
if 
T", 
= 
T
y
, 
then 
T",a 
= 
T
ya
. 
This 
says 
that 
== 
is 
a 
right 
congruence. 
(ii) 
Wh 
ether 
or 
not 
x 
is 
accepted 
by 
M 
is 
completely 
determined 
by 
T",; 
that 
is, 
if 
T", 
= 
T
y
, 
then 
either 
both 
x 
and 
y 
are 
accepted 
by 
M 
or 
neither 
iso 
This 
says 
that 
== 
refines 
L(M). 
These 
observations 
together 
say 
that 
== 
is 
a 
Myhill-Nerode 
relation 
for 
L(M). 
Using 
the 
construction 
== 
1---+ 
M=. 
described 
in 
Lecture 
15, 
we 
can 
obtain 
a 
DFA 
for 
L(M) 
explicitly. 
To 
show 
(i), 
we 
show 
how 
to 
construct 
T",a 
from 
T", 
and 
a. 

_____________________________________________
2DFAs 
and 
Regular 
Sets 
127 
e 
If 
PO,Pl, 
... 
,Pk,qO,ql, 
... 
,qk 
E 
Q 
such 
that 
6(Pi.a) 
= 
(qi,L) 
and 
T;r(q;) 
= 
Pi+l, 
0 
S 
i 
S 
k-1, 
and 
6(Pk,a) 
= 
(qk, 
R), 
then 
T;z;a(Po) 
= 
qk· 
e 
IfpO,Pl, 
... 
,PbqO,ql, 
... 
,qk-l 
E 
Q 
such 
that 
6(Pi,a) 
= 
(q"L) 
and 
T.,(qi) 
= 
Pi+l, 
0 
S 
i 
S k 
-
1, 
and 
Pk 
= 
Pi, 
i 
< 
k, 
then 
T.,a(Po) 
= 
..L. 
e 
IfpO,Pl, 
... 
,pk,qo,ql, 
... 
,qk 
E 
Q 
such 
that 
6(Pi,a) 
= 
(qi,L), 
0 
S 
l 
S 
k, 
T.,(qi) 
= 
Pi+l, 
0 
S 
i 
S k 
-
1, 
and 
T",(qk) 
= 
.1., 
then 
T;ra(Po) 
= 
..L. 
e 
If 
T",(e) 
=..1, 
then 
T;ra(e) 
=..1. 
e 
If 
T.,(e) 
= 
P, 
then 
T",a(e) 
= 
T"a(P). 
For 
(ii), 
suppose 
T., 
= 
T
y 
and 
consider 
the 
sequence 
of 
states 
,\I 
is 
in 
as 
it 
crosses 
the 
boundary 
in 
either 
direction 
between 
the 
input 
string 
and 
the 
right 
endmarker 
-1. 
This 
sequence 
is 
the 
same 
on 
input 
x 
as 
it 
is 
on 
input 
y, 
since 
it 
is 
completely 
determined 
by 
the 
table. 
Both 
strings 
x 
and 
y 
are 
accepted 
iff 
this 
sequence 
contains 
the 
accept 
state 
t. 
We 
have 
shown 
that 
the 
relation 
::= 
is 
a 
Myhill-
N 
erode 
relation 
for 
L( 
M), 
where 
M 
is 
an 
arbitrary 
2DFA. 
The 
construction 
::= 
1--+ 
M=c 
of 
Lecture 
15 
gives 
a 
DFA 
equivalent 
to 
M. 
Recall 
that 
in 
that 
construrtion. 
the 
states 
of 
the 
DFA 
correspond 
in 
a 
one-to-one 
fashion 
with 
the 
::=classes; 
and 
here, 
each 
::=-class 
[xl 
corresponds 
to 
a 
table 
TL 
: 
(Q 
U 
{e} 
) 
(Q 
LJ 
{..1}). 
If 
we 
wanted 
to, 
we 
could 
build 
a 
DFA 
M' 
direetly 
from 
the 
tables: 
Q' 
{T 
: 
(Q 
U { 
e} 
) 
-> 
(Q 
U 
{..L 
} 
)}, 
, 
def 
T 
S 
== 
•, 
6'(T",a) 
Tr.a, 
F
' 
d;:} 
{T", 
I 
x 
E 
L(M)}. 
The 
transition 
funetioll 
/1' 
is 
weil 
defined 
herause 
of 
property 
(i). 
and 
T.r 
E 
F' 
iff 
x 
E 
L( 
M) 
hy 
propf'rty 
(ii 
1. 
As 
usual. 
Olle 
eall 
prove 
by 
indu('l 
i.on 
on 
lyl 
that 
thell 
b/(T", 
y) 
= 
TJ:Y: 
.r 
E 
L(M') 
b'/(s',
1'1 
E 
F' 
<===} 
6' 
(T,,J') 
E 
F' 
{:::::::} 
T" 
E 
F' 
{:::::::} 
l' 
E 
[Pf'i. 
Thus 
L(M') 
= 
LIM). 
Another 
proof. 
due 
to 
Vardi 
[122]. 
is 
givell 
in 
L:r'J'('j,o<, 
',1 

_____________________________________________
128 
Lecture 
18 
Historical 
Notes 
Two-way 
finite 
automata 
were 
first 
studied 
by 
Rabin 
and 
Scott 
[102J 
and 
Shepherdson 
[114J. 
Vardi. 
[122J 
gave 
a 
shorter 
proof 
of 
equivalence 
to 
DFAs 
(Miscellaneous 
Exercise 
61). 

_____________________________________________
Lecture 
19 
Context-Free 
Grammars 
and 
Languages 
You 
may 
have 
seen 
something 
like 
the 
following 
used 
to 
give 
a 
formal 
tion 
of 
a 
language. 
This 
notation 
is 
sometimes 
called 
BNF 
for 
Backus-Naur 
form. 
<stmt> 
::= 
<if-stmt> 
I 
<while-stmt> 
I 
<begin-stmt> 
I 
<assg-stmt> 
<if-stmt> 
::= 
if 
<bool-expr> 
then 
<stmt> 
else 
<stmt> 
<while-stmt> 
::= 
while 
<bool-expr> 
do 
<stmt> 
<begin-stmt> 
::= 
begin 
<stmt-list> 
end 
<stmt-list> 
::= 
<stmt> 
I 
<stmt> 
; 
<stmt-list> 
<assg-stmt> 
::= 
<var> 
:= 
<arith-expr> 
<bool-expr> 
::= 
<arith-expr><compare-op><arith-expr> 
<compare-op> 
::= 
< 
I 
> 
I 
I 
;::: 
I 
= 
I 
i-
<arith-expr> 
::= 
<var> 
I 
<const> 
I 
«arith-expr><arith-op><arith-expr» 
<arith-op> 
::= 
+ 
I -I * I / 
<const> 
::= 
0 
11 
I 
2 
I 
3 
I 
4 
I 
5 
I 
6 
I 
7 
I 
8 
I 
9 
<var> 
::= 
alb 
I 
cl' 
.. 
I 
x 
I 
y 
I 
z 
This 
is 
an 
example 
of 
a 
context-free 
grammar. 
It 
consists 
of 
a 
finite 
set 
of 
rules 
defining 
the 
set 
of 
well-formed 
expressions 
in 
so 
me 
l<:",/Suage; 
in 
this 
example, 
the 
syntactically 
correct 
programs 
of 
a 
simple 
PASCAL-like 
programming 
language. 

_____________________________________________
130 
Lecture 
19 
For 
example, 
the 
first 
rule 
above 
says 
that 
a 
statement 
is 
either 
an 
if 
ment, 
a 
while 
statement, 
a 
begin 
statement, 
or 
an 
assignment 
statement. 
If 
statements, 
while 
statements, 
begin 
statements, 
and 
assignment 
statements 
are 
described 
formally 
by 
other 
rules 
further 
down. 
The 
third 
rule 
says 
that 
a 
while 
statement 
consists 
of 
the 
word 
while, 
followed 
by 
a 
Boolean 
expression, 
followed 
by 
the 
word 
do, 
followed 
by 
astatement. 
The 
objects 
<xxx> 
are 
called 
nonterminal 
Each 
nonterminal 
bol 
generates 
a 
set 
of 
strings 
over 
a 
finite 
alphabet 
in 
a 
systematic 
way 
described 
formally 
below. 
For 
example, 
the 
nonterminal 
<arith-expr> 
above 
generates 
the 
set 
of 
syntactically 
correct 
arithmetic 
expressions 
in 
this 
guage. 
The 
strings 
corresponding 
to 
the 
nonterminal 
<xxx> 
are 
generated 
using 
rules 
with 
<xxx> 
on 
the 
left-hand 
side. 
The 
alternatives 
on 
the 
hand 
side, 
separated 
by 
vertical 
bars 
I, 
describe 
different 
ways 
strings 
sponding 
to 
<xxx> 
can 
be 
generated. 
These 
alternatives 
may 
involve 
other 
nonterminals 
<yyy>, 
which 
must 
be 
further 
eliminated 
by 
applying 
rules 
with 
<yyy> 
on 
the 
left-hand 
side. 
The 
rules 
can 
be 
recursivei 
for 
example, 
the 
rule 
above 
for 
<stmt-list> 
says 
that 
a 
statement 
list 
is 
either 
a 
statement 
or 
a 
statement 
followed 
by 
a 
semicolon 
(i) 
followed 
by 
a 
statement 
list. 
The 
string 
while 
x 
y 
do 
begin 
x 
:= 
(x 
+ 
1) 
i 
Y 
:= 
(y 
-
1) 
end 
(19.1) 
is 
generated 
by 
the 
nonterminal 
<stmt> 
in 
the 
grammar 
above. 
To 
show 
this, 
we 
can 
give 
a 
sequence 
of 
expressions 
called 
sentential 
forms 
starting 
from <stmt> 
and 
ending 
with 
the 
string 
(19.1) 
such 
that 
each 
sentential 
form 
is 
derived 
from 
the 
previous 
by 
an 
application 
of 
one 
of 
the 
rules. 
Each 
application 
consists 
of 
replacing 
some 
nonterminal 
symbol 
<xxx> 
in 
the 
sentential 
form 
with 
one 
of 
the 
alternatives 
on 
the 
right-hand 
side 
of 
a 
rule 
for 
<xxx>. 
Here 
are 
the 
first 
few 
sentential 
forms 
in 
a 
derivation 
of 
(19.1): 
<stmt> 
<while-stmt> 
while 
<bool-expr> 
do 
<stmt> 
while 
<arith-expr><compare-op><arith-expr> 
do 
<stmt> 
while 
<var><compare-op><arith-expr> 
do 
<stmt> 
while 
<var> 
<arith-expr> 
do 
<stmt> 
while 
<var> 
<var> 
do 
<stmt> 
while 
x 
<var> 
do 
<stmt> 
while 
x 
y 
do 
<stmt> 
while 
x 
y 
do 
<begin-stmt> 

_____________________________________________
Context-Free 
Grammars 
and 
Languages 
131 
Applying 
different 
rules 
will 
yield 
different 
results. 
Für 
example, 
the 
string 
begin 
if 
z 
= 
(x 
+ 
3) 
then 
y 
:= 
z 
else 
y 
:= 
x 
end 
can 
also 
be 
generated. 
The 
set 
of 
all 
strings 
not 
containing 
any 
nonterminals 
gent'rated 
by 
the 
grammar 
is 
called 
thc 
languagt 
generated 
by 
the 
grammat. 
In 
general. 
this 
set 
of 
strings 
may 
be 
infinite. 
eVf'n 
if 
the 
set 
of 
rules 
is 
finite. 
There 
mayaiso 
br 
several 
different 
derivations 
of 
the 
same 
string. 
A 
mar 
is 
said 
to 
he 
unambiguous 
if 
this 
cannot 
happen. 
The 
grammar 
given 
above 
is 
unambiguous. 
We 
will 
gin' 
a 
gfllfCral 
definition 
of 
context-free 
gramm 
ars 
(CFGs) 
and 
the 
languages 
t 
hey 
generate. 
The 
language 
(su 
bset 
of 
I;*) 
generated 
by 
the 
contextfree 
grilmmar 
G 
is 
denoted 
L(G). 
A 
subset 
of 
is 
called 
a 
conte.ct-frce 
language 
(CFL) 
if 
it 
is 
L(G) 
for 
some 
CFG 
G. 
CFLs 
are 
good 
for 
describing 
infinite 
sets 
of 
strings 
in 
a 
finite 
way. 
They 
are 
part 
icularly 
useful 
in 
computer 
science 
for 
describing 
the 
syntax 
of 
ming 
languages. 
well-formed 
arithmrtic 
exprfssions, 
well-nested 
begin-end 
blocks, 
strings 
of 
balanced 
parentheses, 
and 
so 
on. 
All 
regular 
sets 
are 
CFLs 
(Homework 
5. 
Exercise 
1), 
but 
not 
necessarily 
vice 
versa. 
The 
following 
are 
examples 
of 
CFLö 
t.hat 
are 
not 
regular: 
Ł 
{anb
n 
In 
2 
O}; 
Ł 
{palindromes 
over 
{a,b}} 
= 
{x 
E 
{a.b}* 
I 
I 
= 
rev 
x}; 
and 
Ł 
{balanced 
strings 
of 
parenthrses}. 
Not 
all 
sets 
are 
CFLs; 
for 
example, 
the 
S(,t 
{a"b"a
n 
I 
n 
2 
o} 
is 
not. 
We 
can 
prove 
this 
formally 
llsing 
a 
pumping 
lemma 
for 
CFLs 
analogous 
to 
the 
pumping 
lemma 
for 
rf'gular 
sets. 
We 
will 
disCllSS 
tllt' 
pumping 
lemma 
for 
CFLs 
in 
Lecture 
n. 
Pushdown 
Automata 
(PDAs): 
A 
Preview 
A 
pushdou:n 
auiomaiun 
(PDAj 
is 
like 
a 
fillite 
automaton, 
except 
that 
in 
addition 
10 
its 
finite 
contro!. 
it 
lias 
a 
stack 
or 
pushdown 
store. 
which 
it 
can 
use 
to 
record 
a 
potentially 
nnbounclrd 
amount 
of 
information. 

_____________________________________________
132 
Lecture 
19 
Q 
A 
push/pop 
finite 
B 
stack 
control 
C 
B 
.1 
The 
input 
head 
is 
read-only 
and 
may 
only 
move 
right. 
The 
machine 
can 
store 
information 
on 
the 
stack 
in 
a 
last-in-first-out 
(LIFO) 
fashion. 
It 
can 
push 
symbols 
onto 
the 
top 
of 
the 
stack 
or 
pop 
them 
off 
the 
top 
of 
the 
stack. 
It 
may 
not 
read 
down 
into 
the 
stack 
without 
popping 
the 
top 
symbols 
off, 
in 
which 
case 
they 
are 
lost. 
We 
will 
define 
these 
machines 
formally 
in 
Lecture 
23 
and 
prove 
that 
the 
dass 
of 
languages 
accepted 
by 
nondetermlnistic 
PDAs 
is 
exactly 
the 
dass 
of 
CFLs. 
Formal 
Definition 
of 
CFGs 
and 
CFLs 
Formally 
a 
context-free 
grammar 
(CFG) 
is 
a 
quadruple 
G 
= 
(N, 
E, 
P, 
S), 
where 
Ł N 
is 
a 
finite 
set 
(the 
nonterminal 
symbols), 
Ł E 
is 
a 
finite 
set 
(the 
terminal 
symbols) 
disjoint 
from 
N, 
Ł P 
is 
a 
finite 
subset 
of 
N 
x 
(N 
U 
E)* 
(the 
productions), 
and 
Ł 
SEN 
(the 
start 
symbol). 
We 
use 
capitalletters 
A, 
B, 
C, 
... 
for 
Bonterminals 
and 
a, 
b, 
c, 
... 
for. 
nal 
symbols. 
Strings 
in 
(N 
U 
E)* 
are 
denoted 
0, 
ß, 
'Y, 
.... 
Instead 
of 
writing 
productions.as 
(A,o), 
we 
write 
A 
-+ 
o. 
We 
often 
use 
the 
vertical 
bar 
I 
as 
in 
the 
example 
above 
to 
abbreviate 
a 
set 
of 
productions 
with 
the 
same 
left-hand 
side. 
For 
example, 
instead 
of 
writing 

_____________________________________________
Context-Free 
Grammars 
and 
Languages 
133 
we 
might 
write 
A 
-t 
0:1 
10:2 
10:3. 
If 
a, 
ß 
E 
(N 
U 
:E)*, 
we 
say 
that 
ß 
is 
derivable 
from 
0: 
in 
one 
step 
and 
write 
1 
0: 
._-> 
ß 
G 
if 
ß 
can 
be 
obtained 
from 
0: 
by 
replacing 
some 
occurrence 
of 
a 
nonterminal 
A 
in 
0: 
with 
" 
where 
A 
-t 
, 
is 
in 
P; 
that 
is, 
if 
there 
exist 
0:1, 
0:2 
E 
(NU 
:E)* 
and 
production 
A 
-t 
, 
such 
that 
0: 
= 
0:1 
A 
0:2 
and 
ß 
= 
0:1,0:2. 
Let 
be 
the 
reflexive 
transitive 
closure 
of 
the 
relation 
that 
is, 
G 
G 
define 
o 
Ł 
0: 
--+ 
0: 
for 
any 
0:, 
G 
Ł 
0: 
ß 
if 
there 
exists 
, 
such 
that 
0: 
-!.:..., 
and 
, 
ß, 
and 
G 
G 
G 
Ł 
0: 
ß 
if 
0: 
-!.:... 
ß 
for 
some 
n 
> 
O. 
G G 
-
Astring 
in 
(NU:E)* 
derivable 
from 
the 
start 
symbol 
S 
is 
called 
a 
sentential 
form. 
A 
sentential 
form 
is 
called 
a 
sentence 
if 
it 
consists 
only 
of 
terminal 
symbols; 
that 
is, 
if 
it 
is 
in 
:E*. 
The 
language 
generated 
by 
G
t 
denoted 
L( 
G), 
is 
the 
set 
of 
all 
sentences: 
L(G) 
= 
{x 
E 
:E* 
I 
S 
x}. 
G 
A 
subset 
B 
:E* 
is 
a 
context-free 
language 
(CFL) 
if 
B 
= 
L( 
G) 
for 
some 
context-free 
grammar 
G. 
Example 
19.1 
The 
nonregular 
set 
{a"b" 
I 
n 
O} 
is 
a 
CFL. 
It 
is 
generated 
by 
the 
grammar 
S 
-t 
aSb 
I 
•, 
where 
• 
is 
the 
null 
string. 
More 
precisely, 
G 
= 
(N, 
:E, 
P, 
S), 
where 
N 
= 
{S}, 
:E 
= 
{a,b}, 
P 
= 
{S 
-t 
aSb, 
S 
-t 
•}. 
Here 
is 
a 
derivation 
of 
a
3
b
3 
in 
G: 
S 
aSb 
aaSbb 
aaaSbbb 
aaabbb. 
G G 
G 
G 
The 
first 
three 
steps 
apply 
the 
production 
S 
-t 
aSb 
and 
the 
last 
applies 
the 
product-ion 
S 
-t 
•. 
Thus 
S 
aaabbb. 
G 

_____________________________________________
134 
Lecture 
19 
One 
can 
show 
by 
induction 
on 
n 
that 
S 
n+l 
nb
n 
Ga 
, 
so 
all 
strings 
of 
the 
form 
an 
b
n 
are 
in 
L 
( 
G); 
conversely, 
the 
only 
strings 
in 
L( 
G) 
are 
of 
the 
form 
anb
n
, 
as 
can 
be 
shown 
by 
induction 
on 
the 
length 
of 
the 
derivation. 
0 
Example 
19.2 
The 
nonregular 
set 
{palindromes 
over 
{a, 
b} 
*} 
= 
{x 
E 
{a, 
b} 
* 
I 
x 
= 
rev 
x} 
is 
a 
CFL 
generated 
by 
the 
grammar 
S 
-+ 
aSa 
I 
bSb 
I 
alb 
I 
•. 
The 
first 
two 
productions 
generate 
any 
number 
of 
balanced 
a's 
or 
b's 
at 
the 
outer 
ends 
of 
the 
string, 
working 
from 
the 
outside 
in. 
The 
last 
three 
productions 
are 
used 
to 
finish 
the 
derivation. 
The 
productions 
S 
-+ 
a 
and 
S 
-+ 
bare 
used 
to 
generate 
an 
odd-length 
palindrome 
with 
an 
a 
or 
b, 
respectively, 
as 
the 
middle 
symbol; 
and 
the 
production 
8 
-+ 
E 
is 
used 
to 
generate 
an 
even-length 
palindrome. 
0 
Historical 
Notes 
Context-free 
grammars 
and 
languages 
were 
introduced 
by 
Chomsky 
[17, 
18, 
20], 
although 
they 
were 
foreshadowed 
by 
the 
systems 
of 
Post 
[100] 
and 
Markov 
[83]. 
Backus-Naur 
form 
was 
used' 
to 
specify 
the 
syntax 
of 
programming 
langu'ages 
h:v 
Backus 
[7] 
and 
Naur 
[93]. 

_____________________________________________
Lecture 
20 
Balanced 
Parentheses 
Intuitively, 
astring 
of 
parentheses 
is 
balanced 
if 
each 
left 
p'arenthesis 
has 
a 
matching 
right 
parenthesis 
and 
the 
matched 
pairs 
are 
weIl 
nested. 
The 
set 
PAREN 
of 
balanced 
strings 
of 
parentheses 
[ ] 
is 
the 
prototypical 
free 
language 
and 
plays 
a 
pivot 
al 
role 
in 
the 
theory 
of 
CFLs. 
The 
set 
PAREN 
is 
generated 
by 
the 
grammar 
S 
--+ 
ES] 
I 
SS 
jE. 
This 
is 
not 
obvious, 
so 
let's 
give 
a 
proof. 
First 
we 
need 
a 
formal 
ization 
of 
balanced. 
To 
avoid 
confusing 
notation, 
we'Il 
use 
L(x) 
#[(x) 
= 
the 
number 
ofleft 
parentheses 
in 
x, 
R(x) 
#] 
(x) 
= 
the 
number 
of 
right 
parentheses 
in 
x. 
We 
will 
define 
astring 
x 
of 
parentheses 
to 
be 
balanced 
if 
and 
only 
if 
(i) 
L(x) 
= 
R(x), 
and 
(ii) 
for 
all 
prefixes 
y 
of 
x, 
L(y) 
R(y). 
(Recall 
that 
aprefix 
of 
x 
is 
astring 
y 
such 
thatx 
= 
yz 
for 
some 
z.) 
To 
see 
that 
this 
definition 
correctly 
captures 
the 
intuitive 
notion 
of 
balanced, 
note 
that 
property 
(i) 
says 
that 
there 
must 
be 
the 
same 
number 
of 
left 

_____________________________________________
136 
Lecture 
20 
parentheses 
as 
right 
parentheses, 
which 
must 
certainly 
be 
true 
if 
x 
is 
anced; 
and 
property 
(ii) 
says 
that 
for 
any 
way 
of 
partitioning 
x 
into 
yz, 
there 
cannot 
be 
more 
parentheses 
in 
y 
than 
left 
parentheses, 
because 
right 
parentheses 
can 
only 
match 
left 
parentheses 
to 
the 
left 
of 
them. 
Thus 
(i) 
and 
(ii) 
are 
certainly 
necessary 
conditions 
for 
astring 
to 
be 
balanced: 
To 
see 
that 
they 
are 
sufficient, 
draw 
the 
graph 
of 
the 
function 
L(y) 
-
R(y) 
as 
y 
ranges 
over 
prefixes 
of 
x: 
4 
3 
2 
1 
. 
__ 
[ [ [ 
[ ] [ 
] 
] 
[ 
] 
] 
[ [ ] 
] 
[ ] ] 
Property 
(i) 
says 
that 
the 
graph 
ends 
with 
value 
0 
(Le., 
L(x) 
-
R(x) 
= 
0), 
and 
(ii) 
says 
that 
it 
never 
dips 
below 
0 
on 
the 
way 
across. 
If 
the 
graph 
satisfies 
these 
two 
properties, 
then 
given 
any 
parenthesis 
in 
the 
string, 
one 
can 
find 
the 
matching 
parenthesis 
by 
shooting 
upward 
and 
ricocheting 
off 
the 
graph 
twice. 
[ [ [ 
[ ] 
[ ] ] [ 
] ] 
[ [ J ] 
[ ] ] 
Thus 
we 
will 
take 
(i) 
and 
(ii) 
as 
our 
formal 
definition 
of 
balanced 
strings 
of 
parentheses 
and 
prove 
that 
the 
given 
grammar 
generates 
exactly 
the 
set 
of 
such 
strings, 
no 
more 
and 
no 
less. 
Theorem 
20.1 
Let 
G 
be 
the 
CPG 
s 
-+ 
ES] 
I 
SS 
I 
•. 
Then 
L(G) 
= 
{x 
E 
{[,J}* 
I 
x 
satisfies 
(i) 
and 
(ii)}. 
Proof. 
We 
show 
the 
inclusion 
in 
both 
directions. 
Both 
arguments 
will 
be 
by 
induction, 
but 
induction 
on 
different 
things. 

_____________________________________________
Balanced 
Parentheses 
137 
first 
we 
show 
the 
forward 
inclusion: 
if 
S 
x, 
then 
x 
satisfies 
(i) 
and 
(ii). 
. 
G 
Thus 
any 
string 
generated 
by 
G 
is 
balanced. 
We 
would 
like 
to 
use 
induction 
on 
the 
length 
of 
the 
derivation 
of 
x 
from 
S, 
but 
since 
the 
intermediate 
sentential 
forms 
in 
this 
derivation 
will 
tain 
nonterminals, 
we 
need 
to 
strengthen 
our 
induction 
hypothesis 
to 
allow 
nonterminals. 
Thus 
we 
will 
actually 
show 
that 
for 
any 
0 
E 
(N 
U 
E)*, 
if 
S 
0, 
then 
o.satisfies 
(i) 
and 
(ii). 
This 
will 
be 
.proved 
by 
induction 
on 
G 
the 
length 
of 
the 
derivation 
S 
o. 
G 
Basis 
If 
S 
0, 
then 
0 
= 
S 
by 
definition 
of 
the 
relation 
But 
the 
sentential 
G 
G 
form 
S 
satisfies 
(i) 
and 
(ii) 
trivially. 
Induction 
step 
Suppose 
S 
o. 
Let 
ß 
be 
the 
sentential 
form 
immediately 
preceding 
0 
in 
G 
the 
derivation. 
Then 
S
n 
1 
--+ 
ß 
--+ 
0:. 
TI 
G 
By 
the 
induction 
hypothesis, 
ß 
satisfies 
(i) 
and 
(ii). 
There 
are 
now 
three 
cases, 
corresponding 
to 
the 
three 
productions 
in 
the 
grammar 
that 
could 
have 
been 
applied 
in 
the 
last 
step 
to 
derive 
0 
from 
ß. 
We 
will 
show 
in 
each 
case 
that 
properties 
(i) 
and 
(ii) 
are 
preserved. 
The 
first 
two 
cases, 
corresponding 
to 
productions 
S 
-+ 
fand 
S 
-+ 
SS, 
are 
easy 
because 
neither 
production 
changes 
the 
number 
or 
order 
of 
ses. 
In 
either'case 
there 
exist 
ßl, 
ß2 
E 
(N 
U 
E)* 
such 
that 
{ 
ßIß2 
if 
S 
-+ 
f 
was 
applied, 
ß 
= 
ß
1
Sß2 
and 
0 
= 
ß
1
SSß2 
if 
S 
-+ 
SS 
was 
applied; 
and 
in 
either 
case 
0 
satisfies 
(i) 
and 
(ii) 
iff 
ß 
does. 
If 
the 
last 
production 
applied 
was 
S 
-+ 
[S], 
then 
there 
exist 
ßl, 
ß2 
E 
(N 
U 
E)* 
such 
that 
ß 
= 
ß
1
Sß2 
and 
0 
= 
ßl 
[S]ß2, 
and 
by 
the 
induction 
hypothesis 
(i) 
and 
(ii) 
hold 
of 
ß. 
Then 
L(o) 
= 
L(ß) 
+ 
1 
= 
R(ß) 
+ 
1 
since 
ß 
satisfies 
(i) 
= 
R(o), 
thus 
(i) 
holds 
of 
0. 
To 
show 
that 
(ii) 
holds 
of 
0, 
let 
'Y 
be 
an 
arbitrary 
prefix 
of 
o. 
We 
want 
to 
show 
that 
L('Y) 
R('Y). 
Either 

_____________________________________________
138 
Lecture 
20 
Ł 
'Y 
is 
aprefix 
of 
ßl, 
in 
which 
case 
'Y 
is 
aprefix 
of 
ß, 
so 
(ii) 
holds 
for 
the 
prefix 
'Y 
by 
the 
induction 
hypothesis; 
or 
Ł 
'Y 
is 
aprefix 
of 
ßl 
[5 
but 
not 
of 
ßl, 
in 
which 
case 
L('Y) 
= 
L(ßd 
+ 
1 
R(ßl) 
+ 
1 
induction 
hypothesis, 
since 
ßl 
is 
aprefix 
of 
ß 
> 
R(ßl) 
= 
R(-y); 
or 
Ł 
'Y 
= 
ßl 
[5]0, 
where 
{) 
is 
aprefix 
of 
ß2, 
in 
which 
case 
L(-y) 
= 
L(ß
1
5{)) 
+ 
1 
R(ß
1
5{)) 
+ 
1 
induction 
hypothesis 
= 
R(-y). 
Thus 
in 
all 
cases 
L(-y) 
R(-y). 
Since 
'Y 
was 
arbitrary, 
(ii) 
holds 
of 
Q. 
This 
concludes 
the 
inductive 
proof 
that 
if 
5 
x, 
then 
x 
is 
balanced. 
G 
Now 
we 
wish 
to 
show 
the 
other 
direction: 
if 
x 
is 
balanced, 
then 
5 
x. 
G 
This 
is 
done 
by 
induction 
on 
lxi. 
Assurne 
that 
x 
satisfies 
(i) 
and 
(ii). 
Basis 
If 
lxi 
= 
0, 
we 
have 
x 
= 
fand 
S 
x 
in 
one 
step 
using 
the 
production 
G 
S-->E. 
Induction 
step 
If 
lxi> 
0, 
we 
break 
the 
argument 
into 
two 
cases: 
either 
(a) 
there 
exists 
a 
proper 
prefix 
y 
of 
x 
(one 
such 
that 
0 
< 
lyl 
< 
lxI) 
satisfying 
(i) 
and 
(ii), 
or 
(b) 
no 
such 
prefix 
exists. 
In 
case 
(a), 
we 
have 
x 
= 
yz 
for 
some 
z, 
° 
< 
Izl 
< 
lxI, 
and 
z 
satisfies 
(i) 
and 
(ii) 
as 
weIl: 
L(z) 
= 
L(x) 
-
L(y) 
= 
R(x) 
-
R(y) 
= 
R(z), 
and 
for 
any 
prefix 
w 
of 
z, 
L(w) 
= 
L(yw) 
-
L(y) 
R(yw) 
-
R(y) 
since 
yw 
is 
aprefix 
of 
x 
and 
L(y) 
= 
R(y) 
= 
R(w). 

_____________________________________________
Balanced 
Parentheses 
139 
By 
the 
induction 
hypothesis, 
S 
y 
and 
S 
z. 
Then 
we 
can 
derive 
x 
G 
G 
by 
starting 
with 
the 
production 
S 
-+ 
SS, 
then 
deriving 
y 
from 
the 
first 
S, 
then 
deriving 
z 
from 
the 
second 
S: 
S 
2.... 
SS 
yS 
yz 
= 
x. 
G G G 
In 
case 
(b), 
no 
such 
y 
exists. 
Then 
x 
= 
[z] 
for· 
some 
z, 
and 
z 
satisfies 
(i) 
and 
(ii). 
It 
satisfies 
(i) 
since 
L(z) 
= 
L(x) 
-
1 
= 
R(x) 
-
1 
= 
R(z), 
and 
it 
satisfies 
(ii) 
since 
for 
al1 
nonnull 
prefixes 
u 
of 
z, 
L(u) 
-
R(u) 
= 
L([u) 
-1-
R([u) 
0 
since 
L( 
[u) 
-
R( 
[u) 
1 
because 
we 
are 
in 
case 
(b). 
By 
the 
induction 
hypothesis, 
S 
+ 
z. 
Com})ining 
this 
derivation 
with 
the 
production 
S 
-+ 
[S], 
we 
get 
a 
derivation 
of 
x: 
1 
Ł 
S 
--+ 
[S] 
--+ 
[z] 
= 
x. 
G G 
Thus 
every 
string 
satisfying 
(i) 
and 
(ii) 
can 
be 
derived. 
o 

_____________________________________________
Lecture 
21 
Normal 
Forms 
For 
many 
applications, 
it 
is 
often 
helpful 
to 
assurne 
that 
CFGs 
are 
in 
one 
or 
another 
special 
restricted 
form. 
Two 
of 
the 
most 
useful 
such 
forms 
are 
Chomsky 
normal 
form 
(CNF) 
and 
Greibach 
normal 
form 
(GNF). 
Definition 
21.1 
A 
CFG 
is 
in 
Chomsky 
normal 
form 
(CNF) 
if 
all 
productions 
are 
of 
the 
form 
A 
-> 
BC 
or 
A 
-> 
a, 
where 
A, 
B, 
CE 
N 
and 
a 
EI:. 
A 
CFG 
is 
in 
Greibach 
normal 
form 
(GNF) 
if 
all 
productions 
are 
of 
the 
form 
A 
-> 
aB
1
B
2 
ŁŁŁ 
Bk 
for 
some 
k 
0, 
where 
A, 
B
1
, 
... 
, 
Bk 
E 
N 
and 
a 
E 
I:. 
Note 
that 
k 
= 
0 
is 
allowed, 
giving 
productions 
of 
the 
form 
A 
-> 
a. 
0 
For 
example, 
the 
two 
grammars 
S 
-> 
AB 
I 
AC 
ISS, 
C 
-> 
SB, 
S 
-> 
[B 
I 
[SB 
I 
[BS 
I 
[SBS, 
A->[, 
B 
->] 
(21.1) 
(21.2) 
are 
grammars 
in 
Chomsky 
and 
Greibach 
normal 
form, 
respectively, 
for 
the 
set 
of 
all 
nonnull 
strings 
of 
balanced 
parentheses 
[ 
]. 
No 
gramm 
ar 
in 
Chomsky 
or 
Greibach 
form 
can 
generate 
the 
null 
string 
E 
(Why 
not?). 
Apart 
from 
this 
one 
exception, 
they 
are 
completely 
general: 

_____________________________________________
Normal 
Forms 
141 
Theorem 
21.2 
For 
any 
CFG 
G, 
there 
is 
a 
CFG 
GI 
in 
Chomsky 
normal 
form 
and 
a 
CFG 
G" 
in 
Greibach 
normal 
form 
such 
that 
L(G") 
= 
L(G
/
) 
= 
L(G) 
-
{E}. 
Getti 
ng 
Rid 
of 
E-
a 
nd 
U n 
it 
P 
rod 
uctions 
To 
prove 
Theorem 
21.2, 
we 
must 
first 
show 
how 
to 
get 
rid 
of 
all 
f-productions 
A 
....... 
fand 
unit 
productions 
A 
....... 
B. 
These 
productions 
are 
bothersome 
because 
they 
make 
it 
hard 
to 
determine 
whether 
applying 
a 
production 
makes 
any 
progress 
toward 
deriving 
astring 
of 
terminals. 
For 
instance, 
with 
unit 
productions, 
there 
can 
be 
loops 
in 
the 
derivation, 
and 
with 
productions, 
one 
can 
generate 
very 
long 
strings 
of 
nonterminals 
and 
then 
erase 
them 
all. 
Without 
E-
or 
unit 
productions, 
every 
step 
in 
the 
derivation 
makes 
demonstrable 
progress 
toward 
the 
terminal 
string 
in 
the 
sense 
that 
either 
the 
sentential 
form 
gets 
strictly 
Ion 
ger 
or 
a 
new 
terminal 
symbol 
appears. 
We 
cannot 
simply 
throw 
out 
the 
t-
and 
unit 
productions, 
because 
they 
may 
be 
needed 
to 
generate 
some 
strings 
in 
L(G); 
so 
before 
we 
throw 
them 
out, 
we 
had 
better 
throw 
in 
some 
other 
productions 
we 
can 
use 
instead. 
Lemma 
21.3 
For 
any 
CFG 
G 
= 
(N, 
P, 
5), 
there 
is 
a 
CFG 
GI 
with no 
f-
or 
unit 
productions 
such 
that 
L( 
GI) 
= 
L( 
G) 
-
{f} 
Proof. 
Let 
P 
be 
the 
smallest 
set 
of 
productions 
containing 
P 
and 
closed 
under 
the 
two 
rules 
(a) 
if 
A 
-+ 
aBß 
and 
B 
-+ 
E 
are 
in 
P, 
then 
A 
....... 
aß 
is 
in 
P; 
and 
(b) 
if 
A 
....... 
Band 
B 
....... 
'Y 
are 
in 
P, 
then 
A 
....... 
'Y 
is 
in 
P. 
We 
can 
construct 
P 
inductively 
from 
P 
by 
adding 
productions 
as 
required 
to 
satisfy 
(a) 
and 
(b). 
Note 
that 
only 
finitely 
many 
proäuctions 
ever 
get 
added, 
each 
new 
right-hand 
side 
is 
a 
substring 
of 
an 
old 
right-hand 
side. 
Thus 
P 
is 
still 
finite. 
N 
ow 
let 
G 
be 
the 
grammar 
G 
= 
(N, 
P, 
S). 
Since 
P 
P, 
every 
derivation 
of 
Gis 
a 
derivation 
of 
Gj 
thus 
L( 
G) 
L( 
0). 
But 
L(G) 
= 
L(O), 
since 
each 
new 
production 
that 
was 
thrown 
in 
because 
of 
rule 
(a) 
or 
(b) 
can 
be 
simulated 
in 
two 
steps 
by 
the 
two 
productions 
that 
caused 
it 
to 
be 
thrown 
in. 

_____________________________________________
142 
Lecture 
21 
Now 
we 
show 
that 
for 
nonnull 
x 
E 
E*, 
any 
derivation 
S 
x 
of 
mini-
G 
mum 
length 
does 
not 
use 
any 
f-
or 
unit 
productions. 
Thus 
the 
f-
and 
unh 
productions 
are 
superfluous 
and 
can 
be 
deleted 
from 
C 
with 
impunity. 
Let 
x 
=F 
fand 
conslder 
a 
minimum-length 
derivation 
S 
...;.. 
x. 
Suppose 
for 
G 
a 
contradiction 
that 
an 
f-prodllction 
B 
-
f 
is 
used 
at 
some 
point 
in 
the 
derivation, 
say 
Ł 
1 Ł 
S 
-,::-+ 
"( 
B 
0 
-,::-+ 
"(0 
-,::-+ 
x. 
G G G 
One 
of 
,,(,0 
is 
nonnull, 
otherwise 
x 
would 
be 
null, 
contradicting 
the 
assum 
tion 
that 
it 
isn't. 
Thus 
that 
occurrence 
of 
B 
must 
first 
have 
appeared 
earlier 
in 
the 
derivation 
when 
a 
production 
of 
the 
form 
A 
-
aBß 
was 
applied: 
S 
1JAO 
1JaBßO 
"(Bo 
"(0 
X 
G G 
G G G 
for 
some 
m,n,k 
2: 
O. 
But 
by 
rule 
(a), 
A 
-
aß 
is 
also 
in 
P, 
and 
this 
production 
could 
have 
been 
applied 
at 
that 
point 
instead, 
giving 
a 
strictly 
shorter 
derivation 
of 
x: 
S 
1JAO 
1JaßO 
"(0 
x. 
G G G G 
This 
contradicts 
our 
assumption 
that 
the 
derivation 
was 
of 
minimum 
length. 
A 
similar 
argument 
shows 
that 
unit 
productions 
do 
not 
appear 
in 
length 
derivations 
in 
C. 
Let 
x 
t 
fand 
consider 
a 
derivation 
S"';" 
x 
of 
G 
minimum 
length. 
Suppose 
a 
unit 
production 
A 
-+ 
B 
ia 
used 
at 
some 
point, 
say 
Ł 
1 
Ł 
S 
-,::-+ 
aAß 
-,::-+ 
aBß 
-,::-+ 
x. 
G G G 
We 
must 
eventually 
dispose 
of 
that 
occurrence 
of 
B, 
say 
by 
applying 
a 
production 
B 
-
"( 
later 
on. 
m 
1 
ß 
n 
0 
lOk 
S 
-,::-+ 
aAß 
-,::-+ 
aB 
-,::-+ 
T} 
B 
-,::-+ 
T}"( 
-;;;:7 
x. 
G G G G G 
But 
by 
rule 
(b), 
A 
-+ 
"( 
is 
also 
in 
P, 
and 
this 
could 
have 
been" 
applied 
instead, 
giving 
a 
strictly 
shorter 
derivation 
of 
x: 
m 
1 
n 
0 
k 
S 
-,::-+ 
aAß 
-,::-+ 
a"( 
ß 
-,::-+ 
T}"( 
-,::-+ 
X. 
G 
0 
G G 
Again, 
this 
contradicts 
the 
minimality 
of 
the 
length 
of 
the 
derivation. 
Thus 
we 
do 
not 
need 
f-productions 
or 
unit 
productions 
to 
generate 
non 
null 
strings. 
If 
we 
discard 
them 
from 
C, 
we 
obtain 
a 
grammar 
G' 
generating 
L(G) 
-
{f}. 
0 

_____________________________________________
Normal 
Forms 
143 
Chomsky 
Normal 
Form 
Once 
we 
are.rid 
of 
and 
unit 
produr.tions, 
it 
is 
a 
simple 
matter 
to 
put 
the 
resulting 
grammar 
into 
Chomsky 
normal 
form. 
For 
each 
terminal 
a 
E 
E, 
introduce 
a 
new 
nonterminal 
A
a 
and 
production 
A
a 
-+ 
a, 
and 
replace 
all 
occurrences 
of 
a 
on 
the 
right-hand 
sides 
of 
old 
productions 
(except 
ductions 
of 
the 
form 
B 
-+ 
a) 
with 
A
a
Ł 
Then 
all 
productions 
are 
of 
the 
form 
A 
-+ 
a 
or 
A 
-+ 
B
l
B
2 
ŁŁŁ 
BIe, 
k 
2, 
where 
the 
Bi 
are 
nonterminals. 
The 
set 
of 
terminal 
strings 
generated 
is 
not 
changedj 
it 
just 
takes 
one 
more 
step 
than 
before 
to 
generate 
a 
terminal 
symbol. 
For 
any 
production 
A 
-+ 
B
l
B
2
ŁŁ· 
Ble 
with 
k 
3, 
introduce 
a 
new 
nonterminal 
C 
and 
replace 
this 
production 
with 
the 
two 
productions 
A 
-+ 
BlC 
and 
C 
-+ 
B
2
B3 
... 
Ble. 
Keep 
doing 
this 
until 
all 
right-hand 
sides 
are 
of 
length 
at 
most 
2. 
Example 
21.4 
Let's 
derive 
a 
CNF 
grammar 
for 
the 
set 
{an 
b
n 
I 
n 
O} 
-
= { 
an 
b
n 
I 
n 
I}. 
Starting 
with 
the 
grammar 
S 
-+ 
aSb 
I 
for 
{anb
n 
I 
n 
O}, 
we 
remove 
the 
as 
described 
in 
Lemma 
21.3 
to 
get 
8 
-+ 
aSb 
I 
ab, 
which 
generates 
{anb
n 
I 
n 
1}. 
Then 
we 
add 
nonterminals 
A,B 
and 
replace 
these 
productions 
with 
S 
-+ 
A8B 
I 
AB, 
A-+a, 
B 
-+ 
b. 
Finally, 
we 
add 
a 
nonterminal 
C 
and 
replace 
S 
-+ 
AS 
B 
with 
S 
-+ 
AC 
and 
C 
-+ 
SB. 
The 
final 
grammar 
in 
Chomsky 
form 
is 
S 
-+ 
AB 
I 
AC, 
C 
-+ 
SB, 
A-+a, 
B 
-+ 
b. 
o 
Example 
21.5 
We 
derive 
a 
CNF 
grammar 
for 
the 
set 
of 
nonnull 
strings 
of 
balanced 
theses 
[ 
]. 
Start 
with 
the 
grammar 
8 
-+ 
[8] 
I 
SS 
I 

_____________________________________________
144 
Lecture 
21 
for 
all 
balanced 
strings 
of 
parentheses. 
Applying 
the 
construction 
of 
Lemma 
21.3 
to 
get 
rid 
of 
the 
f-
and 
unit 
productions, 
we 
get 
S 
[S] 
ISS 
I 
[]. 
Next 
we 
add 
new 
nonterminals 
A, 
Band 
replace 
these 
productions 
with 
S 
ASB 
I 
SS 
I 
AB, 
B 
Finally, 
we 
add 
a 
new 
nonterminal 
C 
and 
replace 
S 
ASB 
with 
S 
AC 
and 
C 
SB. 
The 
resulting 
grammar 
in 
Chomsky 
form 
is 
exactly 
(21.1). 
o 
Greibach 
Normal 
Form 
N 
ow 
we 
show 
how 
to 
convert 
an 
arbitrary 
grammar 
to 
an 
equivalent 
one 
(except 
possibly 
for 
f) 
in 
Greibach 
normal 
form. 
We 
start 
with 
a 
grammar 
G 
= 
(N, 
P, 
S) 
in 
Chomsky 
normal 
form. 
This 
assumption 
is 
mainly 
for 
ease 
of 
presentation; 
we 
could 
easily 
modify 
the 
construction 
to 
apply 
more 
generally. 
The 
construction 
as 
given 
here 
produces 
a 
Greibach 
grammar 
with 
at 
most 
two 
nonterminals 
on 
the 
hand 
side 
(cf. 
[60, 
Exercise 
4.16, 
p. 
104]). 
For 
0:, 
ß 
E 
(N 
U 
write 
L 
0: 
--+ 
ß 
G 
if 
ß 
can 
be 
derived 
from 
0: 
by 
a 
sequence 
of 
steps 
in 
which 
productions 
are 
applied 
only 
to 
the 
leftmost 
symbol 
in 
the 
sentential 
form 
(wh 
ich 
must 
therefore 
be 
a 
nonterminal). 
For 
A 
E 
N 
and 
a 
E 
define 
RA,a 
= 
{ß 
E 
N* 
I 
A 
1; 
aß}. 
For 
example, 
in 
the 
CNF 
grammar 
(21.1), 
we 
would 
have 
C 
-S 
SB 
-S 
SßB 
G G 
so 
CSSB 
E 
R
c
,[. 
-S 
SSSB 
G 
-S 
ACSSB 
G 
L 
--+ 
G 
[CSSB, 
The 
set 
RA,a 
is 
a 
regular 
set 
over 
the 
alphabet 
N, 
because 
the 
grammar 
with 
nonterminals 
{AI 
I 
A 
E 
N}, 
terminals 
N, 
start 
symbol 
S', 
and 
productions 
{AI 
BIC 
I 
A 
BC 
E 
P} 
U 
{A' 
f 
I 
A 
a 
E 
P} 

_____________________________________________
Normal 
Forms 
145 
is 
a 
strongly 
left-linear 
grammar 
for 
it.
I 
This 
may 
seem 
slightly 
bizarre, 
since 
the 
terminals 
of 
this 
grammar 
are 
the 
nonterminals 
N 
of 
G, 
but 
a 
moment's 
thought 
will 
convince 
you 
that 
it 
makes 
perfect 
sense. 
Since 
RA,a 
is 
regular, 
by 
Homework 
5, 
Exercise 
1 
it 
also 
has 
a 
strongly 
right-linear 
grammar 
G 
A,a; 
is, 
one 
in 
which 
all 
productions 
are 
of 
the 
form 
X 
BY 
or 
X 
f, 
where 
X, 
Y 
are 
nonterminals 
of 
G 
A,a 
and 
BEN. 
Let 
T 
A,a 
be 
the 
start 
symbol 
of 
G 
A,a' 
Assurne 
without 
loss 
of 
generality 
that 
the 
sets 
of 
nonterminals 
of 
the 
mars 
G 
A,a 
and 
Gare 
pairwise 
disjoint. 
This 
assumption 
can 
be 
enforced 
by 
renaming 
if 
necessary. 
Form 
the 
grammar 
GI 
by 
adding 
all 
the 
minals 
and 
productions 
of 
all 
the 
GA,a 
to 
G. 
Take 
the 
start 
symbol 
of 
GI 
to 
be 
S. 
Productions 
of 
GI 
are 
of 
the 
following 
three 
forms: 
where 
b 
E 
E 
and 
BEN. 
Note 
that 
GI 
is 
trivially 
equivalf'nt 
to 
G, 
since 
none 
of 
the 
new 
nonterminals 
can 
be 
derived 
from 
S. 
Now 
let 
G
2 
be 
the 
grammar 
obtained 
from 
GI 
by 
removing 
any 
production 
of 
the 
form 
and 
replacing 
it 
with 
the 
productions 
X 
bTB,bY 
for 
all 
b 
E 
E. 
Productions 
of 
G
2 
are 
of 
the 
form 
X 
-> 
b, 
where 
b 
E 
E. 
Finally, 
get 
rid 
of 
the 
f-productions 
in 
G
2 
using 
the 
construction 
of 
Lemma 
21.3. 
This 
construction 
does 
not 
introduce 
any 
unit 
productions, 
since 
every 
non-f-production 
has 
a 
terminal 
symbol 
on 
the 
right-hand 
side. 
Thus 
the 
resulting 
grammar 
G
3 
is 
in 
Greibach 
form 
with 
at 
most 
two 
nonterminals 
c;m 
the 
right-hand 
side 
of 
any 
production. 
Before 
we 
prove 
that 
L(G
3
) 
= 
L(G), 
let's 
pause 
and 
illustrate 
the 
tion 
with 
an 
example. 
Example 
21.6 
Consider 
the 
balanced 
parentheses 
of 
Example 
21.5. 
Starting 
with 
the 
Chomsky 
grammar 
S 
AB 
I 
AC 
ISS, 
C 
-> 
SB, 
A 
-> 
[, 
1 
See 
Homework 
5, 
Exercise 
1. 

_____________________________________________
146 
Lecture 
21 
first 
compute 
the 
regular 
sets 
RD,d: 
R
s
,[ 
= 
(B 
+ 
G)S*, 
R
c
,[ 
= 
(B 
+ 
G)S* 
B, 
R
A
,[ 
= 
RB,] 
= 
{E}, 
and 
all 
other5 
c1re 
0. 
Here 
are 
strongly 
right-linear 
gramm 
ars 
for 
these 
sets: 
T
s
,[ 
-+ 
BX 
I 
GX, 
T
c
,[ 
-+ 
BY 
I 
GY, 
T
A
,[ 
-+ 
E, 
TB,] 
-+ 
f. 
x 
-+ 
SXI 
f, 
Y 
-+ 
SY 
I 
BZ, 
Z 
-+ 
f, 
Combining 
these 
grammars 
with 
G 
and 
making 
the 
substitutions 
as 
scribed 
above, 
we 
obtain 
the 
grammar 
G
2
: 
S 
-+ 
[TA,[B 
I 
[TA,[G 
I 
[Ts,[S, 
T
s
,[ 
-+ 
]TB,]X 
I 
[Tc, 
[X, 
T
c
,[ 
-+ 
]TB,]Y 
I 
[Tc,[Y, 
T
A
,[ 
-+ 
f, 
G 
-+ 
[Ts,[B, 
X 
-+ 
[Ts,[X 
I 
E, 
Y 
-+ 
[Ts,[Y 
I 
]TB,]Z, 
TB,] 
-+ 
E. 
Removing 
f-transitions, 
we 
get 
the 
Greibach 
grammar 
G
3
: 
S 
-+ 
[B 
I 
[G 
I 
[Ts,[S, 
T
s
,[ 
-+]X 
I 
[Tc,[X 
I 
b 
I 
[T
c
,[, 
T
c
,[ 
-+]Y 
I 
[Tc,[Y, 
G 
-+ 
[Ts,[B, 
A 
-+ 
[, 
X 
-+ 
[Ts,[X 
I 
[T
s
,[, 
B 
-+], 
Y 
-+ 
[Ts,[Y 
I]. 
The 
Greibach 
grammar 
produced 
by 
this 
construction 
is 
by 
no 
means 
the 
simplest 
possible, 
as 
can 
be 
seen 
by 
comparing 
it 
to 
the 
somewhat 
simpler 
(21.2). 
0 
Now 
we 
prove 
that 
L( 
G) 
= 
L( 
G
3
). 
Surely 
L( 
G) 
= 
L( 
GI), 
since 
none 
of 
the 
new 
nonterminals 
added 
in 
the 
construction 
of 
GI 
can 
be 
derived 
from 
any 
nonterminalof 
G, 
including 
the 
start 
symbol 
S 
of 
GI. 
Also, 
L(G2) 
= 
L(G3) 
by 
Lemma 
21.3. 
Thus 
the 
heart 
ofthe 
proofis 
showing 
that 
L(Gd 
= 
L(G2)' 
Lemma 
21.7 
For 
any 
non 
terminal 
X and x 
E 
E*, 
X 
x 
{::::::> 
X 
x. 
GI 
G2 
Proof. 
The 
proof 
is 
by 
induction 
on 
the 
length 
of 
derivations. 
If 
x 
can 
be 
derived 
in 
one 
step 
from 
X 
'in 
either 
grammar, 
then 
it 
must 
be 
by 
a 
production 
of 
the 
form 
X 
-+ 
b 
or 
X 
-+ 
E, 
and 
these 
productions 
are 
the 
same 
in 
both 
grammars. 
For 
the 
induction 
step, 
we 
show 
that 

_____________________________________________
x 
X 
starting 
with 
the 
production 
X -
BY 
GI 
if 
and 
only 
if 
Normal 
Forms 
147 
X 
* 
X 
starting 
with 
the 
production 
X -
bTB,bY, 
where 
b 
is 
the 
first 
symbol 
of 
x. 
Note 
that 
X 
must 
have 
a 
first 
symbol, 
since 
derivations 
in 
GI 
starting 
with 
X -
BY 
cannot 
generate 
E, 
because 
B 
is 
a 
nonterminal 
of 
the 
original 
CNF 
grammar 
G, 
therefore 
can 
generate 
only 
nonnull 
strings. 
Any 
leftmost 
derivation 
GI 
GI 
is 
of 
the 
form 
I 
k+l. 
m 
X 
--BY 
--bB
I
B
2
···BkY 
--bz, 
GI 
GI 
GI 
where 
bB
I
B
2
·ŁŁ 
BkY 
is 
the 
first 
sentential 
form 
in 
the 
sequence 
in 
which 
the 
terminal 
b 
appears, 
and 
B
I
B
2
ŁŁŁ 
Bk 
E 
RB,b. 
By 
definition 
of 
the 
grammar 
G 
B,b, 
this 
occurs 
if 
and 
only 
if 
I 
k+1 
. m 
X 
--
bTB 
bY 
--
bB
I
B
2 
ŁŁŁ 
BkY 
--
bz, 
, 
GI 
GI 
where 
the 
subderivation 
TB 
b 
B
I
B
2 
···Bk 
, 
GI 
is 
a 
leftmost 
derivation 
in 
G 
B,b. 
By 
the 
induction 
hypothesis, 
this 
occurs 
iff 
o 
It 
follows 
from 
Lemma 
21.7 
by 
taking 
X 
= 
S 
that 
L(G
I
) 
= 
L(G
2
), 
therefore 
L( 
G) 
= 
L( 
G
3
). 
Historical 
Notes 
Bar-Hillel, 
Perles, 
and 
Shamir 
[8] 
showed 
how 
to 
get 
rid 
of 
E-
and 
unit 
productions. 
Chomsky 
and 
Greibach 
normal 
forms 
are 
due 
to 
Chomsky 
[18] 
and 
Greibach 
[53], 
respectively. 

_____________________________________________
Lecture 
22 
The 
Pumping 
Lemma 
for 
CFLs 
There 
is 
a 
pumping 
lemma 
for 
CFLs 
similar 
to 
the 
one 
for 
regular 
sets. 
It 
can 
be 
used 
in 
the 
same 
way 
to 
show 
that 
certain 
sets 
are 
not 
context-free. 
Here 
is 
the 
official 
version; 
there 
will 
also 
be 
a 
corresponding 
game 
with 
the 
demon 
that 
will 
be 
useful 
in 
practice. 
Theorem 
22.1 
(Pumping 
lemma 
far 
CFLs) 
For 
every 
CFL 
A, 
there 
exists 
k 
0 
such 
that 
every 
z 
E 
A 
01 
length 
at 
least 
k 
can 
bebroken 
up 
into· 
jive 
substrings 
z 
= 
uvwxy 
such 
that 
vx 
'" 
E, 
Ivwxl 
$ 
k, 
and 
lor 
all 
i 
0, 
uviwxiy 
E 
A. 
Informally, 
for 
every 
CFL 
A, 
every 
sufficiently 
long 
string 
in 
A 
can 
be 
subdivided 
into 
five 
segments 
such 
that 
the 
middle 
three 
segments 
are 
not 
too 
long, 
the 
second 
and 
fourth 
are 
not 
both 
null, 
and 
no 
matter 
how 
many 
extra 
copies 
of 
the 
second 
and 
fourth 
you 
pump 
in 
simultaneously, 
the 
string 
stays 
in 
A . 
. 
Note 
that 
this 
differs 
from 
the 
pumping 
lemma 
for 
regular 
sets 
in 
that 
we 
pump 
simultaneously 
on 
two 
substrings 
v 
and 
x 
separated 
by 
a 
substring 
w. 
The 
key 
insight 
that 
gives 
this 
theorem 
is 
that 
ror 
a 
gramm 
ar 
in 
Chomsky 
normal 
form, 
any 
parse 
tree 
for 
a 
very.long 
string 
must 
have 
a 
very 
long 
path, 
and 
any 
very 
long 
path 
must 
have 
at 
least 
two 
occurrences 
of 
some 

_____________________________________________
The 
Pumping 
Lemma 
for 
CFLs 
149 
nonterminal. 
A 
parse 
tree 
or 
derivation 
tree 
of 
astring 
z 
is 
a 
tree 
ing 
the 
productions 
applied 
in 
a 
derivation 
of 
z 
from 
the 
start 
symbol 
S 
independent 
of 
the 
order 
of 
application. 
For 
example, 
consider 
the 
Chomsky 
grammar 
S 
-+ 
AG 
I 
AB, 
A 
-+ 
a, 
B 
-+ 
b, 
G 
-+ 
SB 
for 
{anb
n 
I 
n 
1}. 
Here 
is 
a 
parse 
tree 
for 
the 
string 
a
4
b
4 
in 
this 
grammar: 
S 
/\ 
A 
G 
/ 
/\ 
a 
S 
B 
/\ 
\ 
A G b 
/ 
/\ 
a 
S 
B 
/\ 
\ 
A 
G b 
/ 
/\ 
a 
S 
B 
/\ 
\ 
A B 
b 
/ \ 
a 
b 
The 
productions 
can 
be 
applied 
in 
any 
order. 
For 
example, 
a 
leftmost 
derivation 
of 
a
4
b
4 
(always 
applying 
a 
production 
to 
the 
leftmost 
remaining 
nonterminal) 
would 
give 
S 
-+ 
AG 
-+ 
aG 
-+ 
aSB 
-+ 
aAGB 
-+ 
aaGB 
-; 
aaSBB 
-+ 
aaAGBB 
-+ 
aaaGBB 
-+ 
aaaSBBB 
-; 
aaaABBBB 
-; 
aaaaBBBB 
-+ 
aaaabBBB 
-; 
aaaabbBB 
-+ 
aaaabbbB 
-+ 
aaaabbbb, 
and 
a 
rightmost 
derivation 
would 
give 
S 
-+ 
AG 
-+ 
ASB 
-+ 
ASb 
-+ 
AAGb 
-+ 
AASBb 
-+ 
AASbb 
-+ 
AAAGbb 
-+ 
AAASBbb 
-+ 
AAASbbb 
-+ 
AAAABbbb 
-+ 
AAAAbbbb 
-+ 
AAAabbbb 
-+ 
AAaabbbb 
-+ 
Aaaabbbb 
-+ 
aaaabbbb, 
but 
these 
two 
derivations 
have 
the 
same 
parse 
tree, 
namely 
the 
one 
pictured 
above. 
Parse 
trees 
of 
Chomsky 
grammars 
for 
long 
strings 
must 
have 
long 
paths. 
because 
the 
number 
of 
symbols 
can 
at 
most 
double 
when 
you 
go 
down 
a 
level. 
This 
is 
because 
the 
right-hand 
sides 
of 
productions 
contain 
at 
most 

_____________________________________________
150 
Lecture 
22 
two 
symbols. 
For 
example, 
take 
the 
tree 
above 
and 
duplicate 
the 
terminals 
generated 
at 
each 
level 
on 
alliower 
levels, 
just 
to 
keep 
track 
of 
the 
symbols 
that 
have 
been 
generated 
so 
far: 
S 
/\ 
A 
c 
/ 
/\ 
a 
S 
B 
/ 
/\ 
\ 
a 
A 
C 
b 
/ / 
/\ 
\ 
a 
a 
S 
B 
b 
/ 
/ 
/\ 
\ \ 
a 
A 
C 
b b 
/ 
/ / 
/\ 
\ \ 
a a 
a 
S 
B 
b b 
/ / 
/ 
/\ 
\ \ \ 
a 
a a 
A B 
b b b 
/ / 
/ 
/ 
\ \ 
\ 
\ 
a 
a 
a a 
b b b b 
The 
number 
of 
symbols 
at 
each 
level 
is 
at 
most 
twice 
the 
number 
on 
the 
level 
immediately 
above. 
Thus 
at 
the 
very 
most, 
we 
can 
have 
one 
symbol 
at 
the 
top 
level 
(level 
0), 
2 
at 
level 
1, 
4 
at 
level 
2, 
... 
, 2
i 
at 
level 
i. 
In 
order 
to 
have 
at 
least 
2
n 
symbols 
at 
the 
bottom 
level, 
the 
tree 
must 
be 
of 
depth 
1 
at 
least 
n; 
that 
is, 
it 
must 
have 
at 
least 
n 
+ 
1 
levels. 
Proo/ 
0/ 
the 
pumping 
lemma. 
Let 
G 
be 
a 
grammar 
for 
A 
in 
Chomsky 
normal 
form. 
Take 
k 
= 
2
n
+1, 
where 
n 
is 
the 
number 
of nonterminals 
of 
G. 
Suppose 
z 
E 
A 
and 
Izl 
k. 
By 
the 
argument 
ab 
ove 
, 
any 
parse 
tree 
in 
G 
for 
z 
must 
be 
of 
depth 
at 
least 
n 
+ 
1. 
Consider 
the 
Ion 
gest 
path 
in 
the 
tree. 
(In 
the 
example 
above, 
the 
path 
from 
S 
at 
the 
root 
down 
to 
the 
leftmost 
b 
in 
the 
terminal 
string 
is 
such 
a 
path.) 
That 
path 
is 
of 
length 
at 
least 
n 
+ 
1, 
therefore 
must 
contain 
at 
least 
n 
+ 
1 
occurrences 
of 
nonterminals. 
By 
the 
pigeonhole 
principle, 
some 
nonterminal 
oeeurs 
more 
than 
onee 
along 
the 
path. 
Take 
the 
first 
pair 
of 
oeeurrences 
of 
the 
same 
nonterminal 
along 
the 
path, 
reading 
from 
bottom 
to 
top. 
In 
the 
example 
above, 
we 
would 
take 
the 
two 
circled 
oeeurrenees 
of 
S: 
1 
The 
depth 
is 
the 
number 
of 
edges 
on 
the 
longest 
path 
from 
the 
root 
to 
a 
leaf. 

_____________________________________________
The 
Pumping 
lemma 
for 
CFls 
151 
S 
/\ 
A C 
/ 
/\ 
a 
S B 
/ 
/\ 
\ 
a 
A C b 
/ 
/ 
/\ 
\ 
a a 
B b 
/ 
/ 
/\ 
\ \ 
a a A C b b 
/ / 
\ \ 
a a a 
B b b 
/ 
/ 
/ 
\ 
\ \ 
a a a 
A B 
b b b 
/ / 
/ 
/ 
\ \ \ \ 
a a a a 
b b b b 
Say 
X 
is 
the 
nontetminal 
with 
two 
occurrences. 
Break 
z 
up 
into 
substrings 
'Uvwxy 
such 
that 
w 
is 
the 
string 
of 
terminals 
generated 
by 
the 
lower 
rence 
of 
X 
and 
vwx 
is 
the 
string 
generated 
by 
the 
upper 
occurrence 
of 
X. 
In 
our 
running 
example, 
w 
= 
ab 
is 
the 
string 
generated 
by 
the 
lower 
rence 
of 
Sand 
vwx 
= 
aabb 
is 
the 
string 
generated 
by 
tfle 
upper 
occurrence 
of 
S: 
S 
/\ 
A C 
/ 
/\ 
a 
S B 
/ 
/\ 
\ 
a 
A C b 
/ / 
/\ 
\ 
/a /a 
B\ 
b\ 
a a 
A C b b 
/ / 
/ 
/\ 
\ \ 
/a 
/a 
/ 
a 
B\ 
b\ 
b\ 
a a a 
A B b b b 
/ / / / \ \ 
\ 
\ 
a a a a 
b b b b 
""-v--' 
.......,., 
_____ 
.......,., 
""-v--' 
'U 
V 
W 
X 
Y 

_____________________________________________
152 
Lecture 
22 
Thus 
in 
this 
example 
we 
have 
u 
= 
aa, 
v 
= 
a, 
W 
= 
ab, 
x 
= 
b, 
and 
y 
= 
bb. 
Let 
T 
be 
the 
subtree 
rooted 
at 
the 
upper 
occurrence 
of 
X 
and 
let 
t 
be 
the 
subtree 
rooted 
at 
the 
lower 
occurrence 
of 
X. 
In 
our 
example, 
T 
=1\ 
=1\ 
A 
C 
t 
/ 
/\ 
a 
1\ 
B\ 
A 
B 
/ 
/ 
\ 
a 
A 
B 
b 
a 
b 
/ / 
\ \ 
--------
W 
a 
a 
b b 
--------
v 
W 
x 
By 
removing 
t 
from 
the 
original 
tree 
and 
replacing 
it 
with 
a 
copy 
of 
T, 
we 
get 
a 
valid 
tree 
of 
uv
2
wx
2
y: 
a 
/ 
a 
/ 
a a 
s 
" 
\ 
/ 
A C 
/ 
/\ 
a 
S B 
/ 
/\ 
\ 
a 
A 
C 
b 
/ / 
/\ 
\ 
/a 
/a 
1\ 
B\ 
b\ 
a a 
A C b b 
/ 
/ / 
/\ 
\ \ 
/a 
/a 
1\ 
B\ 
b\ b\ 
a a A C b b b 
/ / / 
/\ 
\ \ 
\ 
a a 
(ß)B 
b b b 
/ 
/\ 
\ 
"--..--' 
v a 
A B b 
x 
y 
/ / \ \ 
a 
a 
b b 
v 
W 
x 
We 
can 
repeat 
this 
cutting 
out 
of 
t 
and 
replacing 
it 
with 
a 
copy 
of 
T 
as 
many 
times 
as 
we 
like 
to 
get 
a 
valid 
parse 
tree 
for 
uviwxiy 
for 
any 
i 
;::: 
l. 
We 
can 
even 
cut 
T 
out 
of 
the 
original 
tree 
and 
replace 
it 
with 
t 
to 
get 
a 
parse 
tree 
for 
uvOwxOy 
= 
?LU'Y: 

_____________________________________________
The 
Pum:;>ing 
Lemma 
for 
CFLs 
153 
-----------------------------------------------------
s 
/\ 
A 
c 
/ 
/\ 
a 
S 
B 
/ 
/\ 
\ 
a 
A 
C 
b 
/ / 
/\ 
\ 
a a 
1\ 
B b 
/ 
/ \ 
\ 
a a 
A 
B 
b b 
/ 
/ 
/ 
\ 
\ 
\ 
a a 
a 
b 
b 
b 
/ 
/ 
-----
\ 
\ 
w 
a a 
b 
b 
/ 
/ 
\ 
\ 
a a 
b b 
'--v-' 
'--v-' 
u 
Y 
Note 
that 
vx 
1= 
•; 
that 
is, 
v 
and 
x 
are 
not 
both 
null. 
We 
also 
have 
Ivwxl 
:s; 
k, 
since 
we 
chose 
the 
first 
repeated 
OCcurrence 
of 
a 
nonterminal 
reading 
from 
the 
bottom, 
and 
we 
must 
see 
such 
arepetition 
by 
the 
time 
w.e 
get 
up 
to 
height 
n+ 
1. 
Since 
we 
took 
the 
Ion 
gest 
path 
in 
the 
tree, 
the 
depth 
of 
the 
subtree 
under 
the 
upper 
occurrence 
of 
the 
repeated 
nonterminal 
X 
is 
at 
most 
n 
+ 
1, 
therefore 
can 
have 
no 
more 
than 
2
n
+1 
= 
k 
terminals. 
0 
Games 
with 
the 
Demon 
Like 
its 
regular 
cousin, 
the 
pumping 
lemma 
for 
CFLs 
is 
most 
usefn1 
::; 
its 
contrapositive 
form. 
In 
this 
form, 
it 
states 
that 
in 
order 
to 
conclude 
that 
A 
is 
not 
context-free, 
it 
suffices 
to 
establish 
the 
following 
property: 
Property 
22.2 
For 
all 
k 
0, 
there 
exists 
z 
E 
A 
of 
length 
at 
least 
k 
such 
that 
for 
all 
ways 
of 
breaking 
z 
up 
into 
substrings 
z 
= 
uvwxy 
with 
vx 
1= 
• 
and 
Ivwxl 
:s; 
k, 
there 
exists 
an 
i 
° 
such 
that 
uviwxiy 
rt 
A. 
Property 
22.2 
is 
equivalent 
to 
saying 
that 
you 
have 
a 
winning 
strategy 
in 
the 
following 
game 
with 
the 
demon: 
1. 
The 
demon 
picks 
k 
0. 
2. 
You 
pick 
z 
E 
A 
of 
length 
at 
least 
k. 

_____________________________________________
154 
Lecture 
22 
3. 
The 
demon 
picks 
strings 
u, 
v, 
w, 
x, 
Y 
such 
that 
z 
= 
uvwxy, 
Ivxl 
> 
0, 
and 
Ivwxl 
$ 
k. 
4. 
You 
pick 
i 
O. 
If 
uviwxiy 
A, 
then 
you 
win. 
If 
you 
want 
to 
show 
that 
a 
given 
set 
A 
is 
not 
context-free, 
it 
suffices 
to 
show 
that 
you 
have 
a 
winning 
strategy 
in 
this 
game; 
that 
is, 
no 
matter 
what 
the 
demon 
does 
in 
steps 
1 
and 
3, 
you 
have 
moves 
in 
steps 
2 
and 
4 
that 
can 
beat 
hirn. 
Example 
22.3 
Let's 
use 
the 
pumping 
lemma 
to 
show 
that 
the 
set 
A 
= 
{anbna
n 
I 
n 
O} 
is 
not 
context-free. 
We'H 
do 
this 
by 
showing 
that 
we 
can 
always 
win 
the 
game 
with 
the 
demon. 
Say 
the 
demon 
picks 
k 
in 
step 
1. 
You 
have 
to 
argue 
that 
you 
can 
win 
no 
matter 
what 
k 
iso 
A 
good 
choice 
for 
you 
in 
step 
2 
is 
to 
pick 
z 
= 
akbka
k
. 
Then 
z 
E 
A 
and 
Izl 
= 
3k 
k. 
Then 
in 
step 
3, 
say 
the 
demon 
picks 
u, 
v, 
w, 
x, 
Y 
such 
that 
z 
= 
uvwxy, 
vx 
i 
E, 
and 
Ivwxl 
$ 
k. 
You 
pick 
i 
= 
2. 
In 
every 
case, 
you 
win: 
if 
the 
demon 
picked 
either 
v 
or 
x 
to 
contain 
at 
least 
one 
a 
and 
at 
least 
one 
b, 
then 
uv
2
wx
2
y 
is 
not 
of 
the 
form 
a* 
b* 
a*, 
hence 
ccrtainly 
not 
in 
A; 
if 
the 
demon 
picked 
v 
and 
x 
to 
contain 
only 
a's, 
then 
uv
2
wx
2
y 
has 
more 
than 
twice 
as 
many 
a's 
as 
b's, 
hence 
is 
not 
in 
A; 
if 
the 
demon 
picked 
v 
and 
x 
to 
contain 
only 
b's, 
then 
uv
2
wx
2
y 
has 
fewer 
than 
twice 
as 
many 
a's 
as 
b's, 
hence 
is 
not 
in 
A; 
and 
finally, 
if 
one 
of 
v 
or 
x 
contains 
only 
a's 
and 
the 
other 
contains 
only 
b's, 
then 
uv
2
wx
2
y 
cannot 
be 
of 
the 
form 
ambma
m
, 
hence 
is 
not 
in 
A. 
In 
all 
cases 
you 
can 
ensure 
uv
2
wx
2
y 
A, 
so 
you 
have 
a 
winning 
strategy. 
By 
the 
pumping 
lemma, 
A 
is 
not 
context-free. 
0 
Example 
22.4 
Let's 
use 
the 
pumping 
lemma 
to 
show 
that 
the 
set 
A 
= 
{ww 
Iw 
E 
{a,b}*} 
is 
not 
context-free. 
Since 
the 
family 
of 
CFLs 
is 
closed 
under 
intersection 
with 
regular 
sets 
(Homework 
7, 
Exercise 
2), 
it 
suffices 
to 
show 
that 
the 
set 
A' 
= 
An 
L(a*b*a*b*) 
= 
{anbmanb
m 
I 
m,n 
O} 
is 
not 
context-free. 
Say 
the 
demon 
picks 
k. 
You 
pick 
z 
= 
akbkakb
k
. 
CaU 
each 
of 
the 
four 
substrings 
of 
the 
form 
a
k 
or 
b
k 
a 
block. 
Then 
z 
E 
A' 
and 
Izl 
k. 
Say 
the 
demon 
picks 
u, 
v, 
w, 
x, 
y 
such 
that 
z 
= 
uvwxy, 
vx 
-:J. 
E, 
and 
Ivwxl 
< 
k. 
No 
matter 
what 
the 
demon 
does, 
you 
can 
win 
by 
picking 
i 
= 
2: 

_____________________________________________
The 
Pumping 
Lemma 
for 
CFLs 
155 
Ł 
If 
one 
of 
v 
or 
x 
contains 
both 
a's 
and 
b's 
(i.e., 
if 
one 
of 
v 
or 
x 
straddles 
a 
block 
boundary), 
then 
uv
2
wx
2
y 
is 
not 
of 
the 
form 
a*b*a*b*, 
thus 
is 
not 
in 
A'. 
Ł 
If 
v 
and 
x 
are 
both 
from 
the 
same 
block, 
then 
uv
2
wx
2
y 
has 
one 
block 
Ion 
ger 
than 
the 
other 
three, 
therefore 
is 
not 
A' 
. 
Ł 
If 
v 
and 
x 
are 
in 
different 
blocks, 
then 
the 
blocks 
must 
be 
adjacent; 
otherwise 
Ivwxl 
would 
be 
greater 
than 
k. 
Thus 
one 
of 
the 
blocks 
taining 
v 
or 
x 
must 
be 
a 
block 
of 
a's 
and 
the 
other 
a 
block 
of 
b's. 
Then 
uv
2
wx
2
y 
has 
either 
two 
blocks 
of 
a's 
of 
different 
size 
(if 
vx 
contains 
an 
a) 
or 
two 
blocks 
of 
b's 
of 
different 
size 
(if 
vx 
contains 
ab) 
or 
both. 
In 
any 
case, 
uv
2
wx
2
y 
is 
not 
of 
the 
form 
anbmanb
m
. 
Since 
you 
can 
always 
ensure 
a 
win 
by 
playing 
this 
strategy, 
A' 
(and 
therefore 
A) 
is 
not 
a 
CFL 
by 
the 
pumping 
lemma. 
Surprisingly, 
the 
complement 
of 
A, 
namely 
{a,b}* 
-
{ww 
I 
w 
E 
{a,b}*}, 
is 
a 
CFL. 
Here 
is 
a 
CFG 
fQr 
it: 
5 
-+ 
AB 
I 
BA 
I 
AlB, 
A 
-+ 
CAC 
I 
a, 
B 
-+ 
CBC 
I 
b, 
C 
-+ 
alb. 
This 
grammar 
generates 
(i) 
an 
strings 
of 
odd 
length 
(starting 
with 
productions 
5 
-+ 
A 
and 
5 
B)j 
or 
(ii) 
strings 
of 
the 
form 
xayubv 
or 
ubvxay, 
where 
x, 
y, 
u, 
v 
E 
{a, 
b} 
*, 
lxi 
= 
Iyl, 
and 
lul 
= 
lvi· 
The 
nonterminal 
A 
generates 
an 
strings 
of 
the 
form 
xay, 
lxi 
= 
lyl. 
The 
nonterminal 
B 
generates 
an 
strings 
of 
the 
form 
ubv, 
lul 
= 
lvi. 
No 
string 
of 
the 
form 
(i) 
can 
be 
of 
the 
form 
ww, 
since 
ww 
is 
always 
of 
even 
length. 
No 
string 
of 
the 
form 
(ii) 
can 
be 
of 
the 
form 
ww, 
since 
there 
are 
occurrences 
of 
a 
and 
b 
separated 
by 
a 
distance 
of 
n/2, 
where 
n 
is 
the 
length 
of 
the 
string. 
This 
example 
shows 
that 
the 
family 
of 
CFLs 
is 
not 
closed 
under 
ment. 
0 
Note 
that 
in 
both 
these 
examples, 
your 
choice 
of 
i 
= 
2 
in 
step 
4 
was 
independent 
of 
the 
demon's 
move 
in 
step 
3. 
This 
may 
not 
always 
be 
possible! 
However, 
keep 
in 
mind 
that 
you 
have 
the 
freedom 
to 
pick 
i 
in 
step 
4 
after 
you 
have 
seen 
what 
the 
demon 
did 
in 
step 
3. 

_____________________________________________
156 
Lecture 
22 
Historical 
Notes 
The 
pumping 
lemma 
for 
CFLs 
is 
due 
to 
Bar-Hillel, 
Perles, 
and 
Shamir 
[8]. 
A 
somewhat 
stronger 
version 
was 
given 
by 
Ogden 
[96]. 

_____________________________________________
Lecture 
23 
Pushdown 
Automata 
A 
nondeterrninistic 
pushdown 
automaton 
(NPDA) 
is 
like 
a 
nondeterministic 
finite 
automaton, 
except 
it 
has 
a 
stack 
or 
pushdown 
store 
that 
it 
can 
use 
to 
record 
a 
potentially 
unbounded 
amount 
of 
information. 
Q 
A 
push/pop 
finite 
B 
stack 
control 
C 
B 
.1. 
The 
input 
head 
is 
read-only 
and 
may 
only 
move 
from 
left 
to 
right. 
The 
machine 
can 
store 
information 
on 
the 
stack 
in 
a 
last-in-first-out 
(LIFO) 
fashion. 
In 
each 
step, 
the 
machine 
pops 
the 
top 
symbol 
off 
the 
stack; 
based 
on 
this 
symbol, 
the 
input 
symbol 
it 
is 
currently 
reading, 
and 
its 
current 
state, 
it 
can 
push 
a 
sequence 
of 
symbols 
onto 
the 
stack, 
move 
its 
read 
head 
one 
ceU 
to 
the 
right, 
and 
enter 
a 
new 
state, 
according 
to 
the 
transition 
rules 

_____________________________________________
158 
Lecture 
23 
of 
the 
machine. 
We 
also 
allow 
E-transitions 
in 
which 
it 
can 
pop 
and 
push 
without 
reading 
the 
next 
input 
symbol 
or 
moving 
its 
read 
head. 
Although 
it 
can 
store 
an 
unbounded 
amount 
of 
information 
on 
the 
stack; 
it 
may 
not 
read 
down 
int<> 
the 
stack 
without 
popping 
the 
top 
elements 
off, 
in 
which case 
they 
are 
lost. 
Thus 
its 
access 
to 
the 
information 
on 
the 
stack 
is 
limited. 
Formally, 
a 
nondeterministic 
PDA 
is 
a 
7-tuple 
M 
= 
(Q, 
E, 
r, 
0, 
s, 
1., 
F), 
where 
Ł Q 
is 
a 
finite 
set 
(the 
states), 
Ł E 
is 
a 
finite 
set 
(the 
input 
alphabet), 
Ł r 
is 
a 
finite 
set 
(the 
stack 
alphabet), 
Ł 0 
(Q 
x 
(E 
U 
{E}) 
X 
r) 
X 
(Q 
x 
r*), 
6 
finite 
(the 
transition 
relation), 
Ł s 
E 
Q 
(the 
start 
state), 
Ł 
1. 
E 
r 
(the 
initial 
stack 
symbol), 
and 
Ł F 
Q 
(the 
final 
or 
accept 
states). 
If 
«p,a,A), 
(q,B
l
B2··· 
Bk)) 
E 
6, 
this 
means 
intuitively 
that 
whenever 
the 
machine 
is 
in 
state 
p 
reading 
input 
symbol 
a 
on 
the 
input 
tape 
and 
A 
on 
the 
top 
of 
the 
stack, 
it 
can 
pop 
A 
off 
the 
stack, 
push 
B
l
B
2
··· 
Bk 
onto 
the 
stack 
(Bk 
first 
and 
B
l 
last), 
move 
its 
read 
head 
right 
one 
cell 
past 
the 
a, 
and 
enter 
state 
q. 
If 
((p, 
E, 
A), 
(q,"BrB2··· 
Bk)) 
E 
6, 
this 
means 
intuitively 
that 
whenever 
the 
machine 
is 
in 
state 
p 
with 
A 
on 
the 
top 
of 
the 
stack, 
it 
can 
pop 
A 
off 
the 
stack, 
push 
B
l
B
2 
ŁŁŁ 
B7c 
onto 
the 
stack 
(Bk 
first 
and 
B
l 
last), 
leave 
its 
read 
head 
where 
it 
is, 
and 
enter 
state 
q. 
The 
machine 
is 
nondeterininistic, 
so 
there 
may 
be 
several 
transitions 
that 
are 
possible. 
Configu 
rations 
A 
configuration 
of 
the 
machine 
M 
is 
an 
element 
of 
Q 
x 
E* 
x 
r* 
describing 
the 
current 
state, 
the 
portion 
of 
the 
input 
yet 
unread 
(Le., 
under 
and 
to 
the 

_____________________________________________
Pushdown 
Automata 
159 
right 
ofthe 
read 
head) 
, 
and 
the 
current 
stack 
contents. 
A 
configuration 
gives 
complete 
information 
about 
the 
global 
state 
of 
M 
at 
some 
point 
during 
a 
computation. 
For 
example, 
the 
configuration 
(p, 
baaabba, 
AB 
AC 
1.) 
might 
describe 
the 
situation 
p 
A 
B 
A 
C 
1. 
The 
portion 
of 
the 
input 
to 
ihe 
left 
of 
the 
input 
head 
need 
not 
be 
represented 
in 
the 
configuration, 
because 
it 
cannot 
affect 
the 
computation 
from 
that 
point 
on. 
In 
general, 
the 
set 
of 
configurations 
is 
infinite. 
The 
start 
configuration 
on 
input 
x 
is 
(s, 
x, 
1.). 
That 
is, 
the 
machine 
always 
starts 
in 
its 
start 
state 
s 
with 
its 
read 
head 
pointing 
to 
the 
leftmost 
input 
symbol 
and 
the 
stack 
containing 
only 
the 
symbol 
1.. 
The 
next 
configuration 
relation 
describes 
how 
the 
machine 
can 
move 
M 
from 
one 
configuration 
to 
another 
in 
one 
step. 
It 
is 
defined 
formally 
as 
follows: 
if 
((p,a,A), 
(q,'Y)) 
E 
6, 
then 
for 
any 
y 
E 
1:* 
and 
ß 
E 
r*, 
(p,ay, 
Aß) 
-b 
(q,y,'Yß); 
and 
if 
((p,f,A), 
(q,'Y)) 
E 
6, 
then 
for 
any 
y 
E 
1:* 
and 
ß 
E 
r*, 
1 
(p,y,Aß) 
-;; 
(q,y,'Yß). 
(23.1 
) 
(23.2) 
In 
(23.1), 
the 
ay 
changed 
to 
y, 
indicating 
that 
the 
input 
symbol 
a 
was 
eatenj 
the 
Aß 
changed 
to 
'Yß, 
indicating 
that 
the 
A 
was 
popped 
and 
'Y 
was 
pushedj 
and 
the 
p 
changed 
to 
q, 
indicating 
the 
change 
of 
state. 
In 
(23.2), 

_____________________________________________
160 
Lecture 
23 
everything 
is 
the 
same 
except 
that 
the 
y 
does 
not 
change, 
indicating 
that 
no 
input 
symbol 
was 
eaten. 
No 
two 
configurations 
are 
related 
by 
2..... 
unless 
M 
required 
by 
(23.1) 
or 
(23.2). 
Define 
the 
relations 
and 
M 
C=D, 
M 
Ł 
-
M 
as 
follows: 
M 
M 
M 
M 
-
M 
Then 
is 
the 
reflexive 
transitive 
closure 
of 
2...... 
In 
other 
words, 
C 
D 
M 
M 
M 
if 
the 
configuration 
D 
follows 
from 
the 
configuration 
C 
in 
zero 
or 
more 
steps 
of 
the 
next 
configuration 
relation 
2...... 
M 
Acceptance 
There 
are 
two 
alternative 
definitions 
of 
acceptance 
in 
common 
use: 
by 
empty 
stack 
and 
by 
final 
state. 
It 
turns 
out 
that 
it 
doesn't 
matter 
which 
definition 
we 
use, 
since 
each 
kind 
of 
machine 
can 
simulate 
the 
other. 
Let's 
consider 
acceptance 
by 
final 
state 
first. 
Informally, 
the 
machine 
M 
is 
said 
to 
accept 
its 
input 
x 
by 
final 
state 
if 
it 
ever 
enters 
astate 
in 
F 
after 
scanning 
its 
entire 
input, 
starting 
in 
the 
start 
configuration 
on 
input 
x. 
Formally, 
M 
accepts. 
x 
by 
final 
state 
if 
(s,x,J..) 
(q,
•,1') 
M 
for 
some 
q 
E 
Fand 
l' 
E 
r*. 
In 
the 
right-hand 
configuration, 
• 
is 
the 
null 
string, 
signifying 
that 
the 
entire 
input 
has 
been 
read, 
and 
l' 
is 
junk 
left 
on 
the 
stack. 
Informally, 
M 
is 
said 
to 
accept 
its 
input 
x 
by 
empty 
stack 
if 
it 
ever 
pops 
the 
last 
element 
off 
the 
stack 
without 
pushing 
anything 
back 
on 
after 
reading 
the 
entire 
input, 
starting 
in 
the 
start 
configuration 
on 
input 
x. 
Formally, 
M 
accepts 
x 
by 
empty 
stack 
if 
(s,x,J..) 
(q,•,•) 
M 
for 
some 
q 
E 
Q. 
In 
this 
definition, 
the 
q 
in 
the 
right-hand 
configuration 
can 
be 
any 
state 
whatsoever, 
and 
the 
• 
in 
the 
second 
and 
third 
positions 
indicate 
that 
the 
entire 
input 
has 
been 
read 
and 
the 
stack 
is 
empty, 
respectively. 
Note 
that 
F 
is 
irrelevant 
in 
the 
definition 
of 
acceptance 
by 
empty 
stack. 
The 
two 
different 
forms 
of 
automata 
can 
simulate 
each 
other 
(see 
Lecture 
E); 
thus 
it 
doesn't 
matter 
which 
one 
we 
work 
with. 

_____________________________________________
Pushdown 
Automata 
161 
In 
either 
definition 
of 
acceptance, 
the 
entire 
input 
string 
has 
to 
be 
read. 
Because 
of 
•-transitions, 
it 
is 
possible 
that 
a 
PDA 
can 
get 
into 
an 
infinite 
loop 
without 
reading 
the 
entire 
input. 
Example 
23.1 
Here 
is 
a 
nondeterministic 
pushdown 
automaton 
that 
accepts 
the 
set 
of 
balanced 
strings 
of 
parentheses 
[ ] 
by 
empty 
stack:. 
It 
has 
just 
one 
state 
q. 
Informally, 
the 
machine 
will 
scan 
its 
input 
from 
to 
right; 
whenever 
it 
sees 
a 
[, 
it 
will 
push 
the 
[ 
onto 
the 
stack, 
and 
whenever 
it 
sees 
a] 
and 
the 
top 
stack 
symbol 
is 
[, 
it 
will 
pop 
the 
[ 
off 
the 
stack. 
(If 
you 
matched 
up 
the 
parentheses, 
you 
would 
see 
that 
the 
] 
it 
is 
currently 
reading 
is 
the 
one 
matching 
the 
[ 
on 
top 
of 
the 
stack 
that 
was 
pushed 
earlier.) 
Formally, 
let 
Q 
= 
{q}, 
E={[,J}, 
r 
= 
{1., 
Cl, 
start 
state 
= 
q, 
initial 
stack 
symbol 
= 
1., 
and 
let 
6 
consist 
of 
the 
following 
transitions: 
(i) 
((q, 
[,1.), 
(q, 
[1.)); 
(ii) 
«q, 
[, 
[), 
(q, 
Er)); 
(iii) 
«q,], 
[), 
(q,•)); 
(iv) 
(q, 
•, 
1.), 
(q, 
•)). 
Informally, 
transitions 
(i) 
and 
(ii) 
say 
that 
whenever 
the 
next 
input 
symbol 
is 
[, 
the 
[is 
pushed 
onto 
the 
stackon 
top 
ofthe 
symbol 
currently 
there 
tually, 
the 
symbol 
currently 
there 
is 
popped 
but 
then 
immediately 
pushed 
back 
on). 
Transition 
(iii) 
says 
that 
whenever 
the 
next 
input 
symbol 
is 
] 
and 
there 
is 
a [ 
on 
top 
of 
the 
stack, 
the 
[ 
is 
popped 
and 
nothing 
else 
is 
pushed. 
Transition 
(iv) 
is 
taken 
when 
the 
end 
of 
the 
input 
string 
is 
reached 
in 
order 
to 
dump 
the 
1. 
off 
the 
stack 
and 
accept. 

_____________________________________________
162 
Lecture 
23 
Here 
is 
a 
sequence 
of 
configurations 
Ieading 
to 
the 
acceptance 
of 
ihe 
bal-
anced 
string 
[ [ 
[] 
] 
[] 
] 
[]. 
Configuration 
Transition 
applied 
(q, 
[[ 
[]J 
[J] 
[], 
1-) 
start 
configuration 
--> 
(q, 
[ 
[]J 
[]J 
[l, 
[ 
1-) 
transition 
(i) 
--> 
(q, 
[]J[]J[], 
[ [ 
1-) 
transition 
(ii) 
--> 
(q, 
]] 
[J] 
[], 
[[ 
[1-) 
transition 
(ii) 
--> 
(q, 
][JJ[], 
[ [ 
1-) 
transition 
(iii) 
--> 
(q, 
t]] 
[J, 
[ 
1-) 
transition 
(iii) 
--> 
(q, 
] ] 
[], 
[[ 
1-) 
transition 
(ii) 
--> 
(q, 
] 
[], 
[ 
1-) 
transition 
(iii) 
--> 
(q, 
[J, 
1-) 
transition 
(iii) 
--> 
(q, 
] 
, 
[ 
1-) 
transition 
(i) 
--> 
(q, 
f, 
1-) 
transition 
(iii) 
--> 
(q, 
f, 
f) 
transition 
(iv) 
The 
machine 
could 
weIl 
have 
taken 
transition 
(iv) 
prematurelyat 
a 
couple 
of 
places; 
for 
example, 
in 
its 
very 
first 
step. 
This 
would 
have 
led 
to 
the 
configuration 
(q, 
[[ 
[]] 
[]] 
[],f), 
and 
the 
machine 
would 
have 
been 
stuck, 
since 
no 
transition 
is 
possible 
from 
a 
configuration 
with 
an 
empty 
stack. 
Moreover, 
this 
is 
not 
an 
accept 
configuration, 
since 
there 
is 
a 
nonnull 
portion 
of 
the 
input 
yet 
unread. 
However, 
this 
is 
not 
a 
problem, 
since 
the 
machine 
is 
nondeterministic 
and 
the 
usual 
rules 
for 
nondeterminism 
apply: 
the 
machine 
is 
said 
to 
accept 
the 
input 
if 
some 
sequence 
of 
transitions 
leads 
to 
an 
accept 
configuration. 
If 
it 
does 
take 
transition 
(iv) 
prematurely, 
then 
this 
was 
just 
a 
bad 
guess 
where 
the 
end 
of 
the 
input 
string 
was. 
To 
prove 
that 
this 
machine 
is 
correct, 
one 
must 
argue 
that 
for 
every 
anced 
string 
x, 
there 
is 
a 
sequence 
of 
transitions 
leading 
to 
an 
accept 
figuration 
from 
the 
start 
configuration 
on 
input 
X; 
and 
for 
every 
unbalanced 
string 
X, 
no 
sequence 
of 
transitions 
leads 
to 
an 
accept 
configuration 
from 
the 
start 
configuration 
on 
input 
x. 
0 
Example 
23.2 
We 
showed 
in 
Lecture 
22 
using 
the 
pumping 
lemma 
that 
the 
set 
{ww 
I 
w 
E 
{a,b}*} 
is 
not 
a 
CFL 
(and 
therefore, 
as 
we 
will 
show 
in 
Lecture 
25, 
not 
accepted 
by 
any 
NPDA) 
but 
that 
its 
complement 
{a,b}* 
-
{ww 
I 
W 
E 
{a,b}*} 
(23.3) 
iso 
The 
set 
(23.3) 
can 
be 
accepted 
by 
a 
nondeterministic 
pushdown 
ton 
as 
foIlows. 
InitiaIly 
guess 
whether 
to 
check 
for 
an 
odd 
number 
of 
input 

_____________________________________________
Pushdown 
Automata 
163 
symbols 
or 
for 
an 
even 
number 
of 
the 
form 
xayubv 
or 
ubvxay 
with 
lxi 
= 
lyl 
and 
lul 
= 
lvi. 
To 
check 
for 
the 
former 
condition, 
we 
do 
not 
need 
the 
stack 
at 
all-we 
can 
just 
count 
mod 
2 
with 
a 
finite 
automaton 
encoded 
in 
the 
finite 
control 
of 
the 
PDA. 
To 
check 
for 
the 
latter, 
we 
scan 
the 
input 
for 
a 
nondeterministically 
chosen 
length 
of 
time, 
pusJüng 
the 
input 
symbols 
onto 
the 
stack. 
We 
use 
the 
stack 
as 
an 
integer 
counter. 
At 
some 
terministically 
chosen 
time, 
we 
remember 
the 
current 
input 
symbol 
in 
the 
finite 
control-this 
is 
the 
a 
or 
b 
that 
is 
guessed 
to 
be 
the 
symbol 
in 
the 
first 
half 
not 
matching 
the 
corresponding 
symbol 
in 
the 
second 
half-then 
continue 
to 
scan, 
popping 
one 
symbol 
off 
the 
stack 
for 
each 
input 
symbol 
read. 
When 
the 
initial 
stack 
symbol 
.1. 
is 
on 
top 
of 
the 
stack, 
we 
start 
ing 
symbols 
again. 
At 
some 
point 
we 
nondeterministically 
guess 
where 
the 
corresponding 
symbol 
in 
the 
second 
half 
iso 
If 
it 
is 
the 
same 
symbol 
as 
the 
one 
remembered 
from 
the 
first 
half, 
reject. 
Otherwise 
we 
scan 
the 
rest 
of 
the 
input, 
popping 
the 
stack 
as 
we 
go. 
If 
the 
stack 
contains 
only 
.1. 
when 
the 
end 
of 
the 
input' 
is 
reached, 
we 
accept 
by 
popping 
the 
.1., 
leaving 
an 
em 
pty 
stack. 
CI 
We 
elose 
with 
a 
few 
technical 
re 
marks 
about 
NP 
DAs: 
1. 
In 
deterministic 
PDÄs 
(Supplementary 
Lecture 
F), 
we 
will 
need 
an 
endmarker 
on 
the 
input 
so 
that 
the 
machine 
knows 
when 
it 
is 
at 
the 
end 
of 
the 
input 
string. 
In 
NPDAs, 
the 
endmarker 
is 
unnecessary, 
since 
the 
machine 
can 
guess 
nondeterministically 
where 
the 
end 
of 
the 
string 
iso 
If 
it 
gllesses 
wrong 
and 
empties 
its 
stack 
before 
scanning 
the 
entire 
input, 
then 
that 
was 
just 
a 
bad 
guess. 
2. 
We 
distinguish 
the 
initial 
stack 
symbol 
.l 
only 
because 
we 
need 
itto 
define 
the 
start 
configuration. 
Other 
than 
that, 
it 
is 
treated 
like 
any 
other 
stack 
symbol 
and 
can 
be 
pushed 
and 
popped 
at 
any 
time. 
In 
particular, 
it 
need 
not 
stay 
on 
the 
bottom 
of 
the 
stack 
after 
the 
start 
configuration; 
it 
can 
be 
popped 
in 
the 
first 
move 
and 
something 
else 
pushed 
in 
its 
place 
if 
desired. 
3. 
A 
transition 
((p, 
a, 
A), 
(q, 
ß)) 
or 
((p, 
•, 
A), 
(q, 
ß» 
does 
not 
apply 
unless 
Ais 
on 
top 
of 
the 
stack. 
In 
particular, 
no 
transition 
applies 
if 
the 
stack 
is 
empty. 
In 
that 
case 
the 
machine 
is 
stuck. 
4. 
In 
acceptance 
by 
empty 
stack, 
the 
stack 
must 
be 
empty 
in 
a 
ration, 
that 
is, 
after 
applying 
a 
transition. 
In 
our 
intuitive 
description 
above, 
when 
a 
transition 
such 
as 
((p, 
•, 
A), 
(q, 
BC) 
is 
taken 
with 
only 
A 
on 
the 
stack, 
the 
stack 
is 
momentarily 
empty 
between 
the 
time 
A 
is 
popped 
and 
BC 
is 
pushed. 
This 
does 
not 
count 
in 
the 
definition 
of 
acceptance 
by 
empty 
stack. 
To 
accept 
by 
empty 
stack, 
everything 
must 
be 
popped 
and 
nothing 
pushed 
back 
on. 

_____________________________________________
Supplementary 
Lecture 
E 
Final 
State 
Versus 
Empty 
Stack 
It 
doesn't 
matter 
whether 
we 
take 
NPDAs 
to 
accept 
by 
empty 
stack 
or 
by 
final 
state; 
the 
two 
methods 
of 
acceptance 
are 
equivalent 
in 
the 
sense 
that 
each 
type 
of 
machine 
can 
simulate 
the 
other. 
Given 
an 
arbitrary 
NP 
DA 
M 
that 
accepts 
by 
final 
state 
or 
empty 
stack, 
we 
will 
show 
how 
to 
construct 
an 
equivalent 
NPDA 
M' 
with 
a 
single 
accept 
state 
for 
which 
acceptance 
by 
empty 
stack 
and 
by 
final 
state 
coincide. 
The 
construction 
of 
M' 
differs 
slightly, 
depending 
on 
whether 
M 
accepts 
by 
final 
state 
or 
by 
empty 
stack, 
but 
there 
is 
enough 
in 
common 
between 
the 
two 
constructions 
that 
we 
will 
do 
them 
together, 
pointing 
out 
where 
they 
differ. 
We 
have 
not 
discussed 
deterministic 
PDAs 
yet-we 
will 
do 
so 
in 
Supple" 
mentary 
Lecture 
F-but 
for 
future 
reference, 
the 
construction 
we 
are 
about 
to 
give 
can 
be 
made 
to 
preserve 
determinism. 
Let 
M 
= 
(Q, 
r, 
8, 
s, 
.1, 
F) 
be 
an 
NPDA 
that 
accepts 
by 
empty 
stack 
or 
by 
final 
state. 
Let 
'1./., 
t 
be 
two 
new 
states 
not 
in 
Q, 
and 
let 
JL 
be 
a 
new 
stack 
symbol 
not 
in 
r. 
Define 
if 
M 
accepts 
by 
empty 
stack, 
if 
M 
accepts 
by 
final 
state; 

_____________________________________________
Final 
State 
Versus 
Empty 
Stack 
165 
6. 
{ 
{JL} 
-
rU{JL} 
Consider 
the 
NPDA 
if 
M 
accepts 
by 
empty 
stack, 
if 
M 
accepts 
by 
final 
state. 
M' 
= 
rU,{JL}, 
0
'
, 
u, 
JL, 
{tl), 
where 
0' 
contains 
aH 
the 
transitions 
of 
0, 
as 
weH 
as 
the 
transitions 
((u,•,JL), 
(s,..LJL)), 
((q,•,A), 
(t,A)), 
q 
E 
G, 
A 
E 
6., 
((t,•,A), 
(t,•)), 
AErU{JL}. 
(E.l) 
(E.2) 
(E.3) 
Thus 
the 
new 
automaton 
M' 
has 
a 
new 
start 
state 
u, 
a 
new 
initial 
stack 
symbol 
JL, 
and 
a 
new 
single 
final 
state 
t. 
In 
the 
first 
step, 
by 
transition 
(E.l), 
it 
pushes 
the 
old 
initial 
stack 
symbol 
..L 
on 
'top 
of 
JL, 
then 
enters 
the 
old 
start 
state 
s. 
It 
can 
then 
run 
as 
M 
would, 
since 
it 
contains 
aH 
the 
transitions 
of 
M. 
At 
some 
point 
it 
might 
enter 
state 
t 
according 
to 
(E.2). 
Once 
it 
enters 
state 
t, 
it 
can 
dump 
everything 
off 
its 
stack 
using 
transitions 
(E.3). 
Moreover, 
this 
is 
the 
only 
way 
it 
can 
empty 
its 
stack, 
since 
it 
cannot 
pop 
JL 
except 
in 
state 
t. 
Thus 
acceptance 
by 
empty 
stack 
and 
by 
final 
state 
coincide 
for 
M'. 
Now 
we 
show 
that 
L(M') 
= 
L(M). 
Suppose 
first 
that 
M 
accepts 
by 
empty 
stack. 
If 
M 
accepts 
x, 
then 
(s,x,..L) 
(q,•,•) 
M 
for 
some 
n. 
But 
then 
(u,x,JL) 
(s,x,..LJL) 
7 
(q,•,JL) 
-b 
(t,•,JL) 
-b 
(t,•,•). 
Now 
suppose 
M 
accepts 
by 
final 
state. 
If 
M 
accepts 
x, 
then 
(s,x,..L) 
(q,•,'Y), 
q 
E 
F. 
M 
Then 
(u,x,JL) 
(s,x,..LJL) 
(q,•,'Y
JL
) 
(t,•,'Y
JL
) 
-i;: 
(t,•,•). 
Thus 
in 
either 
case, 
M' 
accepts 
x. 
Since 
x 
was 
arbitrary, 
L(M) 
<;;; 
L(M'). 
Conversely, 
suppose 
M' 
accepts 
x. 
Then 
(u, 
x, 
JL) 
-b 
(s, 
x, 
..LJL) 
(q, 
y, 
'Y
JL
) 
-b 
(t, 
y, 
'Y
JL
) 
-i? 
(t, 
•, 
•) 
for 
some 
q 
E 
G. 
But 
y 
= 
•, 
since 
M' 
cannot 
read 
any 
input 
symbols 
once 
it 
enters 
state 
t; 
therefore, 
(s,x,..L) 
(q,•,'Y). 
M 
(EA) 

_____________________________________________
166 
Supplementary 
Lecture 
E 
Now 
let's 
consider 
the 
definitions 
of 
G 
and 
6. 
and 
transitions 
(E.2) 
ing 
the 
first 
move 
into 
state 
t, 
and 
ask 
how 
the 
transition 
(q,f,,),JL) 
(t,f,,),JL) 
M 
could 
come 
about. 
If 
M 
accepts 
by 
empty 
stack, 
then 
we 
must 
have 
')' 
= 
f. 
On 
the 
other 
hand, 
if 
M 
accepts 
by 
final 
state, 
then 
we 
must 
have 
q 
E 
F. 
In 
either 
case, 
(EA) 
says 
that 
M 
accepts 
x. 
Since 
x 
was 
arbitrary, 
L(M') 
L(M). 

_____________________________________________
Lecture 
24 
PDAs 
and 
CFGs 
In 
this 
lecture 
and 
the 
next 
we 
will 
show 
that 
nondeterministic 
pushdown 
automata 
and 
context-free 
grammars 
are 
equivalent 
in 
expressive 
power: 
the 
languages 
accepted 
by 
NPDAs 
are 
exactly 
the 
context-free 
languages. 
In 
this 
lecture 
we 
will 
show 
how 
to 
convert 
a 
given 
CFG 
to 
an 
equivalent 
NPDA. 
We 
will 
do 
the 
other 
direction 
in 
Lecture 
25. 
Suppose 
we 
are 
given 
a 
CFG 
G 
= 
(N, 
P, 
S). 
We 
wish 
to 
construct 
an 
NPDA 
M 
such 
that 
L(M) 
= 
L(G). 
By 
a 
simple 
construction 
from 
Lecture 
21, 
we 
can 
assume 
without 
loss 
of 
generality 
that 
all 
productions 
of 
Gare 
of 
the 
form 
where 
c E 
U 
{E} 
and 
k 
o. 
We 
construct 
from 
G 
an 
equivalent 
NP 
DA 
M 
with 
only 
onestate 
that 
accepts 
by 
empty 
stack. 
Let 
M 
= 
({q}, 
N, 
8, 
q, 
S, 
0), 
where 
Ł q 
is 
the 
only 
state, 
Ł 
the 
set 
of 
terminals 
of 
G, 
is 
the 
input 
alphabet 
of 
M, 

_____________________________________________
168 
Lecture 
24 
Ł 
N, 
the 
set 
of 
nonterminals 
of 
C, 
is 
the 
stack 
alphabet 
of 
M, 
Ł 8 
is 
the 
transition 
relation 
defined 
below, 
Ł q 
is 
the 
start 
state, 
Ł 
S, 
the 
start 
symbol 
of 
C, 
is-
the 
initial 
stack 
symbol 
of 
M, 
Ł 
0, 
the 
null 
set, 
is 
the 
set 
of 
final 
states 
(actually, 
this 
is 
irrelevant, 
since 
M 
accepts 
by 
empty 
stack). 
The 
transition 
relation 
8 
is 
defined 
as 
follows. 
For 
each 
production 
A 
-+ 
cB
I
B
2 
..Ł 
Bk 
in 
P, 
let 
8 
contain 
the 
transition 
Thus 
8 
has 
one 
transition 
for 
each 
production 
of 
C. 
Recall 
that 
for 
c E 
this 
says, 
"When 
in 
state 
q 
scanning 
input 
symbol 
c 
with 
A 
on 
top 
of 
the 
stack, 
scan 
past 
the 
c, 
pop 
A 
off 
the 
stack, 
push 
BI 
B
2 
ŁŁŁ 
Bk 
onto 
the 
stack 
(Bk 
first), 
and 
enter 
state 
q," 
and 
for 
c 
= 
1', 
"When 
in 
state 
q 
with 
A 
on 
top 
of 
the 
stack, 
without 
scanning 
an 
input 
symbol, 
pop 
A 
off 
the 
stack, 
push 
B
I
B
2
Ł·· 
Bk 
onto 
the 
stack 
(Bk 
first), 
and 
enter 
state 
q." 
That 
completes 
the 
description 
of 
M. 
Before 
we 
prove 
L(M) 
= 
L(C), 
let's 
look 
at 
an 
example. 
Consider 
the 
set 
of 
nonnull 
balanced 
strings 
of 
parentheses 
[ 
]. 
Below 
we 
give 
a 
list 
of 
productions 
of 
a 
gramm 
ar 
in 
Greibach 
normal 
form 
for 
this 
set. 
Beside 
each 
production, 
we 
give 
the 
corresponding 
transition 
of 
the 
NP 
DA 
as 
specified 
by 
the 
construction 
above. 
(i) 
S 
-+ 
[BS 
((q, 
[,S), 
(q,BS)) 
(ii) 
S 
-+ 
[B 
((q, 
[, 
S), 
(q, 
B)) 
(iii) 
S 
-+ 
[SB 
((q, 
[, 
S), 
(q, 
SB)) 
(iv) 
S 
-+ 
[SBS 
((q, 
[,S), 
(q,SBS)) 
(v) 
B 
-+ 
J 
((q,J,B), 
(q,•)) 
Recall 
that 
a 
leftmost 
derivation 
is 
one 
in 
which 
productions 
are 
always 
applied 
to 
the 
leftmost 
nonterminal 
in 
the 
sentential 
form. 
We 
will 
show 
that 
a 
leftmost 
derivation 
in 
G 
of 
a 
terminal 
string 
x 
corresponds 
exact1y 
to 
an 
accepting 
computation 
of 
M 
on 
input 
x. 
The 
sequence 
of 
sentential 
forms 
in 
the 
leftmost 
derivation 
corresponds 
to 
the 
sequence 
of 
configurations 
of 
M 
in 
the 
computation. 

_____________________________________________
PDAs 
and 
CFGs 
169 
For 
example, 
consider 
the 
input 
x 
= [ [ [ ] ] 
[ ] 
]. 
In 
the 
middle 
column 
below 
is 
a 
sequence 
of 
sentential 
forms 
in 
a 
leftmost 
derivation 
of 
x 
in 
G. 
In 
the 
right 
column 
is 
the 
corresponding 
sequence 
of 
configurations 
of 
M. 
In 
the 
left 
column 
is 
the 
number 
of 
the 
production 
or 
transition 
applied. 
Sentential 
forms 
in 
a 
Configurations 
of 
M 
in 
an 
Rule 
leftmost 
derivation 
of 
accepting 
computation 
of 
applied 
x 
in 
G 
M 
on 
input 
x 
S 
(q, 
[ [ 
[] 
] 
[] 
], 
S) 
(iii) 
[SB 
(q, 
[[]] 
[]], 
SB) 
(iv) 
[ 
[SBSB 
(q, 
[] 
] 
[] 
], 
SBSB) 
(ii) 
[[ 
[BBSB 
(q, 
]] 
[]], 
BBSB) 
(v) 
[ [ 
[]BSB 
(q, 
] 
[] 
], 
BSB) 
(v) 
[[[]]SB 
(q, 
[]], 
SB) 
(ii) 
[ [ 
[] 
] 
[BB 
(q, 
]], 
BB) 
(v) 
[[[]][]B 
(q, 
], 
B) 
(v) 
[[[]][]] 
(q, 
•, 
•) 
In 
the 
middle 
column, 
the 
first 
sentential 
form 
is 
the 
start 
symbol 
of 
G 
and 
the 
last 
sentential 
form 
is 
the 
terminal 
siring 
x. 
In 
the 
right 
column 
the 
first 
configuration 
is 
the 
start 
configuration 
of 
M 
on 
input 
x 
and 
the 
last 
configuration 
is 
an 
accept 
configuration 
(the 
two 
•'s 
denote 
that 
the 
entire 
input 
has 
been 
read 
and 
the 
stack 
is 
empty). 
One 
can 
see 
from 
this 
example 
the 
correspondence 
between 
the 
sentential 
forms 
and 
the 
configurations. 
In 
the 
sentential 
forms, 
the 
terminal 
string 
x 
is 
generated 
from 
left 
to 
right, 
one 
terminal 
in 
each 
step, 
just 
like 
the 
input 
string 
x 
in 
the 
automaton 
is 
scanned 
off 
from 
left 
to 
right, 
one 
bol 
in 
each 
step. 
Thus 
the 
two 
strings 
of 
terminals 
appearing 
in 
each 
row 
always 
concatenate 
to 
give 
x. 
Moreover, 
the 
string 
of 
nonterminals 
in 
each 
sentential 
form 
is 
exactly 
the 
contents 
of 
the 
stack 
in 
the 
corresponding 
configuration 
of 
the 
PDA. 
We 
can 
formalize 
this 
observation 
in 
a 
general 
lemma 
that 
relates 
the 
tential 
forms 
in 
leftmost 
derivations 
of 
x 
E 
G 
and 
the 
configurations 
of 
M 
in 
accepting 
computations 
of 
M 
on 
input 
x. 
This 
lemma 
holds 
not 
just 
for 
the 
example 
above 
but 
in 
general. 
Lemma 
24.1 
For 
any 
z, 
y 
E 
E*, 
"{ 
E 
N*, 
and 
A 
E 
N, 
A 
z"{ 
via 
a 
leftmost 
derivation 
if 
and 
only 
if 
(q, 
zy, 
A) 
(q, 
y, 
"(). 
G 
M 
For 
example, 
in 
the 
fourth 
row 
of 
the 
table 
above, 
we 
would 
have 
z 
= [ [ 
[, 
y 
= ] ] 
[] 
], 
"{ 
= 
BBSB, 
A 
= 
S, 
and 
n 
= 
3. 
Proof. 
The 
proof 
is 
by 
induction 
on 
n. 

_____________________________________________
170 
Lecture 
24 
Basis 
For 
n 
= 
0, 
we 
have 
A 
z'Y 
{=:::} 
A 
= 
z'Y 
G 
Induction 
step 
{=:::} 
z 
= 
t: 
and 
'Y 
= 
A 
{=:::} 
(q, 
zy, 
A) 
= 
(q, 
y, 
'Y) 
o 
{=:::} 
(q, 
zy, 
A) 
-
(q, 
y, 
'Y). 
M 
We 
do 
the 
two 
implications 
=> 
and 
<= 
separately 
for 
clarity. 
First 
suppose 
A 
z'Y 
via 
a 
leftmost 
derivation. 
Suppose 
that 
B 
-+ 
cß 
was 
G 
the 
last 
production 
applied, 
where 
c E 
E 
U 
{t:} 
and 
ß 
E 
N*. 
Then 
A 
uBa 
ucßa 
= 
Z'Y, 
G G 
.. 
where 
z 
= 
uc 
and 
'Y 
= 
ßa. 
By 
the 
induction.hypothesis, 
(q,ucy,A) 
(q,cy, 
Ba). 
M 
By 
the 
definition 
of 
M, 
((q,c,B), 
(q,ß)) 
E 
8, 
thus 
1 
(q,cy,Ba) 
-'---.t 
(q,y,ßa). 
M 
Combining 
(24.1) 
and 
(24.2), 
we 
have 
(q, 
zy, 
A) 
=. 
(q, 
ucy, 
A) 
(q, 
y,ßa) 
= 
(q, 
y, 
'Y). 
Conversely, 
suppose 
(q, 
zy, 
A) 
(q, 
y, 
'Y), 
and 
let 
((q,c,B), 
(q,ß)) 
E 
0 
(24.1) 
(24.2) 
be 
the 
last 
transition 
taken. 
Then 
z 
= 
uc 
for 
some 
u 
E 
E*, 
'Y 
= 
ßa 
for 
some 
a 
E 
r*, 
and 
(q,ucy,A) 
(q,cy,Ba).2... 
(q,y,ßa). 
M 
M 
By 
the 
induction 
hypothesis, 
G 

_____________________________________________
PDAs 
and 
CFGs 
171 
via 
a 
leftmost 
derivation 
in 
G, 
and 
by 
construction 
of 
M, 
B 
--+ 
cß 
ia 
a 
production 
of 
G. 
Applying 
this 
production 
to 
the 
sentential 
form 
uBo:, 
we 
get 
A 
uBo: 
ucßo: 
= 
G G 
via 
a 
leftmost 
derivation. 
Theorem 
24.2 
L(M) 
= 
L(G). 
Proof· 
xE 
L(G) 
-<==> 
S 
x 
by 
a 
leftmost 
derivation 
definition 
of 
L( 
G) 
G 
-<==> 
(q,x,S) 
(q,•,•) 
Lemma 
24.1 
M 
o 
-<==> 
x 
E 
L(M) 
definition 
of 
L(M). 
0 

_____________________________________________
Lecture 
25 
Simulating 
NPDAs 
by 
CFGs 
We 
have 
shown 
that 
every 
CFL 
is 
accepted 
by 
some 
NPDA. 
Now 
we 
show 
conversely 
that 
NPDAs 
accept 
only 
_ 
CFLs. 
Thus 
NPDAs 
and 
CFGs 
are 
equivalent 
in 
expressive 
power. 
We 
will 
do 
this 
in 
two 
steps 
by 
showing 
that 
(i) 
every 
NPDA 
can 
be 
simulated 
by 
an 
NPDA 
with 
one 
state; 
and 
(ii) 
every 
NPDA 
with 
one 
state 
has 
an 
equivalent 
CFG. 
Actually, 
step 
(ii) 
is 
the 
easier 
of 
the 
two, 
since 
we 
have 
already 
done 
all 
the 
work 
in 
Lecture 
24. 
In 
that 
lecture, 
given 
a 
grammar 
in 
which 
all 
productions 
were 
of 
the 
form 
A 
---+ 
cB
1
B
2 
ŁŁŁ 
Bk 
for 
some 
k 
0 
and 
c 
E 
U 
{•}, 
we 
constructed 
an 
equivalent 
NPDA 
with 
one 
state. 
That 
construction 
is 
invertible. 
Suppose 
we 
have 
an 
NPDA 
with 
one 
state 
M 
= 
({q}, 
r, 
6, 
q, 
.1., 
0) 
that 
accepts 
by 
empty 
stack. 
Define 
the 
grammar 
G 
= 
(r, 
P, 
.1.), 

_____________________________________________
where 
P 
contains 
a 
production 
A 
-> 
cB
1
B
2 
ŁŁŁ 
Bk 
for 
every 
transition 
((q, 
c, 
A), 
(q, 
B
1
B
2
··· 
Bk)) 
E 
0, 
Simulating 
NPDAs 
by 
CFGs 
173 
where 
c 
E 
U 
{•}. 
Then 
Lemma 
24.1 
and 
Theorem 
24.2 
apply 
verbatim, 
thus 
L(G) 
= 
L(M). 
It 
remains 
to 
show 
how 
to 
simulate 
an 
arbitrary 
NPDA 
by 
an 
NPDA 
with 
one 
state. 
Essentially, 
we 
will 
maintain 
all 
state 
information 
on 
the 
stack. 
By 
the 
construction 
of 
Supplementary 
Lecture 
E, 
we 
can 
assume 
without 
loss 
of 
generality 
that 
M 
is 
of 
the 
form 
M 
= 
(Q, 
r, 
0, 
s, 
..L, 
{t}); 
that 
is, 
M 
has 
a 
single 
final 
state 
t, 
and 
M 
can 
empty 
its 
stack 
after 
it 
enters 
state 
t. 
Let 
r' 
Q 
x 
r 
x 
Q. 
Elements 
Qf 
r' 
are 
written 
(p 
A 
q), 
where 
p,q 
E 
Q 
and 
A 
E 
r. 
We 
will 
construct 
a 
new 
NPDA 
M' 
= 
:(", 
8', 
*, 
(s..Lt), 
0) 
with 
one 
state 
* 
that 
accepts 
by 
empty 
stack. 
The 
new 
machine 
M' 
will 
be 
able 
to 
scan 
astring 
x 
starting 
with 
only 
(p 
A 
q) 
on 
its 
stack 
and 
end 
up 
with 
an 
empty 
stack 
Hf 
M 
can 
scan 
x 
starting 
in 
state 
p 
with 
only 
A 
on 
its 
stack 
and 
end 
up 
in 
state 
q 
with 
an 
empty 
stack. 
The 
transition 
relation 
8' 
of 
M' 
is 
defined 
as 
folIows: 
for 
each 
transition 
((p, 
c, 
A), 
(qO, 
B
1
B
2
·Ł· 
Bk)) 
E 
0, 
w 
here 
c 
E 
U 
{•}, 
include 
in 
8' 
the 
transitions 
((*,c, 
(PAqk)), 
(*, 
(qO 
B
l 
ql)(ql 
B
2 
q2)··· 
(qk-l 
Bk 
qk))) 
for 
all 
possible 
choices 
of 
q1. 
q2, 
... 
, 
qk. 
For 
k 
= 
0, 
this 
reduces 
to: 
if 
((p,c,A), 
(qO,•)) 
E 
8, 
include 
in 
8' 
the 
tt:ansition 
((*,c,(pAqo}), 
(*,•)). 
Intuitively, 
M' 
simulates 
M, 
guessing 
nondeterministically 
what 
states 
M 
will 
be 
in 
at 
certain 
future 
points 
in 
the 
computation, 
saving 
those 
guesses 
on 
the 
stack, 
and 
then 
verifying 
later 
that 
those 
guesses 
were 
correct. 

_____________________________________________
174 
Lecture 
25 
The 
following 
lemma 
formalizes 
the 
intuitive 
relationship 
between 
tations 
of 
M 
and 
M'. 
Lemma 
25.1 
Let 
M' 
be 
the 
NPDA 
constructed 
from 
M 
as 
above. 
Then 
(p, 
x, 
B
l
B
2 
.. 
· 
Bk) 
7 
(q, 
•, 
•) 
if 
and 
only 
if 
there 
exist 
qo, 
ql, 
... 
,qk 
such 
that 
p 
= 
qo, 
q 
= 
qk, 
and 
(*, 
x, 
(qO 
BI 
ql}(ql 
B2 
q2) 
... 
(qk-l 
Bk 
qk}) 
(*, 
•, 
•). 
M 
In 
particular, 
(p, 
x, 
B) 
(q, 
•, 
•) 
{::::} 
(*, 
x, 
(p 
Bq}) 
h 
•, 
•). 
M 
M 
Proof. 
By 
induction 
on 
n 
(What 
else?). 
For 
n 
= 
0, 
both 
sides 
are 
equivalent 
to 
the 
assertion 
that 
p 
= 
q, 
x 
= 
•, 
and 
k 
= 
0. 
Now 
suppose 
that 
(p, 
x, 
B
l
B2'" 
Bk) 
(q, 
•, 
•). 
Let 
M 
((p,c,B
l
), 
(r,C
l
C
2
· 
.. 
C
m
)) 
be 
the 
first 
transition 
applied, 
where 
c 
E 
E U 
{•} 
and 
m 
2: 
0. 
Then 
x 
= 
GY 
and 
1 
(p, 
x, 
B
l
B2'" 
Bk) 
-;; 
(r, 
y, 
Cl 
C
2
··· 
C
m
B
2
··· 
Bk) 
(q,•,•). 
M 
By 
the 
induction 
hypothesis, 
there 
exist 
ro,Tl, 
... 
,Tm-l,ql, 
... 
,qk-l,qk 
such 
that 
r 
= 
ro, 
q 
= 
qk, 
and 
(*, 
y, 
(ro 
Cl 
Tl}(rl 
C
2 
r2) 
... 
(rm-l 
C
m 
ql}(ql 
B2 
q2) 
... 
(qk-l 
Bk 
qk}) 
i-t 
(*,•,•). 
Also, 
by 
construction 
of 
M', 
((*,c, 
(pBl 
ql}), 
(*, 
(ro 
Cl 
Tl}(Tl 
C
2 
r2}'" 
(Tm-l 
C
m 
ql})) 
is 
a 
transition 
of 
M'. 
Combining 
these, 
we 
get 
(*, 
x, 
(p 
BI 
ql}(ql 
B
2 
q2) 
... 
(qk-l 
Bk 
qk)) 
1 
]i/ 
(*, 
y, 
(TO 
Cl 
rl}(Tl 
C
2 
T2) 
... 
(rm-l 
C
m 
ql}(ql 
B
2 
q2) 
... 
(qk-l 
Bk 
qk)) 
Conversely, 
suppose 
(*, 
x, 
(qO 
BI 
ql)(ql 
B2 
q2) 
... 
(qk-l 
Bk 
qk)) 
(*, 
•, 
•). 
M 
Let 

_____________________________________________
Simulating 
NPDAs 
by 
CFGs 
175 
be 
the 
first 
transition 
applied, 
where 
c 
E 
L: 
U 
{•} 
and 
m 
O. 
Then 
x 
= 
cy 
and 
(*,x, 
(qO 
BI 
ql)(ql 
B
2 
q2)··· 
(qk-l 
Bk 
qk)) 
(*, 
y, 
(ro 
Cl 
rl)(rl 
C
2 
r2) 
... 
(rm-l 
C
m 
ql)(ql 
B
2 
q2) 
... 
(qk-l 
Bk 
qk)) 
M 
(*,•,•). 
By 
the 
induction 
hypothesis, 
(ro, 
y, 
Cl 
G2··· 
G
m
B
2··· 
Bk) 
(qk, 
•, 
•). 
M 
Also, 
by 
construction 
of 
M', 
((qo,c,BI), 
(ro,C
l
C
2
··· 
Gm)) 
is 
a 
transition 
of 
M. 
Combining 
these, 
we 
get 
1 
(qo,x,B
l
B
2
··· 
Bk) 
7 
(rO,y,G
l
G
2
··· 
G
m
B
2
ŁŁŁ 
Bk) 
(qk,•,•). 
M 
Theorem 
25.2 
L(M') 
= 
L(M). 
Proof. 
For 
all 
x 
E 
L:*, 
xE 
L(M') 
{=} 
(*,x, 
(s 
1. 
t)) 
-i:;: 
(*,•,•) 
{=} 
(s,x,1.) 
(t,•,•) 
M 
{=} 
x 
E 
L(M). 
Historical 
Notes 
Lemma 
25.1 
o 
o 
Pushdown 
automata 
were 
introduced 
by 
Oettinger 
[95J. 
The 
equivalence 
of 
PDAs 
and 
CFGs 
was 
established 
by 
Chomsky 
[19], 
Schützenberger 
[112], 
and 
Evey 
[361. 

_____________________________________________
Supplementary 
Lecture 
F 
Deterministic 
Pushdown 
Automata 
A 
deterministic 
pushdown 
automaton 
(DPDA) 
is 
an 
octuple 
M 
= 
(Q, 
r, 
8, 
..1, 
-i, 
s, 
F), 
where 
everything 
is 
the 
same 
as 
with 
NPDAs, 
except: 
(i) 
-i 
is 
a 
special 
symbol 
not 
in 
called 
the 
right 
endmarker, 
and 
8 
(ii) 
8 
is 
deterministic 
in 
the 
sense 
that 
exactly 
one 
transition 
applies 
in 
any 
given 
situation. 
This 
means 
that 
for 
any 
p 
E 
Q, 
a 
E 
U 
{-i}, 
and 
A 
E 
r, 
8 
contains 
exactly 
one 
transition 
of 
the 
form 
((p,a,A), 
(q,ß)) 
or 
((p,f,A), 
(q,ß)). 
(iii) 
8 
is 
restricted 
so 
that 
..1 
is 
always 
on 
the 
bottom 
of 
the 
stack. 
The 
machine 
may 
pop 
..1 
off 
momentarily, 
but 
must 
push 
it 
directly 
back 
on. 
In 
other 
words, 
all 
transitions 
involving 
..1 
must 
be 
of 
the 
form 
((p,a,..1), 
(q,ß..1)). 
The 
right 
endmarker 
-i 
delimits 
the 
input 
string 
and 
is 
a 
necessary 
addition. 
With 
NPDAs, 
we 
could 
guess 
where 
the 
end 
of 
the 
input 
string 
was, 
but 
with 
DPDAs 
we 
have 
no 
such 
luxury. 
The 
restriction 
in 
(iii) 
is 
so 
that 
the 
machine 
never 
deadlocks 
by 
emptying 
its 
stack. 
This 
assumption 
is 
without 
loss 
of 
generality; 
even 
if 
the 
machine 

_____________________________________________
Deterministic 
Pushdown 
Automata 
177 
did 
not 
obey 
it, 
we 
could 
make 
it 
do 
so 
by 
a 
construction 
involving 
a 
new 
stack 
symbol 
JL, 
as 
in 
Supplementary 
Lecture 
E. 
In 
that 
construction, 
we 
must 
modify 
the 
transitions 
(E.2) 
to 
read 
the 
symbol 
.... 
1. 
We 
conslder 
only 
acceptance 
by 
final 
state. 
One 
can 
define 
acceptance 
by 
empty 
stack 
and 
prove 
that 
such 
machines 
are 
equivalent. 
The 
assumption 
(iii) 
would 
have 
to 
be 
modified 
accordingly. 
The 
definitions 
of 
configurations 
and 
acceptance 
by 
-final 
state 
are 
the 
same 
as 
with 
NPDAs. 
The 
start 
configuration 
on 
input 
x 
E 
is 
(s, 
x 
-1, 
.1..), 
and 
x 
is 
accepted 
iff 
(s,x 
-1,.1) 
(q,f,ß) 
M 
for 
some 
q 
E 
Fand 
ß 
E 
r*. 
A 
language 
accepted 
by 
a 
DPDA 
is 
called 
a 
deterministic 
context-free 
guage 
(DCFL). 
Surely 
every 
DCFL 
is 
a 
CFL, 
since 
every 
DPDA 
can 
be 
simulated 
by 
an 
NPDA. 
In 
this 
lecture 
we 
will 
show 
that 
the 
family 
of 
DCFLs 
is 
closed 
under 
complement. 
We 
know 
that 
there 
exist 
CFLs 
whose 
complements 
are 
not 
CFLs; 
for'example, 
{a,b}* 
-
{ww 
I 
w 
E 
{a,b}*}. 
These 
CFLs 
cannot 
be 
DCFLs. 
Thus, 
unlike 
finite 
automata, 
ism 
in 
PDAs 
gives 
strictly 
more 
power. 
To 
show 
that 
the 
DCFLs 
are 
closed 
under 
complement, 
we 
will 
construct, 
given 
any 
DPDA 
M 
with 
input 
alphabet 
a 
new 
DPDA 
M' 
such 
that 
L(M
'
) 
= 
-
L(M). 
We 
would 
like 
to 
build 
M' 
to 
simulate 
M 
and 
accept 
iff 
M 
does 
not 
accept. 
Unfortunately, 
we 
cannot 
just 
switch 
accept 
and 
nonaccept 
states 
like 
we 
did 
with 
DFAs. 
The 
main 
difficulty 
here 
is 
that 
unlike 
finite 
automata, 
DPDAs 
need 
not 
scan 
aIl 
of 
their 
input; 
they 
may 
loop 
infinitelyon 
inputs 
they 
do 
not 
accept 
without 
reading 
thc 
entire 
input 
string. 
The 
machine 
M' 
will 
have 
to 
detect 
any 
such 
pathological 
behavior 
in 
M, 
since 
M' 
will 
have 
to 
scan 
the 
entire 
input 
and 
enter 
an 
accept 
state 
on 
aIl 
those 
inputs 
that 
are 
not 
accepted 
by 
lvI, 
including 
those 
inputs 
on 
which 
M 
loops 
prematurely. 
We 
solve 
this 
problem 
by 
showing 
how 
to 
modify 
M 
to 
detect 
such 
spurious 
looping 
and 
deal 
with 
it 
gracefully. 
After 
each 
modification 
step, 
we 
will 
argue 
that 
the 
resulting 
machine 
still 
accepts 
the 
same 
set 
as 
M 
and 
is 
still 
deterministic. 

_____________________________________________
178 
Supplementary 
Lecture 
F 
Checking 
for 
End 
of 
Input 
It 
will 
be 
useful 
to 
include 
one 
bit 
of 
information 
in 
the 
finite 
control 
of 
M 
to 
remember 
wh 
ether 
or 
not 
M 
has 
seen 
the 
endmarker 
-1 
yet. 
Formally, 
we 
duplicate 
the 
finite 
control 
Q 
to 
get 
a 
new 
copy 
Q' 
= 
{q' 
I 
q 
E 
Q}.disjoint 
from 
Q 
and add 
a 
new 
transition 
((p',a,A), 
(q',ß)) 
for 
each 
transition 
((p,a,A), 
(q,ß)) 
E 
fJ. 
We 
remove 
any 
transition 
of 
the 
form 
((p,-1,A), 
(q,ß)) 
and 
replace 
it 
with 
((p,-1,A), 
(q',ß)). 
The 
primed 
states 
thus 
behave 
exactly 
like 
the 
unprimed 
original 
states, 
except 
that 
we 
jump 
from 
an 
unprimed 
state 
to 
a 
primed 
state 
when 
we 
scan 
the 
endmarker. 
The 
start 
state 
will 
still 
be 
s, 
but 
we 
will 
take 
as 
final 
states 
all 
primed 
states 
corresponding 
to 
final 
states 
in 
the 
old 
machi:te 
M; 
that 
is, 
the 
new 
set 
of 
final 
states 
will 
be 
F' 
= 
{q' 
I 
q 
E 
F}. 
The 
new 
machine 
is 
still 
deterministic, 
since 
there 
is 
still 
exactly 
one 
sition 
that 
applies 
in 
any 
configuration. 
1t 
accepts 
the 
same 
set, 
since 
if 
(S,x 
-1,.1) 
(q,f,,), 
q 
E 
F 
M 
in 
the 
old 
machine, 
then 
there 
must 
be 
some 
intermediate 
transition 
that 
reads 
the 
-1: 
(s,x 
-1,1..) 
(p, 
-I,Aß) 
(r,E,aß) 
(q,E,,). 
M 
M 
M 
Then 
in 
thc 
new 
machine, 
(S, 
x 
-1,.1) 
--i;; 
(p, 
-1, 
Aß) 
-/i: 
(r', 
f, 
aß) 
--i;; 
(q', 
f, 
,). 
Conversely, 
if 
(s, 
x 
-1,.1) 
-i? 
(q', 
f, 
,), 
q' 
E 
fi" 
in 
the 
new 
machine, 
then 
removing 
the 
prim 
es 
gives 
a 
valid 
accepting 
computation 
sequence 
(s,x 
-1,.1) 
(q,f,,), 
q 
E 
F 
M 
in 
the 
old 
machine. 
Now 
we 
can 
tell 
from 
information 
in 
the 
state 
whether 
we 
have 
seen 
-1 
or 
not. 
vVith 
this 
information, 
we 
ran 
make 
the 
machine 
remain 
in 
an 
accept 
state 
if 
it 
has 
already 
scanned 
the 
-1 
and 
accepted. 
This 
is 
done 
by 
deleting 
every 
primed 
transition 
((p',f,A), 
(q',ß)) 
with 
p' 
E 
F' 
and 
replacing 
it 
with 
((p',E, 
A), 
(p',A)). 

_____________________________________________
DetHministic 
Pushdown 
Automata 
179 
Getting 
Rid 
of 
Spurious 
Loops 
We 
include 
two 
new 
nonaccept 
states 
rand 
r
' 
and 
transitions 
((r,a,A), 
(r,A)), 
((r,-1,A), 
(r/,A)), 
((r/,E,A), 
(r/,A)), 
a 
EI;, 
A 
E 
r, 
A 
E 
r, 
A 
E 
r. 
We 
can 
think 
of 
r
' 
as 
a 
reject 
state. 
If 
the 
machine 
is 
in 
state 
r, 
it 
will 
always 
scan 
to 
the 
end 
of 
the 
input 
and 
enter 
state 
r
/
, 
leaving 
the 
stack 
intact. 
Once 
the 
machine 
is 
in 
state 
r
/
, 
it 
stays 
there. 
(So 
far 
there 
is 
no 
way 
for 
the 
machine 
to 
reach 
state 
r 
or 
r/, 
but 
just 
wait 
... 
) 
We 
will 
now 
show 
how 
to 
modify 
the 
machine 
so 
that 
für 
all 
inputs, 
the 
machine 
scans 
the 
entire 
input 
and 
enters 
either 
an 
accept 
state 
or 
the 
state 
r'. 
Let 
x 
be 
any 
input. 
Because 
of 
determinism, 
there 
is 
a 
unique 
infinite 
sequence 
of 
configurations 
the 
machine 
goes 
through 
on 
input 
X. 
Let 
li 
denote 
the 
stack 
contents 
at 
time 
i 
E 
{O, 
1,2, 
... 
}. 
There 
exist.s 
an 
infinite 
sequence 
of 
tim 
es 
io 
< 
i
1 
< 
i2 
< 
... 
such 
that 
for 
all 
ik, 
(F.I) 
We 
can 
take 
io 
= 
0, 
since 
10 
= 
..L 
and 
1,0 
I 
= 
1, 
and 
the 
machine 
never 
empties 
its 
stack. 
Proceeding 
inductively, 
we 
can 
take 
ik+l 
to 
be 
the 
earliest 
time 
after 
ik 
such 
that 
l'iHll 
is 
minimum 
among 
all 
lid, 
i 
> 
ik. 
Now 
we 
pick 
an 
infinite 
subsequence 
jo 
< 
h 
< 
h 
< 
... 
of 
io 
< 
i
1 
< 
i
2 
< 
... 
such 
that 
the 
same 
transition, 
say 
((p, 
E, 
A), 
(q, 
ß)), 
is 
applied 
at 
times 
jo,jl,h, 
.... 
Such 
a 
subsequence 
exists 
by 
the 
pigeonhole 
principle: 
there 
are 
only 
finitely 
many 
transitions 
in 
8, 
so 
at 
least 
one 
must 
be 
applied 
infinitely 
often. 
The 
states 
p, 
q 
can 
be 
primed 
or 
unprimed. 
The 
transition 
must 
be 
an 
E-transition, 
sinee 
it 
is 
applied 
infinitely 
often, 
and 
there 
are 
only 
finitely 
many 
input 
symbols 
to 
sean. 
By 
(F.I), 
the 
machine 
never 
sees 
any 
stack 
symbol 
below 
the 
top 
symbol 
of 
Ij. 
after 
time 
jk, 
and 
the 
top 
symbol 
is 
A. 
Thus 
the 
only 
stack 
symbols 
it 
sees 
after 
time 
jk 
are 
those 
it 
pushes 
after 
time 
jk. 
Since 
the 
machine 
is 
deterministic, 
once 
it 
applies 
transition 
((p, 
E, 
A), 
(q, 
ß)), 
it 
is 
in 
a 
loop 
and 
will 
go 
through 
the 
same 
periodic 
sequence 
of 
E-transitions 
repeated 
forever, 
since 
it 
sees 
nothing 
that 
can 
force 
it 
to 
do 
anything 
different. 
Moreover, 
this 
behavior 
is 
independent 
of 
the 
input. 
Thus 
if 
p 
is 
not 
an 
aceept 
state, 
then 
the 
input 
is 
not 
accepted. 
We 
might 
as 
weIl 
remove 
the 
transition 
((p,E,A), 
(q,ß)) 
from 
8 
and 
replace 
it 
with 
the 
transition 
((p,E,A), 
(r,A)) 
if 
pis 
an 
unprimed 
state 
or 
((p,E,A), 
(r',A)) 
if 
pis 
a 
primed 
state. 
The 
language 
accepted 
by 
the 
automaton 
is 
not 
changed. 
If 
this 
is 
done 
for 
all 
transitions 
((p,f,A), 
(q,ß)) 
causing 
such 
spurious 
loops, 
we 
obtain 
a 
machine 
equivalent 
to 
M 
that 
on 
any 
input 
scans 
the 

_____________________________________________
180 
Supplementary 
Lecture 
F 
entire 
input 
string 
and 
the 
endmarker 
., 
and 
enters 
either 
an 
accept 
state 
or 
the 
state 
r
'
. 
To 
get 
a 
machine 
M' 
accepting 
the 
complement 
of 
L(M), 
make 
r
' 
the 
unique 
accept 
state 
of 
M'. 
Historical 
Notes 
Deterministic 
PDAs 
were 
first 
studied 
by 
Fischer 
[37], 
Schützenberger 
[112], 
Haines 
[54], 
and 
Ginsburg 
and 
Greibach 
[44]. 

_____________________________________________
Lecture 
26 
Parsing 
One 
of 
the 
most 
important 
applications 
of 
context-free 
languages 
and 
down 
automata 
is 
in 
compilers. 
The 
input 
to 
a 
PASCAL 
compiler 
is 
a 
PASCAL 
program, 
but 
it 
is 
presented 
to 
the 
compiler 
as 
astring 
of 
ASCII 
characters. 
Before 
it 
can 
do 
anything 
else, 
the 
compiler 
has 
to 
scan 
this 
string 
and 
determine 
the 
syntactic 
structure 
of 
the 
program. 
This 
process 
is 
called 
parsing. 
The 
syntax 
of 
the 
programming 
language 
(or 
at 
least 
big 
parts 
of 
it) 
is 
often 
specified 
in 
terms 
of 
a 
context-free 
grammar. 
The 
process 
of 
parsing 
isessentially 
determining 
a 
parse 
tree 
or 
derivation 
tree 
of 
the 
program 
in 
that 
grammar. 
This 
tree 
provides 
the 
structure 
the 
compiler 
needs 
to 
know 
in 
order 
to 
generate 
code. 
The 
subroutine 
of 
the 
compiler 
that 
parses 
the 
input 
is 
called 
the 
parser. 
Many 
parsers 
use 
a 
single 
stack 
and 
resemble 
deterministic 
PDAs. 
By 
now 
the 
theory 
of 
deterministic 
PDAs 
and 
parsing 
is 
so 
weil 
developed 
that 
in 
many 
instances 
a 
parser 
for 
a 
given 
grammar 
can 
be 
generated 
automatically. 
This 
technology 
is 
used 
in 
what 
we 
call 
compiler 
compilers. 
Example 
26.1 
Consider 
well-parenthesized 
expressions 
of 
propositional 
logic. 
There 
are 
propositional 
variables 
P, 
Q, 
R, 
... 
, 
constants 
.1., 
T 
(for 
false 
and 
true, 
respectively), 
binary 
operators 
/\ 
(and), 
V 
(or), 
--> 
(implication 
or 
if-then), 
and 
..... 
(if 
and 
only 
if 
or 
biconditional), 
and 
unary 
operator..., 
(not), 
as 
weil 
as 
parentheses. 
The 
following 
grammar 
generates 
the 
well-parenthesized 

_____________________________________________
182 
Lecture 
26 
propositional 
expressions 
(we've 
used 
=> 
instead 
of 
the 
usual 
-+ 
for 
pro 
tions 
to 
avoid 
confusion 
with 
the 
propositional 
implication 
operator): 
E 
=> 
(EBE) 
I 
(UE) 
I 
C 
I 
V, 
B 
=> 
V 
1/\ 
I 
-+ 
I 
...... 
,. 
U 
=>..." 
C 
=> 
.11 
T, 
V=>PIQIRI 
.. 
· 
(26.1 
) 
The 
words 
"well-parenthesized" 
mean 
that 
there 
must 
be 
parentheses 
around 
any 
compound 
expression. 
(We'll 
show 
how 
to 
get 
rid 
of 
them 
later 
using 
erator 
precedence.) 
The 
presence 
of 
the 
parentheses 
ensures 
that 
the 
mar 
is 
unambiguous-that 
is, 
each 
expression 
in 
the 
language 
has 
a 
unique 
parse 
tree, 
so 
that 
there 
is 
one 
and 
only 
one 
way 
to 
parse 
the 
expression. 
A 
typical 
expression 
in 
this 
language 
is 
(((PV 
Q) 
/\ 
R) 
V 
(Q 
/\ 
(...,P))). 
(26.2) 
Each 
expression 
represents 
an 
expression 
tree 
that 
gives 
the 
order 
of 
uation. 
The 
expression 
tree 
corresponding 
to 
the 
propositional 
expression 
(26.2) 
is 
/\ 
/\ 
V 
R 
/\ 
P 
Q 
,\ 
/\ 
Q 
T 
P 
In 
order 
to 
generate 
code 
to 
evaluate 
the 
expression, 
the 
compiler 
needs 
to 
know 
this 
expression 
tree. 
The 
expression 
tree 
and 
the 
unique 
parse 
tree 
for 
the 
expression 
in 
the 
grammar 
(26.1) 
above 
contain 
the 
same 
information; 
in 
fact, 
the 
expression 
tree 
can 
be 
read 
off 
immediately 
from 
the 
parse 
tree. 
Thus 
parsing 
is 
essentially 
equivalent 
to 
prod 
ucing 
the 
expression 
tree, 
which 
can 
then 
be 
used 
to 
generate 
code 
to 
evaluate 
the 
expression. 
Here's 
an 
example 
of 
a 
parser 
for 
propositional 
expressions 
that 
pro 
duces 
the 
expression 
tree 
directly. 
This 
is 
a 
typical 
parser 
you 
might 
see 
in 
areal 
compiler. 
Start 
with 
only 
the 
initial 
stack 
symbol 
.1 
on 
the 
stack. 
Scan 
the 
expression 
from 
left 
to 
right, 
performing 
one 
of 
the 
following 
actions 
depending 
on 
each 
symbol: 
(i) 
If 
the 
symbol 
is 
a 
(, 
push 
it 
onto 
the 
stack. 

_____________________________________________
Parsing 
183 
(ii) 
If 
the 
symbol 
is 
an 
operator, 
either 
unary 
or 
binary, 
push 
it 
onto 
the 
stack. 
(iü) 
If 
the 
symbol 
is 
a 
constant, 
push 
apointer 
to 
it 
onto 
the 
stack. 
(iv) 
If 
the 
symbol 
is 
a 
variable, 
look 
it 
up 
in 
the 
symbol 
table 
and 
push 
a 
pointer 
to 
the 
symbol 
tabfe 
entry 
onto 
the 
stack. 
The 
symbol 
table 
is 
a 
dictionary 
containing 
the 
name 
of 
every 
variable 
used 
in 
the 
program 
and 
apointer 
to 
a 
memory 
location 
where 
its 
value 
will 
be 
stored. 
If 
the 
variable 
has 
never 
been 
seen 
before, 
a 
new 
symbol 
table 
entry 
is 
created. 
(v) 
If 
the 
symbol 
is 
a 
), 
do 
a 
reduce. 
This 
is 
where 
all 
the 
action 
takes 
place. 
A 
reduce 
step 
consists 
of 
the 
following 
sequence 
of 
actions: 
(a) 
Allocate 
a 
block 
of 
storage 
for 
a 
new 
noM 
in 
the 
expression 
tree. 
The 
block 
has 
space 
for 
the 
name 
of 
an 
operator 
and 
pointers 
to 
left 
and 
right 
operands. 
(b) 
Pop 
the 
top 
object 
off 
the 
stack. 
It 
had 
better 
be 
apointer 
to 
an 
operand 
(either 
a 
constant, 
variable, 
or 
node 
in 
the 
expression 
tree 
created 
previously). 
If 
not, 
give 
a 
syntax 
error. 
If 
so, 
save 
the 
pointer 
in 
the 
newly 
allocated 
node 
as 
the 
right 
operand. 
(c) 
Pop 
the 
top 
object 
off 
the 
stack. 
It 
had 
better 
be 
an 
operator. 
If 
not, 
give 
a 
syntax 
error. 
If 
so, 
save 
the 
operator 
name 
in 
the 
newly 
allocated 
node. 
If 
the 
operator 
is 
unary, 
skip 
the 
next 
step 
(d) 
and 
go 
directly 
to 
(e). 
(d) 
Pop 
the 
top 
object 
off 
the 
stack. 
It 
had 
better 
be 
apointer 
to 
an 
operand. 
If 
not, 
give 
a 
syntax 
error. 
If 
50,save 
the 
pointer 
in 
the 
newly; 
allocated 
node 
as 
the 
left 
operand. 
(e) 
Pop 
the 
top 
object 
off 
the 
stack. 
It 
had 
better 
be 
a 
(. 
Thil': 
is 
the 
left 
parenthesis 
matching 
the 
right 
parenthesis 
we 
just 
scanned. 
If 
not, 
give 
a 
syntax 
error. 
(f) 
Push 
apointer 
to 
the 
newly 
allocated 
node 
onto 
the 
stack. 
Let's 
illustrate 
this 
algorithm 
on 
the 
input 
string 
(((PV 
Q) 
1\ 
R) 
V 
(Q 
1\ 
(-.,P))). 
We 
start 
with 
the 
stack 
containing 
only 
an 
initial 
stack 
symbol 
.L 
We 
scan 
the 
first 
three 
('s 
and 
push 
them 
according 
to 
(i). 
We 
scan 
P 
and 
push 
a 
pointer 
to 
its 
symbol 
table 
entry 
according 
to 
(iv). 
We 
push 
the 
operator 
V 
according 
to 
(ii), 
then 
push 
apointer 
to 
Q 
according 
to 
(iv). 
At 
this 
point 
the 
stack 
looks 
like 

_____________________________________________
184 
Lecture 
26 
Q 
v 
P 
( 
( 
( 
1. 
We 
now 
scan 
the 
) 
and 
do 
a 
reduce 
step 
(v). 
We 
allocate 
a 
block 
of 
storage 
for 
a 
new 
node 
in 
the 
expression 
tree, 
pop 
the 
operands 
and 
operator 
on 
top 
of 
the 
stack 
and 
save 
them 
in 
the 
node, 
pop 
the 
matching 
(, 
and 
push 
apointer 
to 
the 
new 
node. 
Now 
we 
have 
( 
( 
1. 
v 
/\ 
Q 
P 
on 
the 
stack, 
and 
we 
are 
scanning 
(((P 
V 
Q) 
1\ 
R) 
V 
(Q 
1\ 
(-,P))) 
i 
We 
push 
the 
1\ 
and 
apointer 
to 
R. 
At 
that 
point 
we 
have 
1\ 
( 
( 
1. 
R 
V 
/\ 
Q 
P 
and 
we 
scan 
the 
next 
), 
so 
we 
reduce, 
giving 
1\ 
( 
V 
/\ 
R 
1. 
/ \ 
P 
Q 

_____________________________________________
and 
we 
are 
left 
scanning 
«(P 
V 
Q) 
1\ 
R) 
V 
(Q 
1\ 
(-,P))) 
T 
Parsing 
185 
Now 
we 
scan 
and 
push 
everything 
up 
to 
the 
next 
). 
This 
gives 
P 
-, 
( 
1\ 
Q 
( 
V 
1\ 
( 
V 
/\ 
R 
.L 
/ 
\ 
P 
Q 
We 
scan 
the 
first 
of 
the 
final 
three 
)'s 
and 
reduce. 
This 
gives 
-, 
1\ 
I 
Q 
P 
( 
V 
1\ 
( 
/ 
V 
\ 
R 
.L 
/\ 
P 
Q 
We 
scan 
the 
next 
) 
and 
reduce, 
giving 

_____________________________________________
186 
Lecture 
26 
/\ 
v 
/\ 
Q 
/\ 
( 
/\ 
v 
R 
..., 
I 
P 
-1 
/\ 
P 
Q 
Finally, 
we 
scan 
the 
last) 
and 
reduce, 
giving 
When 
we 
come 
to 
the 
end 
of 
the 
expression, 
we 
are 
left 
with 
nothing 
on 
the 
stack 
but 
apointer 
to 
the 
entire 
expression 
tree 
and 
the 
initial 
stack 
symbol. 
0 
Operator 
Precedence 
The 
grammar 
of 
the 
preceding 
example 
is 
unambiguous 
in 
the 
sense 
that 
there 
is 
one 
and 
only 
one 
parse 
tree 
(and 
hence 
only 
one 
possible 
expression 
tree) 
for 
every 
expression 
in 
the 
language. 
If 
we 
don 
't 
want 
to 
write 
all 
those 
parentheses, 
we 
can 
change 
the 
language 
to 
allow 
us 
to 
omit 
them 
if 
we 
like: 
E:::} 
EBE 
I 
U 
Eie 
I 
V 
I 
(E), 
B 
:::} 
V 
1 
/\ 
1 
-+ 
1 
.... 
, 
U 
:::} 
..." 
C=}o 
11, 
V:::}PIQIRI 
.. 
· 
But 
the 
problem 
with 
this 
is 
that 
the 
grammar 
is 
now 
ambiguous. 
For 
example, 
there 
are 
two 
possible 
trees 
corresponding 
to 
the 
expression 
P 
V 
Q 
/\ 
R, 
namely 

_____________________________________________
Parsing 
187 
1\ 
V 
/\ 
/\ 
V 
R 
p 
1\ 
/\ 
/\ 
p 
Q 
Q 
R 
with 
very 
different 
semantics. 
We 
need 
a 
way 
of 
resolving 
the 
ambiguity. 
This 
is 
often 
done 
by 
giving 
a 
precedence 
relation 
on 
operators 
that 
specifies 
which 
operators 
are 
to 
be 
evaluated 
first 
in 
case 
of 
ambiguity. 
The 
dence 
relation 
is 
just 
a 
partial 
order 
on 
the 
operators. 
To 
resolve 
ambiguity 
among 
operators 
of 
equal 
precedence, 
we 
will 
perform 
the 
operations 
from 
left 
to 
right. 
(The 
left-to-right 
convention 
corresponds 
to 
common 
informal 
usage, 
but 
some 
programming 
languages, 
such 
as 
APL, 
use 
the 
opposite 
convention.) 
Under 
these 
conventions, 
we 
only 
need 
parentheses 
when 
we 
want 
to 
depart 
from 
the 
clefault 
parse 
tree. 
For 
example, 
consider 
the 
following 
grammar 
for 
well-formed 
arithmetic 
expressions 
over 
constants 
0,1, 
variables 
a, 
b, 
c, 
and 
operator 
symbols 
+ 
(addition), 
binary 
-
(subtraction), 
unary 
-
(negation), 
. 
(multiplication), 
and 
/ 
(division): 
E 
-
E 
+ 
EIE 
-
EIE. 
EIE 
/ 
E 
I 
-EI 
C 
I 
V 
I 
(E), 
C -
011, 
V 
_. 
alb 
I 
c. 
(26.3) 
This 
grammaris 
ambiguous. 
For 
example, 
there 
are 
five 
different 
parse 
trees 
for 
the 
expression 
a+b'c+d 
(26.4) 
corresponding 
to 
the 
five 
expression 
trees 
+ + 
+/""+ 
+ + 
/\ 
/\ 
/\ /\ 
d 
+ 
d 
a 
+ 
a 
/\ 
/\ 
/\ 
/\ 
/\ 
/\ 
+ 
c 
a 
a 
b 
c 
d 
d b 
+ 
/\ 
/\ 
/\ 
/\ 
a 
b 
b 
c 
b 
c c 
d 
The 
usual 
precedence 
relation 
on 
the 
arithmetic 
operators 
gives 
unary 
nus 
highest 
precedence, 
followed 
by 
. 
and 
/, 
which 
have 
equal 
precedence, 
followed 
by 
+ 
and 
binary 
-, 
which 
have 
equal 
and 
lowest 
precedence. 
Thus 
for 
the 
arithmetic 
expression 
(26.4), 
the 
preferred 
expression 
tree 
is 
the 
second 
from 
the 
left. 
This 
is 
because 
the 
. 
wants 
to 
be 
performed 
before 
either 
of 
the 
+'s, 
and 
between 
the 
+'s 
the 
leftmost 
wants 
to 
be 
performed 
first. 
If 
we 
want 
the 
expression 
evaluated 
differently, 
say 
according 
to 
the 

_____________________________________________
188 
Lecture 
26 
middle 
expression 
tree, 
then 
we 
need 
to 
use 
parentheses: 
(a+b)·(c+d). 
The 
operators 
+ 
and 
binary 
-
have 
equal 
precedence, 
since 
common 
usage 
would 
evaluate 
both 
a 
-
b 
+ 
c 
and 
a 
+ 
b 
-c 
from 
left 
to 
right. 
One 
can 
modify 
the 
grammar 
(26.3) 
so 
as 
to 
obtain 
an 
equivalent 
biguous 
grammar. 
The 
same 
set 
of 
strings 
is 
generated, 
but 
the 
precedence 
of 
the 
operators 
is 
accounted 
for. 
In 
the 
modified 
grammar, 
each 
generated 
string 
again 
has 
a 
unique 
parse 
tree, 
and 
this 
parse 
tree 
correctly 
reflects 
the 
precedence 
of 
the 
operators. 
Such 
a 
grammar 
equivalent 
to 
(26.3) 
would 
be 
E 
-+ 
E 
+ 
F 
1 
E 
-
F 
1 
F, 
F 
-+ 
F· 
GI 
FjG 
1 
G, 
G 
-+ 
-G 
1 
H, 
H 
-+ 
C 
1 
V 
1 
(E), 
C 
-+ 
011, 
V 
--+ 
alb 
1 
C. 
Given 
a 
precedence 
relation 
on 
the 
operators, 
the 
parsing 
algorithm 
above 
can 
be 
modified 
to 
handle 
that 
are 
not 
fully 
parenthesized 
as 
follows. 
Whenever 
we 
are 
about 
to 
scan 
a 
binary 
operator 
B, 
we 
check 
to 
make 
sure 
there 
ia 
an 
operand 
on 
top 
of 
the 
stack, 
then 
we 
look 
at 
the 
stack 
symbol 
A 
immediately 
below 
it. 
If 
A 
is 
a 
symbol 
of 
lower 
precedence 
than 
B 
(and 
for 
this 
purpose 
the 
left 
parenthesis 
and 
the 
initial 
stack 
symbol 
have 
lower 
precedence 
than 
any 
operator), 
we 
push 
B. 
If 
Ais 
a 
symbol 
of 
high 
er 
or 
equal 
precedence, 
then 
we 
reduce 
and 
repeat 
the 
process. 
Let's 
illustrate 
with 
the 
expression 
a+b·c+d. 
We 
start 
with 
the 
stack 
containing 
only 
1.. 
We 
scan 
the 
variable 
a 
and 
push 
apointer 
to 
it. 
1 
... 
-+1-----
a 
At 
this 
point 
we 
are 
about 
to 
scan 
the 
+ 
(we 
don't 
actually 
scan 
past 
it 
yet, 
we 
just 
look 
at 
it). 
We 
check 
under 
the 
operand 
on 
top 
of 
the 
stack 
and 
see 
the 
initial 
stack 
symbol 
1., 
which 
has 
lower 
precedence 
than 
+, 
SO 
we 
scan 
and 
push 
the 
+. 
We 
then 
scan 
and 
push 
the 
b, 
giving 

_____________________________________________
Parsing 
189 
b 
+ 
a 
J.. 
At 
this 
point 
we 
are 
ab 
out 
to 
scan 
the 
'. 
We 
check 
under 
the 
operand 
on 
top 
of 
the 
stack 
and 
see 
the 
+, 
which 
has 
lower 
precedence 
than 
" 
so 
we 
scan 
and 
push 
the 
'. 
We 
then 
scan 
and 
push 
the 
c, 
giving 
C 
b 
+ 
a 
J.. 
Now 
we 
are 
about 
to 
scan 
the 
second 
+ 
(we 
don't 
actually 
scan 
past 
it 
yet). 
We 
check 
under 
the 
operand 
on 
top 
of 
the 
stack 
and 
see 
the 
" 
which 
has 
higher 
precedence 
than 
+, 
so 
we 
reduce. 
This 
gives 
+ 
1\ 
b 
C 
a 
1. 
In 
this 
reduce 
step, 
we 
don't 
try 
to 
pop 
the 
(. 
Good 
thing; 
since 
it's 
not 
there. 
Left 
parentheses 
are 
popped 
only 
when 
they 
are 
there 
and 
when 
the 
symbol 
ab 
out 
to 
be 
scanned 
is 
a 
right 
parenthesis. 
Now 
we 
repeat 
the 
process. 
We 
still 
haven't 
scanned 
the 
second 
+, 
so 
we 
ask 
again 
whether 
the 
symbol 
immediately 
below 
the 
operand 
on 
top 
of 
the 
stack 
is 
of 
high 
er 
or 
equal 
precedence. 
In 
this 
case 
it 
is 
the 
+, 
which 
is 
of 
equal 
precedence, 
so 
we 
reduce. 
1 
L 
... 
Ł 
+1--
.... 
:/+\ 
1\ 
b 
C 

_____________________________________________
190 
Lecture 
26 
We 
are 
stilllooking 
at 
the 
+ 
in 
the 
input 
string. 
Now 
we 
check 
again 
below 
the 
operand 
on 
top 
of 
the 
stack 
and 
find 
.L, 
which 
is 
of 
lower 
precedence, 
so 
we 
scan 
and 
push 
the 
+, 
then 
sc 
an 
and 
push 
the 
d. 
d 
+ 
+ 
.L 
/\ 
a 
/\ 
b 
c 
At 
this 
point 
we 
come 
to 
the 
end 
of 
the 
expression. 
We 
now 
reduce 
until 
no 
furt 
her 
reduce 
steps 
are 
possible. 
We 
are 
left 
with 
nothing 
on 
the 
stack 
but 
apointer 
to 
the 
desired 
expression 
tree 
and 
the 
initial 
stack 
symbol. 
+ 
/\ 
+ 
d 
/\ 
a 
/\ 
b 
c 
Historical 
Notes 
An 
early 
paper 
on 
parsing 
is 
Knuth 
[71]. 
The 
theory 
is 
by 
now 
quite 
weH 
developed, 
and 
we 
have 
only 
scratched 
the 
surface 
here. 
Good 
introductory 
texts 
are 
Aho 
and 
Ullman 
[2, 
3, 
4] 
and 
Lewis, 
Rosenkrantz, 
and 
Stearns 
[80]. 

_____________________________________________
Lecture 
27 
The 
Cocke-Kasami-Younger 
Aigorithm 
Given 
a 
CFL 
A 
and 
astring 
x 
E 
how 
do 
we 
tell 
whether 
x 
is 
in 
A? 
If 
A 
is 
a 
deterministic 
CFL 
and 
we 
are 
given 
a 
deterministic 
PDA 
for 
it, 
we 
can 
easily 
write 
a 
pro 
gram 
to 
simulate 
the 
PDA. 
In 
fact, 
this 
is 
a 
good 
way 
to 
do 
it 
in 
practice 
and 
is 
done 
frequently 
in 
compilers; 
we 
saw 
an 
example 
of 
this 
in 
Lecture 
26. 
What 
if 
A 
is 
not 
a 
deterministic 
CFL, 
or 
even 
if 
it 
is 
but 
we 
are 
not 
given 
a 
deterministic 
PDA 
for 
it? 
If 
A 
is 
given 
by 
a 
CFG 
G, 
we 
can 
first 
convert 
G 
to 
Chomsky 
normal 
form 
(A 
-+ 
Be 
or 
A 
-+ 
a) 
so 
that 
each 
production 
either 
pro 
duces 
a 
terminal 
or 
increases 
the 
length 
of 
the 
sentential 
form, 
then 
try 
all 
derivations 
of 
length 
21xl-
1 
to 
see 
if 
any 
of 
them 
produce 
x. 
nately, 
there 
might 
be 
exponentially 
many 
such 
derivations, 
so 
this 
is 
rather 
inefficient. 
Alternatively, 
we 
might 
produce 
an 
equivalent 
NPDA 
and 
try 
all 
computation 
sequences, 
but 
again 
because 
of 
the 
nondeterminism 
there 
may 
be 
exponentially 
many 
to 
try. 
Here 
is 
a 
cubic-time 
algorithm 
due 
to 
Cocke, 
Kasami, 
and 
Younger 
[65, 
125]. 
It 
is 
an 
example 
of 
the 
technique 
of 
dynamic 
programming, 
a 
very 
useful 
technique 
in 
the 
design 
of 
eflicient 
algorithms. 
It 
determines 
for 
each 
substring 
y 
of 
x 
the 
set 
of 
all 
nonterminals 
that 
generate 
y. 
This 
is 
done 
inductively 
on 
the 
length 
of 
y. 

_____________________________________________
192 
Lecture 
27 
For 
simplicity, 
we 
will 
assume 
that 
the 
given 
grammar 
G 
is 
in 
Chomsky 
normal 
form. 
One 
can 
give 
a 
more 
general 
version 
of 
the 
algorithm 
that 
works 
for 
grammars 
not 
in 
this 
form. 
We 
illustrate 
the 
algorithm 
with 
an 
exampie. 
Consider 
the 
following 
mar 
for 
the 
set 
of 
all 
non 
null 
strings 
with 
equally 
many 
a's 
and 
b's: 
S 
-
AB 
I 
BA 
I 
SS 
I 
AC 
I 
BD, 
A-a, 
B-b, 
C-SB, 
D-SA. 
We'll 
run 
the 
algorithm 
on 
the 
input 
string 
x 
= 
aabbab. 
Let 
n 
be 
the 
length 
of 
the 
string 
(here 
n 
= 
6). 
Draw 
n 
+ 
1 
verticallines 
separating 
the 
letters 
of 
x 
and 
number 
them 
0 
to 
n: 
lalalblblalbl 
0123456 
For 
0 
i 
< 
j 
n, 
let 
Xij 
denote 
the 
substring 
of 
x 
between 
lines 
i 
and 
j. 
In 
this 
exampIe, 
Xl,4 
= 
abb 
and 
X2,6 
= 
bbab. 
The 
wh 
oie 
string 
x 
is 
XO,n' 
Build 
a 
table 
T 
with 
(2) 
entries, 
one 
for 
each 
pair 
i,j 
such 
that 
0 
i 
< 
j 
n. 
o 
1 
2 
3 
4 
5 
6 
The 
i,jth 
entry 
of 
T, 
denoted 
Tij, 
refers 
to 
the 
substring 
Xij' 
We 
will 
fill 
in 
each 
entry 
Tij 
of 
T 
with 
the 
set 
of 
nonterminals 
of 
G 
that 
erate 
the 
substring 
Xij 
of 
x. 
This 
information 
will 
be 
produced 
inductively, 
shorter 
substrings 
first. 
We 
start 
with 
the 
substrings 
of 
Iength 
one. 
These 
are 
the 
substrings 
of 
x 
of 
the 
form 
Xi,i+l 
for 
0 
i 
n 
-
1 
and 
correspond 
to 
the 
table 
entries 
along 
the 
top 
diagonal. 
For 
each 
such 
substring 
c 
= 
Xi,i+l, 
if 
there 
is 
a 
production 
X 
-
c E 
G, 
we 
write 
the 
nonterminal 
X 
in 
the 
table 
at 
Iocation 
i, 
i 
+ 
1. 
o 
A 
1 
A 
2 
B 
3 
B 
4 
A 
5 
B 6 

_____________________________________________
The 
Cocke-Kasami-Younger 
Aigorithm 
193 
In 
this 
example, 
B 
is 
written 
in 
T
3
,4 
because 
X3,4 
= 
band 
B 
-+ 
b 
is 
a 
production 
of 
G. 
In 
general, 
Ti,i+! 
may 
contain 
several 
nonterminals, 
because 
there 
may 
be 
several 
different 
productions 
with 
c 
= 
Xi,i+! 
on 
the 
right-hand 
side. 
We 
write 
them 
all 
in 
the 
table 
at 
position 
Ti,i+!. 
Now 
we 
proceed 
to 
the 
substrings 
of 
length 
two. 
These 
correspond 
to 
the 
diagonal 
in 
T 
immediately 
belbw 
the 
top 
diagonal 
we 
just 
filled 
in. 
For 
each 
such 
sub&tring 
Xi,i+2, 
we 
break 
the 
up 
into 
two 
null 
substrings 
Xi,i+l 
and 
Xi+!,i+2 
of 
length 
one 
and 
check 
the 
table 
entries 
Ti,i+l, 
Ti+I,i+2 
corresponding 
to 
those 
substrings.These 
entries 
occur 
mediately 
above 
and 
to 
the 
right 
of 
Ti,i+2. 
We 
select 
a 
nonterminal 
from 
each 
of 
these 
locations 
(say 
X 
from 
Ti,i+l 
and 
Y 
from 
T
i
+
1
,i+2) 
and 
look 
to 
see 
if 
there 
are 
any 
productions 
Z 
-+ 
XY 
in 
G. 
For 
each 
such 
tion 
we 
find, 
we 
label 
Ti,i+2 
with 
Z. 
We 
do 
this 
for 
all 
possible 
choices 
of 
X 
E 
Ti,i+! 
and 
Y 
E 
T
i
+!,i+2. 
In 
our 
example, 
for 
XO,2 
= 
aa, 
we 
find 
only 
A 
E 
TO,I 
and 
A 
in 
T
I
,2, 
so 
we 
look 
for 
a 
production 
with 
AA 
on 
the 
right-hand 
side. 
There 
aren't 
any, 
so 
T
O
,2 
is 
the 
empty 
set. 
Let's 
write 
0 
in 
the 
table 
to 
indicate 
this. 
For 
T
I
,3, 
we 
find 
A 
immediately 
above 
and 
B 
to 
the 
right, 
so 
we 
look 
for 
a 
production 
with 
AB 
on 
the 
right-hand 
side 
and 
find 
S 
--
AB, 
so 
we 
label 
T
I
,3 
with 
S. 
We 
continue 
in 
this 
fashion 
until 
all 
the 
T
i
,i+2 
are 
filled 
in. 
0 
A 
1 
0 
A 
2 
S 
B 
3 
0 
B 
4 
S 
A 
5 
S 
B 
6 
Now 
we 
proceed 
to 
strings 
of 
length 
three. 
For 
each 
such 
stri!l6, 
tnere 
are 
two 
way!:: 
to 
break 
it 
up 
into 
two non 
null 
substrings. 
For 
example, 
We 
need 
to 
check 
both 
posslbilities. 
For 
the 
first, 
we 
find 
A 
• 
To,l 
and 
SE 
T
I
,3, 
so 
we 
look 
for 
a 
production 
with 
right-hand 
side 
AS. 
There 
aren't 
a.ny. 
Now 
we 
check 
T
O
,2 
and 
T2,3. 
We 
find 
0 
in 
T
O
,2, 
so 
there 
is 
not 
hing 
more 
to 
check. 
We 
didn't 
find 
a 
nonterminal 
generating 
XO,3, 
so 
we 
label 
T
O
,3 
with 
0. 
For 
XI,4 
:::: 
Xl,2X2,4 
:::: 
XI,3X3,4, 
we 
find 
A 
E 
Tl,2 
and 
0 
in 
T2,4, 
so 
there 
is 
t\Qtlling 
hßre 
tQ 
check; 
J\.nd 
we 
find 
S 
E 
T13 
and 
BETa,., 
so 
we 
look 
for 
a 
pfoduction 
with 
right-hand 
side 
SB 
and 
find 
C 
---> 
SB; 
thus 
we 
label 
T
I
,4 

_____________________________________________
194 
Lecture 
27 
with 
C. 
0 
A 
1 
0 
A 
2 
0 
S 
B 
3 
C 
0 
B 
4 
S 
A 
5 
S 
B 
6 
We 
eontinue 
in 
this 
fashion, 
filling 
in 
the 
rest 
of 
the 
entries 
eorresporiding 
to 
strings 
of 
length 
three, 
then 
strings 
of 
length 
four, 
and 
so 
forth. 
For 
strings 
of 
length 
four, 
there 
are 
three 
ways 
to 
break 
them 
up, 
and 
all 
must 
be 
eheeked. 
The 
following 
is 
the 
final 
result: 
0 
A 
1 
0 
A 
2 
0 
S 
B 
3 
S 
C 
0 
B 
4 
D 
S 
0 
S 
A 
5 
S 
C 
0 
C 
S 
B 
6 
We 
see 
that 
T
O
,6 
eontains 
S, 
the 
start 
symbol, 
indieating 
that 
=x, 
G ' 
so 
we 
eonclude 
that 
x 
is 
generated 
by 
G. 
In 
this 
example, 
there 
is 
at 
most 
one 
nonterminal 
in 
each 
Ioeation. 
This 
is 
beeause 
for 
this 
partic11Iar 
grammar, 
the 
nonterminals 
generate 
disjoint 
sets. 
In 
general, 
there 
may 
be 
more 
than 
one 
nonterminal 
in 
eaeh 
Ioeation. 
A 
formal 
description 
of 
the 
algorithm 
is 
given 
below. 
One 
can 
ascertain 
from 
the 
nested 
Ioop 
structlue 
that 
the 
C'omplexity 
of 
the 
algorithm 
is 
O(pn
3
), 
where 
n 
= 
lxi 
and 
p 
is 
the 
number 
of 
productions 
of 
G. 

_____________________________________________
The 
Cocke-Kasami-Younger 
Aigorithm 
195 
for 
i 
:= 
0 
to 
n 
-
1 
do 
/* 
strings 
of 
length 
1 
first 
* / 
begin 
Ti,i+! 
:= 
0; 
/* 
initialize 
to 
0 
* / 
for 
A 
-+ 
a 
a 
production 
of 
G 
do 
if 
a 
= 
Xi,i+1 
then 
T"i+1 
:= 
Ti,i+1 
U 
{A} 
end; 
for 
m 
:= 
2 
to 
n 
do 
1* 
for 
each 
length 
m 
2 
* / 
for 
i 
:= 
0 
to 
n 
-
m 
do 
1* 
for 
each 
substring 
* / 
begin 
1* 
of 
length 
m 
* / 
Ti,;+m 
:= 
0; 
/* 
initialize 
to 
0 
'" 
/ 
for 
j 
:= 
i 
+ 
1 
to 
i 
+ 
m -
1 
do 
/* 
for 
all 
ways 
to 
break 
* / 
for 
A 
-+ 
BC 
a 
production 
of 
G 
do 
1* 
up 
the 
string 
* / 
if 
B 
E 
T;,j 
1\ 
C 
E 
Tj,i+m 
then 
T;,i+m 
:= 
Ti,i+m 
U 
{A} 
end; 
Closure 
Properties 
of 
CFLs 
CFLs 
are 
closed 
under 
union: 
ifA 
and 
Bare 
CFLs, 
say 
generated 
by 
mars 
G
l 
and 
G
2 
with 
start 
symbols 
51 
and 
52, 
respectively, 
one 
can 
form 
a 
grammar 
generating 
A 
U 
B 
by 
combining 
all 
productions 
of 
G
1 
and 
G
2 
along 
with 
a 
new 
start 
symbol 
5 
and 
new 
productions 
5 
-+ 
51 
and 
5 
-+ 
52. 
Before 
combining 
the 
grammars, 
we 
must 
first 
make 
sure 
G
1 
and 
G
2 
have 
disjoint 
sets 
of 
nonterminals. 
lf 
not, 
just 
rename 
the 
nonterminals 
in 
one 
of 
them. 
Similarly, 
CFLs 
closed 
under 
set 
concatenation: 
if 
A 
and 
Bare 
CFLs 
generated 
by 
grammars 
G
1 
and 
G
2 
as 
above, 
one 
can 
form 
a 
grammar 
generating 
AB 
= 
{xy 
I 
x 
E 
A, 
y 
E 
B} 
by 
combining 
G
1 
and 
G
2 
with 
a. 
new 
start 
symbol 
Sand 
new 
production 
5 
-+ 
5
1
5
2
Ł 
CFLs 
are 
closed 
under 
asterate: 
if 
A 
is 
a 
CFL 
generated 
by 
a 
gtammar 
with 
start 
symbol 
51, 
then 
A 
* 
is 
generated 
by 
the 
same 
grammar 
but 
with 
new 
start 
symbol 
Sand 
new 
productions 
5 
-+ 
5
1
5 
I 
E. 
CFLs 
are 
closed 
under 
intersection 
with 
regular 
sets: 
if 
A 
is 
a 
CFL 
and 
R 
is 
regular, 
then 
AnR 
is 
a 
CFL 
(Homework 
7, 
Exercise 
2). 
This 
can 
be 
shown 
by 
a 
product 
construction 
involving 
a 
PDA 
for 
A 
and 
a 
DFA 
for 
R 
similar 
to 
the 
construction 
we 
used 
to 
show 
that 
the 
intersection 
of 
two 
regular 
sets 
is 
regular. 
This 
property 
is 
useful 
in 
simplifying 
proofs 
that 
certain 
sets 
are 
not 
context-free. 
For 
example, 
to 
show 
that 
the 
set 
A 
= 
{x 
E 
{a,b,c}* 
I 
#a(x) 
= 
#b(x) 
= 
#dx)} 

_____________________________________________
196 
Lecture 
27 
is 
not 
context-free, 
intersect 
it 
with 
a* 
b* 
c* 
to 
get 
A 
na*b*c* 
== 
{a"bnc
n 
In? 
O}, 
which 
we 
have 
already 
shown 
is 
not 
context-free. 
Techniques 
similar 
to 
th08e 
uscd 
in 
Lecture 
10 
to 
show 
that 
the 
regular 
sets 
are 
closed 
under 
homomorphic 
images 
and 
preimages 
can 
be 
used 
to 
Show 
the 
same 
results 
for 
CFLs 
(Mis('ellaneous 
Exercise 
79). 
CFLs 
are 
not 
c\osed 
under 
intersect.ion: 
{ambmc
n 
I 
m,n? 
O} 
n 
{a'''/Jn
c" 
I 
m,n? 
O}:= 
{anb"c" 
I 
TI 
O}. 
The 
product 
construct.ion 
does 
not 
work 
for 
two 
CFLsj 
intuitively, 
there 
is 
no 
way 
to 
simulate 
two 
independent 
stacks 
with 
a 
single 
stack. 
We 
have 
also 
shown 
in 
Lecture 
22 
that 
the 
family 
of 
CFLs 
is 
not 
closed 
under 
complement: 
the 
set 
{ww 
I 
w 
E 
{a,b}*} 
is 
not 
a 
CFL, 
but 
its 
complement 
iso 
Closure 
Properties 
of 
DCFLs 
A 
deterministic 
context-Jree 
language 
(DCFL) 
is 
a 
language 
accepted 
by 
a 
deterministic 
PDA 
(DPDA). 
These 
automata 
were 
introduced 
in 
mentary 
Lecture 
F. 
A 
DPDA 
is 
like 
an 
NPDA, 
except 
that 
its 
transition 
relation 
i5 
single-valued 
(Le., 
is 
a 
function). 
We 
also 
need 
to 
include 
a 
cial 
right 
endmarker 
-I 
so 
that 
the 
machine 
can 
tell 
when 
it 
reaches 
the 
end 
of 
the 
input 
string. 
The 
endmarker 
is 
not 
necessary 
for 
an 
NPDA, 
because 
an 
NPDA 
can 
guess 
nondeterministically 
where 
the 
end 
of 
the 
input 
string 
IS. 
Most 
of 
the 
important 
examples 
of 
CFLs 
we 
have 
seen 
have 
been 
DCFLs. 
For 
example, 
{anb
n 
I 
TI 
? 
O} 
is 
a 
DCFL. 
The 
shift-reduce 
parser 
of 
Lecture 
26 
was 
also 
a 
DPDA. 
Every 
DCFL 
is 
a 
CFL, 
but 
not 
vice 
versa. 
The 
set 
{a,b}* 
-
{ww 
I 
W 
E.{a,b}*} 
is 
an 
example 
of 
a 
CFL 
that 
is 
not 
a 
DCFL 
We 
showed 
in 
Lecture 
22 
that 
this 
set 
i8 
a 
CFL, 
but 
its 
complement 
i8 
not. 
This 
implies 
that 
neither 
set 
is 
accepted 
by 
any 
DPDA, 
since 
as 
we 
showed 
in 
Supplementary 
ture 
F, 
the 
family 
of 
DCFLs 
is 
c10sed 
under 
complement. 
Thus, 
unlike 
the 
fAßt! 
cf 
finite 
ft.lltnmata, 
c:leterministic 
PD.-\s 
art' 
iitrictly 
leSil 
po'werfui 
nandetorministic 
oues. 

_____________________________________________
The 
Cocke-Kasami-Younger 
Aigorithm 
197 
DCFLs 
are 
not 
closed 
under 
union. 
For 
example, 
consider 
the 
union 
{ambnc
lc 
I 
m"l 
n} 
U 
{ambnc
lc 
I 
n:l 
k}. 
Each 
set 
is 
a 
DCFL. 
The 
union 
is 
a 
CFL-an 
NPDA 
could 
guess 
ministically 
which 
condition 
to 
check 
for-but 
it 
iS'not 
a 
DCFL. 
If 
it 
were, 
then 
its 
complement 
...... 
{ambnc" 
I 
m"l 
n} 
n 
""{ambnc" 
In 
=F 
k} 
would 
be. 
But 
then 
intersecting 
with 
the 
regular 
set 
a*b"'c· 
would 
give 
{anbnc
n 
I 
n 
O}, 
wh 
ich 
is 
not 
even 
context-free. 
Similarly, 
DCFLs 
are 
not 
closed 
under 
reversal, 
although 
proving 
this 
is 
a. 
little 
harder. 
The 
set 
{bambnc
k 
I 
m"l 
n} 
U 
{cambnc
k 
I 
n"l 
k} 
over 
the 
alphabet 
{a, 
b, 
c} 
is 
an 
example 
of 
a 
DCFL 
whose 
reversa.l 
is 
not 
a 
DCFL 
(Miscellaneous 
93). 
Historical 
Notes 
The 
CKY 
algorithm 
first 
appeared 
in 
print 
in 
Kasami 
[65] 
and 
Younger 
[125], 
although 
Hopcroft 
and 
Ullman 
[60J 
credit 
the 
original 
idea 
to 
John 
Cocke. 
Closure 
properties 
of 
CFLs 
were 
studied 
by 
Scheinberg 
[110J, 
Ginsburg 
and 
Rose 
[46, 
48], 
Bar-Hillel, 
PerIes, 
and 
Shamir 
[8], 
and 
Ginsburg 
and 
Spanier 
[49]. 
See 
p. 
180 
for 
the 
history 
of 
DPDAs 
and 
DCFLs. 

_____________________________________________
Supplementary 
Lecture 
G 
The 
Chomsky-Schützenberger 
Theorem 
Let 
PAREN 
n 
denote 
the 
language 
consisting 
of 
all 
balanced 
strings 
of 
parentheses 
of 
n 
distinct 
types. 
This 
language 
is 
generated 
by 
the 
grammar 
S 
-+ 
[S] 
I 
[S] 
I 
... 
I 
[S] 
I 
SS 
I 
•. 
112-2 
nn 
The 
languages 
PAREN
n 
are 
sometimes 
called 
Dyck 
languages 
in 
the 
ature. 
The 
following 
theorem 
shows 
that 
the 
parenthesis 
languages 
PAREN
n 
play 
a 
special 
role 
in 
the 
theory 
of 
context-free 
languages: 
every 
CFL 
is 
tially 
a 
parenthesis 
language 
modified 
in 
soine 
relatively 
simple 
way. 
In 
a 
sense, 
balanced 
parentheses 
capture 
the 
essential 
structure 
of 
CFLs 
that 
differentiates 
them 
from 
the 
regular 
sets. 
Theorem 
G.I 
(Chomsky-Schützenberger) 
Every 
context-free 
language 
is 
a 
morphic 
image 
of 
the 
intersection 
of 
a 
parenthesis 
language 
and 
a 
regular 
set. 
In 
other 
words, 
for 
every 
CFL 
A, 
there 
is 
an 
n 
0, 
a 
regular 
set 
R, 
and 
a 
homomorphism 
h 
such 
that 
A 
= 
h(PAREN
n 
n 
R). 
Recall 
from 
Lecture 
10 
that 
a 
homomorphism 
is 
a 
map 
h 
: 
r* 
-+ 
E* 
such 
that 
h(xy) 
= 
h(x)h(y) 
for 
all 
x,y 
E 
r*. 
It 
follows 
from 
this 
property 
that 
h(•) 
= 
• 
and 
that 
h 
is 
completely 
determined 
by 
its 
values 
on 
r. 
The 

_____________________________________________
The 
Chomsky-Schützenberger 
Theorem 
199 
homomorphic 
image 
of 
a 
set 
B 
r* 
under 
h 
is 
the 
set 
{h( 
x) 
I 
x 
E 
B} 
E*, 
denoted 
h(B). 
Proof. 
Let 
G 
= 
(N, 
E, 
P, 
S) 
be 
an 
arbitrary 
CFG 
in 
Chomsky 
normal 
form. 
Denote 
prod 
uetions 
in 
P 
by 
7r, 
p, 
(T, 
Ł 
ŁŁ 
Ł 
For 
7r 
E 
P, 
define 
1 
12 
2 
{ 
, 
1r 
1r1f' 
'lf 
7r 
= 
1122 
[][] 
1r 
1r1r 
1r 
if 
7r 
= 
A 
BC, 
if 
7r 
= 
A 
aj 
and 
define 
the 
grammar 
G' 
:;= 
(N, 
r, 
pI, 
S) 
with 
1 1 2 2 
r 
= 
{[, 
], 
[, 
] 
17r 
E 
P}, 
1r'7r1r1r 
p' 
= 
{7r
' 
17r 
E 
P}. 
The 
idea 
here 
is 
that 
a 
balaneed 
string 
of 
parentheses 
generated 
by 
G' 
eneodes 
a 
eorresponding 
string 
generated 
by 
G 
along 
with 
its 
parse 
tree. 
Let 
PAREN 
r 
be 
the 
parenthesis 
language 
over 
parentheses 
r. 
Surely 
L( 
G') 
PARENr, 
sinee 
the 
produetions 
of 
G' 
generate 
parentheses 
in 
well-nested 
matehed 
pairs. 
However, 
not 
all 
strings 
in 
PARENr 
are 
generated 
by 
G'. 
Here 
are 
some 
properties 
satisfied 
by 
strings 
in 
L( 
G') 
that 
are 
not 
satisfied 
by 
strings 
in 
PAREN 
r 
in 
general: 
1 
2 
(i) 
Every 
] 
is 
immediately 
followed 
by 
a 
[. 
"" 
"" 
2 
(ii) 
No 
] 
is 
immediately 
followed 
by 
a 
left 
parenthesis. 
"" 
(iii) 
1 
1 
If 
7r 
= 
A 
BC, 
then 
every 
( 
is 
immediately 
followed 
by 
[ 
for 
some 
"" 
2 
P 
pEP 
with 
left-hand 
side 
B, 
and 
every 
[ 
is 
immediately 
followed 
by 
1 
"" 
[ 
for 
so 
me 
(7 
E 
P 
with 
left-hand 
side 
C. 
(iv) 
1 
1 
2 
If 
7r 
= 
A 
a, 
then 
every 
[ 
is 
immediately 
followed 
by 
] 
and 
every 
[ 
""2 
"" 
"" 
is 
imrnediately 
followed 
by 
]. 
"" 
In 
addition, 
aU 
strings 
x 
such 
that 
A 
x 
satisfy 
the 
property 
1 
(v 
A) 
The 
string 
x 
begins 
with 
[ 
for 
sorne 
7r 
E 
P 
with 
left-hand 
side 
A. 
"" 
Eaeh 
of 
the 
properties 
(i) 
though 
(VA) 
ean 
be 
deseribed 
by 
a 
regular 
expression; 
thus 
the 
sets 
RA::: 
{x 
E 
r* 
I 
x 
satisfies 
(i) 
through 
(VA)} 

_____________________________________________
200 
Supplementary 
lectute 
G 
lemma 
G.2 
are 
regular. 
We 
claim 
A 
-.!... 
x 
{:::::} 
x 
E 
PAREN
r 
n 
RA. 
G' 
Proof. 
The 
direction 
(=» 
is 
a 
straightforward 
proof 
by 
induction 
on 
the 
length 
of 
the 
derivation. 
For 
the 
direction 
({::), 
suppose 
x 
E 
PAREN
r 
n 
RA. 
We 
proceed 
by 
induction 
on 
the 
length 
of 
x. 
It 
follows 
from 
properties 
(i) 
through 
(VA) 
and 
the 
fact 
that 
x 
is 
astring 
of 
balanced 
parentheses 
that 
x 
is 
01 
the 
form 
1 
12 
2 
X 
= 
[yl 
[Z] 
11' 
11'11' 
11' 
for 
some 
y, 
Z 
E 
r* 
and 
11' 
with 
left-hand 
side 
A. 
If 
'11' 
= 
A 
-
BO, 
then 
from 
property 
(iii), 
y 
satisfies 
(VB) 
and 
z 
satisfies 
(vc). 
Also, 
y 
and 
.t 
satisfy 
(i) 
through 
(iv) 
and 
are 
balanced. 
Thus 
y 
E 
PARENr 
n 
RB 
and 
Z 
E 
PARENr 
nRc. 
By 
the 
induction 
hypothesis, 
B 
7 
y 
and 
07 
Zj 
therefore, 
1 1 
12 
2 Ł 1 
12 
2 
A 
-;+ 
[E] 
[C) 
----;+ 
[V) 
[z] 
= 
x. 
G 
11' 
11'11' 
,.. 
G 
,.. 
,..,.. 
11' 
If 
'11' 
= 
A 
-I 
a, 
then 
from 
property 
(iv), 
y 
= 
Z 
= 
E, 
and 
1 1 
12 
2 
A 
--+ 
[1 
[J 
= 
X. 
G' 
"''''11' 
11' 
o 
It 
follows 
from 
Lemma 
G.2 
that 
L((}') 
= 
PAREN
r 
n 
Rs. 
Now 
define 
the 
homomorphism 
h: 
r* 
-+ 
r;* 
ag 
follows. 
For 
'11' 
of 
the 
form 
A 
-
BG, 
take 
1 1 2 2 
h( 
c) 
= 
h( 
J) 
= 
h( 
r) 
= 
h( 
J) 
= 
E. 
1f 
.,.. 
.". 
11' 
For 
'11' 
of 
the 
form 
A 
-+ 
a, 
take 
1 2 2 
h(J) 
= 
h([) 
= 
h()) 
= 
E, 
,.. 
11' 
11' 
1 
h( 
r) 
= 
a. 
11' 
Applying 
h 
to 
the 
production 
'11" 
of 
P' 
gives 
the 
production 
11' 
ol 
Pj 
thU8 
L(G) 
= 
h(L(G')) 
= 
h(PARENr 
n 
Rs). 
This 
completes 
the 
prool 
ol 
the 
Chomsky-Schützenberger 
theorem. 
0 
Historical 
Notes 
The 
pivotal 
importance 
of balanced 
parentheses 
in 
the 
.theory 
of 
free 
languages 
Wag 
recognized 
quite 
early 
on. 
The 
Chomsky-Schützenberger 
theorem 
is 
due 
to 
Chomsky 
and 
Schützenberger 
[19, 
22J. 

_____________________________________________
Supplementary 
Lecture 
H 
Parikh's 
Theorem 
Here 
is 
a 
theorem 
that 
says 
a 
Httle 
more 
about 
the 
structure 
of 
CFts. 
It 
s&ys 
that 
for 
any 
CFL 
A, 
if 
we 
look 
only 
at 
the 
relative 
number 
oE 
occurrences 
of 
terminal 
symbols 
in 
strings 
in 
A 
without 
regard 
to 
their 
order,' 
then 
A 
is 
indistinguishable 
from 
a 
regular 
set. 
Formally, 
let. 
E 
= 
{al" 
.. 
, 
aA:}. 
The 
Pflf'ikh 
map 
is 
the 
function 
1/J: 
E* 
-+ 
Nie 
defined 
by 
1/J(x) 
(#al 
(x), 
#a2(x), 
.
.. 
, 
#a",(z)). 
That 
is, 
1/J(x) 
records 
the 
number 
of 
occurrences 
of 
each 
symbol 
in 
z. 
The 
structure 
E* 
with 
binary 
operation· 
(concatenation) 
and 
constant 
E 
forms 
a 
monoid,l 
as 
does 
the 
structure 
Nie 
with 
binary 
operation 
+ 
(componentwise 
addition) 
and 
identity 
0 
= 
(0, 
... 
,0), 
and 
1/J 
is 
a 
monoid 
homomorphism: 
1/J(xY) 
= 
1/J(x) 
+ 
1/J(y), 
t/J(t') 
= 
Ö. 
'Recall 
from 
Lecture 
2 
that 
a 
monoid 
is 
an 
algebraic 
structure 
consisting 
oe 
a 
set 
with 
an 
associative 
binary 
operation 
and 
an 
identity 
for 
that 
operation. 

_____________________________________________
202 
Supplementary 
Lecture 
H 
The 
main 
difference 
between 
the 
monoids 
and 
N
k 
is 
that 
the 
latter 
is 
commutative,2 
whereas 
the 
former 
is 
not, 
except 
in 
the 
case 
k 
= 
1. 
In 
fact, 
if 
== 
is 
the 
smallest 
monoid 
congruence
3 
on 
such 
that 
aiaj 
== 
ajai, 
1 
::; 
i,j 
::; 
k, 
then 
N
k 
is 
isomorphie 
to 
the 
quotient
4 
j-==. 
The 
monoid 
is 
sometimes 
called 
the 
free 
monoid 
on 
k 
generators, 
and 
N
k 
is 
sometimes 
called 
the 
free 
commutative 
monoid 
on 
k 
generators. 
The 
word 
"free" 
refers 
to 
the 
fact 
that 
the 
structures 
do 
not 
satisfy 
any 
equAtions 
besides 
the 
logical 
consequences 
of 
the 
monoid 
or 
commutative 
monoid 
axioms, 
respectively. 
The 
commutative 
image 
of 
a 
set 
A 
is 
its 
image 
under 
'!fJ: 
'!fJ(A) 
{'!fJ(x) 
I 
x 
E 
A}. 
If 
Ul, 
... 
, 
Um 
E 
N
k
, 
the 
submonoid 
of 
N
k 
generated 
by 
Ul, 
... 
, 
Um 
is 
denoted 
<Ul, 
... 
, 
Um>. 
This 
is 
the 
smallest 
subset 
of 
N
k 
containing 
UI, 
... 
, 
Um 
and 
the 
monoid 
identity 
0 
and 
closed 
under 
+. 
Equivalently, 
<Ul,··· 
,Um> 
= 
{alUl 
+ 
... 
+ 
a
m 
Um 
I 
al, 
... 
,am 
E 
N} 
N
k
. 
A 
subset 
of 
N
k 
is 
called 
linear 
if 
it 
is 
a 
coset 
of 
such 
a 
finitely 
generated 
submonoid; 
that 
is, 
if 
it 
is 
of 
the 
form 
Uo 
+ 
<Ul, 
... 
, 
um> 
= 
{uo 
+ 
al 
Ul 
+ 
... 
+ 
amU
m 
I 
al,··· 
, 
a
m 
E 
N}. 
A 
subset 
of 
N
k 
is 
called 
semilinear 
if 
it 
is 
a 
union 
of 
finitely 
many 
linear 
sets. 
For 
example, 
'!fJ({anb
n 
I 
n 
O}) 
= 
'!fJ({x 
E 
{a,b}* 
I 
#a(x) 
= 
#b(x)}) 
= 
{(n, 
n) 
I 
n 
O} 
= 
«1,1» 
is 
a 
semilinear 
(in 
fact, 
linear) 
subset 
of 
but 
({n, 
n
2
) 
I 
n 
O} 
is 
not. 
Theorem 
H.l 
(Parikh) 
For 
any context-free 
language 
A, 
'!fJ(A) 
is 
semilinear. 
The 
converse 
does 
not 
hold: 
the 
set 
{anbnc
n 
I 
n 
O} 
is 
not 
context-free 
but 
has 
a 
semilinear 
image 
under 
'!fJ. 
This 
is 
also 
the 
image 
of 
the 
CFL 
{(ab)ncn 
I 
n 
O} 
and 
the 
regular 
set 
(abc)*. 
2 
A 
monoid 
is 
commutative 
jf 
xy 
= 
yx 
for 
all 
x 
and 
y. 
3 
A 
monoid 
congruence 
is 
an 
equivalence 
relation 
== 
on 
the 
monoid 
that 
respects 
the 
monoid 
structure 
in 
the 
sense 
that 
if 
x 
== 
x' 
and 
y 
== 
y', 
then 
xy 
== 
x' 
y'. 
'The 
quotient 
of 
a 
mono:d 
M 
by 
a 
congmence 
== 
is 
a 
monoid 
whose 
elements 
are 
the 
congruence 
c1asses 
[x] 
{y 
I 
y 
== 
x}, 
binary 
operation 
[x]· 
[y] 
[xy], 
and 
constant 
[1], 
where 
1 
is 
the 
identity 
of 
M. 
See 
Supplementary 
Lectures 
C 
and 
D 
for 
more 
information 
on 
these 
concepts. 

_____________________________________________
Parikh's 
Theorem 
203 
For 
every 
semilinear 
set 
5 
N
k
, 
it 
is 
not 
hard 
to 
construct 
a 
regular 
set 
R 
E* 
such 
that 
'IjJ(R) 
= 
S. 
For 
example, 
{(n,n) 
I 
n 
O} 
= 
'IjJ((ab)*). 
For 
this 
reason, 
Parikh's 
theorem 
is 
sometimes 
stated 
as 
follows: 
Every 
context-free 
language 
is 
letter-equivalent 
to 
a 
regular 
set, 
where 
letter-equivalence 
means 
the 
sets 
have 
the 
same 
commutative 
image. 
In 
order 
to 
prove 
Parikh's 
theorem, 
we 
need 
some 
definitions. 
Let 
G 
= 
(N, 
E, 
P, 
S) 
be 
an 
arbitrary 
CFG 
in 
Chomsky 
normal 
form. 
Let 
s, 
t, 
... 
denote 
parse 
trees 
of 
G 
with 
a 
nonterminal 
at 
the 
root, 
nonterminals 
ing 
the 
internal 
nodes, 
and 
terminals 
or 
nonterminals 
labeling 
the 
leaves. 
Define 
root(s) 
the 
nonterminal 
at 
the 
root 
of 
Sj 
yield( 
s) 
the 
string 
of 
terminals 
and 
nonterminals 
at 
the 
leaves 
of 
s, 
reading 
left 
to 
rightj 
depth(s) 
the 
length 
of 
the 
longest 
path 
in 
s 
from 
a 
leaf 
up 
to 
the 
root 
(the 
length 
of 
a 
path 
is 
the 
number 
of 
edges, 
or 
the 
number 
of 
nodes 
less 
one)j 
N(s) 
the 
set 
of 
nonterminals 
appearing 
in 
s. 
Define 
a 
pump 
to 
be 
a 
parse 
tree 
s 
such 
that 
(i) 
s 
contains 
at 
least 
two 
nodesj 
and 
(ii) 
yield(s) 
= 
x· 
root(s)· 
y 
for 
some 
x,y 
E 
E*j 
that 
is, 
allleaves 
are 
beled 
with 
terminal 
symbols 
except 
one, 
and 
the 
nonterminallabeling 
that 
leaf 
is 
the 
same 
as 
the 
one 
labeling 
the 
root. 
These 
objects 
arose 
in 
the 
proof 
of 
the 
pumping 
lemma 
for 
CFLs 
in 
Lecture 
22: 
wherever 
a 
nonterminal 
A 
appears 
in 
a 
parse 
tree 
s, 
the 
tree 
can 
be 
split 
apart 
at 
that 
point 
and 
a 
pump 
u 
with 
root( 
u) 
= 
A 
inserted 
to 
get 
a 
larger 
parse 
tree 
t. 
For 
parse 
trees 
s, 
t, 
define 
s 
<I 
t 
if 
t 
can 
be 
obtained 
from 
s 
by 
splitting 
s 
at 
anode 
labeled 
with 
some 
nonterminal 
A 
and 
inserting 
a 
pump 
with 
root 
labeled 
A. 
The 
relation 
<I 
is 
not 
a 
partial 
order 
(it 
is 
not 
reflexive 
or 
transitive), 
but 
it 
is 
well 
founded 
in 
the 
sense 
that 
there 
exists 
no 
infinite 
descending 
chain 
So 
[> 
SI 
[> 
S2 
[> 
"', 
because 
if 
s 
<I 
t, 
then 
s 
has 
strictly 
fewer 
nodes 
than 
t. 
Define 
a 
pump 
t 
to 
be 
a 
basic 
pump 
if 
it 
is 
<I-minimal 
among 
all 
pumpsj 
that 
is, 
if 
it 
does 
not 
properly 
contain 
another 
pump 
that 
could 
be 
cut 
out. 
In 
other 
words, 
a 
pump 
t 
is 
a 
basic 
pump 
if 
the 
only 
s 
such 
that 
s 
<I 
t 
is 

_____________________________________________
204 
Supplementary 
Lecture 
H 
the 
trivial 
one-node 
parsetree 
labeled 
with 
the 
nonterminal 
root(t). 
Basic 
pumps 
cannot 
be 
too 
big: 
Lemma 
H.2 
11 
s 
is 
a 
basic 
pump, 
then 
depth(s) 
::s 
2n, 
where 
.n 
is 
the 
number 
01 
nonterminals 
in 
N. 
Proof. 
Let 
11" 
denote 
the 
path 
in 
s 
from 
the 
unique 
leaf 
with 
label 
root( 
s) 
up 
to 
the 
root. 
The 
path 
11" 
can 
be 
no 
longer 
than 
n, 
because 
if 
it 
were, 
it 
would 
have 
a 
repeated 
nonterminal 
and 
would 
therefore 
contain 
a 
pump 
that 
could 
be 
removed,contradicting 
the 
<l-minimality 
of 
s. 
For 
any 
other 
leaf, 
the 
path 
from 
that 
leaf 
up 
to 
the 
first 
node 
on 
the 
path 
11" 
can 
be 
no 
longer 
than 
n 
+ 
1 
for 
the 
same 
reason. 
Thus 
the 
total 
length 
of 
any 
path 
from 
a 
leaf 
to 
the 
root 
can 
be 
no 
longer 
than 
2n. 
0 
It 
follows 
from 
Lemma 
H.2 
and 
the 
fact 
that 
there 
are 
only 
finitely 
many 
productions 
in 
G 
that 
the 
number 
of 
basic 
pumps 
is 
finite, 
say 
p. 
Lemma 
H.3 
Every 
parse 
tree 
t 
with 
yield(t) 
E 
E* 
is 
either 
<l-minimal 
or 
contains 
a 
basic 
pump. 
Prool. 
If 
t 
is 
not 
<l-minimal, 
then 
by 
definition 
it 
contains 
a 
pump 
s. 
Let 
s 
be 
<l-minimal 
among 
all 
pumps 
contained 
in 
t. 
Then 
s 
is 
a 
basic 
pump, 
because 
if 
it 
were 
not, 
then 
it 
would 
contain 
a 
smaller 
pump 
u, 
and 
u 
would 
be 
a 
smaller 
pump 
contained 
in 
t, 
contradicting 
the 
minimality 
uf 
s. 
0 
Define 
s 
::s 
t 
if 
t 
can 
be 
obtained 
from 
s 
by 
some 
finite 
sequenee 
of 
insertions 
of 
basic 
pumps 
u 
such 
that 
N(u) 
N(s). 
In 
other 
words, 
starting 
from 
s, 
we 
are 
allowed 
to 
eh,oose 
any 
occurrence 
of 
a 
nonterminal 
A 
in 
sand 
insert 
a 
basic 
pump 
u 
with 
root(u) 
= 
A 
at 
that 
point, 
provided 
u 
eontains 
no 
new 
nonterminals 
that 
are 
not 
already 
eontained 
in 
s. 
If 
t 
can 
be 
obtained 
from 
s 
by 
a 
finite 
number 
of 
repetitions 
of 
this 
process, 
then 
s 
::s 
t. 
If 
a 
E 
(N 
U 
E)*, 
define 
'I/J(a) 
= 
'I/J(x), 
where 
x 
is 
the 
string 
obtained 
from 
a 
by 
deleting 
all 
nonterminals. 
Let 
'I/J(t) 
abbreviate 
'I/J(yield(t)). 
Lemma 
W4 
The 
set 
{'I/J(t) 
I 
s::s 
t} 
is 
linear. 
Proof. 
{'I/J(t) 
I 
s::s 
t} 
= 
'I/J(s) 
+ 
<{'I/J(u) 
I 
u 
is 
a 
basic 
pump 
with 
N(u) 
N(s)}>. 
o 
Lemma 
H.5 
11 
s 
is 
::s 
-minimal, 
then 
depth( 
s) 
::s 
(p 
+ 
1)( 
n 
+ 
1), 
where 
p 
is 
the 
number 
01 
distinct 
basic 
pumps 
and 
n 
is 
the 
size 
01 
N. 
Prool. 
If 
s 
had 
a 
path 
longer 
than 
depth(s) 
::s 
(p 
+ 
l)(n 
+ 
1), 
then 
that 
path 
could 
be 
broken 
up 
into 
p 
+ 
1 
segments, 
each 
of 
length 
at 
least 
n 
+ 
1, 
and 
each 
segment 
would 
have 
a 
repeated 
nonterminal. 
Then 
there 
would 
be 

_____________________________________________
Parikh's 
Theorem 
205 
p 
+ 
1 
disjoint 
pumps. 
(Two 
pumps 
are 
considered 
disjoint 
if 
they 
have 
no 
nodes 
in 
common, 
or 
if 
the 
root 
of 
one 
is 
a 
leaf 
of 
the 
other.) 
Each 
of 
these 
p 
+ 
1 
pumps 
either 
is 
basic 
or 
contains 
a 
basic 
pump 
by 
Lemma 
H.3; 
thus 
there 
would 
be 
p 
+ 
1 
disjoint 
basic 
pumps. 
But 
there 
are 
only 
p 
distinct 
basic 
pumps 
in 
aB, 
so 
by 
the 
pigeonhole 
principle 
there 
must 
be 
two 
disjoint 
occurrences 
of 
the 
same 
basic 
pump. 
But 
this 
contradicts 
the 
of 
s, 
since 
one 
of 
these 
basic 
pumps 
could 
be 
deleted 
without 
changing 
the 
set 
of 
nonterminals 
contained 
in 
the 
tree. 
0 
Proof 
of 
Theorem 
H.l. 
Let 
M 
= 
{s 
I 
s 
is 
root(s) 
= 
S, 
yield(s) 
E 
We 
show 
that 
'IjJ(L(G)) 
= 
U 
{7P(t) 
I 
s 
t}. 
sEM 
This 
set 
is 
semilinear 
by 
Lemma 
H.5, 
which 
implies 
that 
M 
is 
finite, 
and 
by 
Lemma 
H.4. 
Any 
t 
such 
that 
s 
t 
for 
some 
sEM 
has 
root(t) 
= 
Sand 
yield(t) 
E 
thus 
yield(t) 
E 
L(G) 
and 
'ljJ(t) 
E 
'ljJ(L(G)). 
Conversely, 
any 
string 
XE 
L(G) 
has 
a 
parse 
tree 
t 
with 
root(t) 
= 
Sand 
yield(t) 
= 
x, 
and 
there 
must 
exist 
a 
s 
t. 
Then 
sEM 
and 
'ljJ(X) 
E 
{'ljJ(t) 
I 
s 
t}. 
o 
Historical 
Notes 
Parikh's 
theorem 
was 
first 
proved 
by 
Rohit 
Parikh 
[98]. 
Alternative 
proofs 
have 
been 
given 
by 
Goldstine 
[52], 
Harrison 
[55], 
and 
Kuich 
[75]. 

_____________________________________________
Lecture 
28 
Turing 
Machines 
and 
Effective 
Computability 
In 
this 
lecture 
we 
introduce 
the 
most 
powerful 
of 
the 
automata 
we 
will 
study: 
Turing 
machines 
(TMs), 
named 
after 
Alan 
Turing, 
who 
invented 
them 
in 
1936. 
Turing 
machines 
can 
compute 
any 
function 
normally 
sidered 
computable; 
in 
fact, 
it 
is 
quite 
reasonable 
to 
define 
computable 
to 
mean 
computable 
by 
a 
TM. 
TMs 
were 
invented 
in 
the 
1930s, 
long 
before 
real 
computers 
appeared. 
They 
came 
at 
a 
time 
when 
mathematicians 
were 
trying 
to 
come 
to 
grips 
with 
the 
notion 
of 
effective 
computation. 
They 
knew 
various 
algorithms 
for 
puting 
things 
effectively, 
but 
they 
weren't 
quite 
sure 
how 
to 
define 
tively 
computable" 
in 
a 
general 
way 
that 
would 
allow 
them 
to 
distinguish 
between 
the 
computable 
and 
the 
noncomputable. 
Several 
alternative 
malisms 
evolved, 
each 
with 
its 
own 
peculiarities, 
in 
the 
attempt 
to 
nail 
down 
this 
notion: 
Ł 
Turing 
machines 
(Alan 
Turing 
[120]); 
Ł 
Post 
systems 
(Emii 
Post 
[99, 
100]); 
Ł 
IL-recursive 
functions 
(Kurt 
Gödel 
[51 
J, 
Jacques 
Herbrand 
)j 
Ł 
>.-calculus 
(Alonzo 
Church 
[23], 
Stephen 
C. 
Kleene 
[66]); 
and 
Ł 
combinatory 
logic 
(Moses 
Schönfinkel 
[111 
j, 
Haskell 
B. 
Curry 
[29]). 

_____________________________________________
Turing 
Machines 
and 
Effective 
Computability 
207 
All 
of 
these 
systems 
embody 
the 
idea 
of 
effective 
compu.tation 
in 
one 
form 
or 
another. 
They 
work 
on 
various 
types 
of 
data; 
for 
example, 
Turing 
machines 
manipulate 
strings 
over 
a 
finite 
alphabet, 
JL-recursive 
functions 
manipulate 
the 
natural 
numbers, 
the 
'\-calculus 
manipulates 
'\-terms, 
and 
combinatory 
logic 
manipulates 
terms 
built 
[rom 
combinator 
symbols. 
However, 
there 
are 
natural 
translations 
between 
all 
these 
different 
types 
of 
data. 
For 
example, 
there 
is 
a 
simple 
one-to-one 
'correspondence 
between 
strings 
in 
{O, 
I} 
* 
and 
natural 
numbers 
N 
= 
{O, 
1,2, 
... 
} 
defined 
by 
X 
f-+ 
#(lx) 
-
1, 
(28.1) 
where 
#y 
is 
the 
natural 
number 
represented 
by 
the 
binary 
string 
y. 
versely, 
it 
is 
easy 
to 
encode 
just 
about 
anything 
(natural 
numbers, 
'\-terms, 
strings 
in 
{O,1,2, 
... 
,9}*, 
trees, 
graphs, 
... 
) 
as 
strings 
in 
{O,l}*. 
Under 
these 
natural 
encodings 
of 
the 
data, 
it 
turns 
out 
that 
all 
the 
formalisms 
above 
can 
simulate 
one 
another, 
so 
despite 
their 
superficial 
differences 
they 
are 
all 
computationally 
equivalent. 
Nowadays 
we 
can 
take 
unabashed 
advantage 
of 
our 
more 
modern 
tive 
and add 
programming 
languages 
such 
as 
PASCAL 
or 
C 
(or 
idealized 
versions 
of 
them) 
to 
this 
list-a 
true 
luxury 
compared 
to 
what 
Church 
and 
Gödel 
had 
to 
struggle 
with. 
Of 
the 
classical 
systems 
listed 
above, 
the 
one 
that 
most 
closely 
resembles 
a 
modern 
computer 
is 
the 
Turing 
machine. 
Besides 
the 
off-ihe-shelf 
model 
we 
will 
define 
below, 
there 
are 
also 
many 
custom 
variations 
(nondeterministic, 
multitape, 
multidimensional 
tape, 
two-way 
infinite 
tapes, 
and 
so 
on) 
that 
all 
turn 
out 
to 
be 
computationally 
equivalent 
in 
the 
sense 
that 
they 
can 
all 
simulate 
one 
another. 
Church's 
Thesis 
Because 
these 
vastly 
dissimilar 
formalisms 
are 
all 
computati.onally 
lent, 
the 
common 
notion 
of 
computability 
that 
they 
embody 
is 
extremely 
robust, 
which 
is 
to 
say 
thatit 
is 
invariant 
under 
fairly 
radical 
perturbations 
in 
the 
model. 
All 
these 
mathematicians 
with 
their 
pet 
systems 
turned 
out 
to 
be 
looking 
at 
the 
same 
thing 
from 
different 
angles. 
This 
was 
too 
striking 
to 
be 
mere 
coincidence. 
They 
soon 
came 
to 
the 
realization 
that 
the 
monality 
among 
all 
these 
systems 
must 
be 
the 
elusive 
notion 
of 
effective 
computability 
that 
they 
had 
sought 
for 
so 
long. 
Computability 
is 
not 
just 
Turing 
machines, 
nor 
the 
'\-calcuhis, 
nor 
the 
JL-recursive 
functions, 
nor 
the 
PASCAL 
programming 
language, 
but 
the 
common 
spirit 
embodied 
by 
them 
all. 
Alonzo 
Church 
[25] 
gave 
voice 
to 
this 
thought, 
and 
it 
has 
since 
become 
known 
as 
Church's 
thesis 
(or 
the 
Church-Turing 
thesis). 
It 
is 
not 
a 
the-

_____________________________________________
208 
Lecture 
28 
orem, 
but 
rather 
a 
dec1aration 
that 
all 
these 
formalisms 
capture 
precisely 
our 
intuition 
about 
what 
it 
means 
to 
be 
effectively 
computable 
in 
principle, 
no 
more 
and 
no 
less. 
Church's 
thesis 
may 
not 
seem 
like 
such 
a 
big 
deal 
in 
retrospect, 
since 
by 
now 
we 
are 
thoroughly 
familiar 
with 
the 
capabilities 
of 
modern 
computersj 
but 
keep 
in 
mind 
that 
at 
the 
time 
it 
was 
first 
mulated, 
computers 
and 
programming 
languages 
had 
yet 
to 
be 
invented. 
Coming 
to 
this 
realization 
waS 
an 
enormous 
intellectual 
leap. 
Probably 
the 
most 
compelling 
development 
leading 
to 
the 
acceptance 
of 
Church's 
thesis 
was 
the 
Thring 
machine. 
It 
was 
the 
first 
model 
that 
could 
be 
considered 
readily 
programmable. 
If 
someone 
laid 
one 
of 
the 
other 
tems 
out 
in 
front 
öf 
you 
and 
dec1ared, 
"This 
system 
captures 
exact1y 
what 
we 
mean 
by 
effectively 
computable," 
you 
might 
harbor 
some 
skepticism. 
But 
it 
is 
hard 
to 
argue 
with 
Thring 
machines. 
One 
can 
rightly 
challenge 
Church's 
thesis 
on 
the 
grounds 
that 
there 
are 
aspects 
of 
computation 
that 
-
are 
not 
addressed 
by 
Thring 
machines 
(for 
example, 
randomized 
or 
active 
computation), 
but 
no 
one 
could 
dispute 
that 
the 
notion 
of 
effective 
computability 
as 
captured 
by 
Turing 
machines 
is 
robust 
and 
important. 
Universality 
and 
Self-Reference 
One 
of 
the 
most 
intriguing 
aspects 
of 
these 
systems, 
and 
a 
pervasive 
theme 
in 
our 
study 
of 
them, 
is 
the 
idea 
of 
programs 
as 
data. 
Each 
of 
these 
gramming 
systems 
is 
powerful 
enough 
that 
programs-can 
be 
written 
that 
understand 
and 
manipulate 
other 
programs 
that 
are 
encoded 
as 
data 
in 
some 
reasonable 
way. 
For 
example, 
in 
the 
'x-calculus, 
'x-terms 
act 
as 
both 
programs 
and 
dataj 
combinator 
symbols 
in 
combinatory 
logic 
manipulate 
other 
combinator 
symbolsj 
there 
is 
a 
so-called 
Gödel 
numbering 
of 
the 
recursive 
functions 
in 
which 
each 
function 
has 
a 
number 
that 
can 
be 
used 
as 
input 
to 
other 
J-L-recursive 
functionsj 
and 
Turing 
mach 
in 
es 
can 
interpret 
their 
input 
strings 
as 
descriptions 
of 
other 
Thring 
machines. 
It 
is 
not 
a 
far 
step 
from 
this 
idea 
to 
the 
notion 
of 
universal 
simulation, 
in 
which 
a 
versal 
program 
or 
machine 
U 
is 
constructed 
to 
take 
an encoded 
description 
of 
another 
program 
or 
machine 
M 
and 
astring 
x 
as 
input 
and 
perform 
a 
step-by-step 
simulation 
of 
M 
on 
input 
x. 
A 
modern-day 
example 
of 
this 
phenomenon 
would 
be 
a 
SCHEME 
interpreter 
written 
in 
SCHEME. 
One 
far-reaching 
corollary 
of 
universality 
is 
the 
notion 
of 
self-reference. 
It 
is 
exactly 
this 
capability 
that 
led 
to 
the 
discovery 
of 
natural 
putable 
problems. 
If 
you 
know 
some 
set 
theory, 
you 
can 
convince 
yourself 
that 
uncomputable 
problems 
must 
exist 
by 
a 
cardinality 
argument: 
there 
are 
uncountably 
many 
decision 
problems 
but 
only 
countably 
many 
Thring 
machines. 
However, 
self-reference 
allows 
us 
to 
construct 
very 
simple 
and 
natural 
examples 
of 
uncomputable 
problems. 
For 
example, 
there 
do 
not 

_____________________________________________
T 
uring 
Machines 
and 
Effective 
Computability 
209 
exist 
genera:l 
procedures 
that 
can 
determine 
whether 
a 
given 
block 
of 
code 
in 
a 
given 
PASCAL 
program 
is 
ever 
going 
to 
be 
executed, 
or 
whether 
a 
given 
PASCAL 
program 
will 
ever 
halt. 
These 
are 
important 
problems 
that 
compiler 
builders 
would 
like 
to 
solve; 
unfortunately, 
one 
can 
give 
a 
formal 
proof 
that 
they 
are 
unsolvable. 
Perhaps 
the 
most 
striking 
example 
of 
the 
power 
of 
self-reference 
is 
the 
incompleteness 
theorem 
of 
KJlrt 
Gödel. 
Starting 
near 
the 
beginning 
of 
the 
twentieth 
century 
with 
Whitehead 
and 
Russell's 
Principia 
Maihematica 
[1231, 
there 
was 
a 
movement 
to 
reduce 
all 
of 
mathematics 
to 
pure 
symbol 
manipulation, 
independent 
of 
semantics. 
This 
was 
in 
part 
to 
understand 
and 
avoid 
the 
set-theoretic 
paradoxes 
discovered 
by 
Russell 
and 
others. 
This 
movement 
was 
advocated 
by 
the 
mathematician 
David 
Hilbert 
and 
became 
known 
as 
the 
formalist 
program. 
It 
attracted 
a 
lot 
of 
followers 
and 
fed 
the 
development 
of 
formal 
logic 
as 
we 
know 
it 
today. 
Its 
proponents 
believed 
that 
all 
mathematical 
truths 
could 
be 
derived 
in 
some 
fixed 
formal 
system, 
just 
by 
starting 
with 
a 
few 
axioms 
and 
applying 
rules 
of 
inference 
in 
a 
purely 
mechanical 
way. 
This 
view 
of 
mathematical 
proof 
is 
highly 
computational. 
The 
formal 
deductive 
system 
most 
popular 
at 
the 
time 
for 
reasoning 
about 
the 
natural 
numbers, 
called 
Peano 
arithmetic 
(PA), 
was 
believed 
to 
be 
adequate 
for 
expressing 
and 
deriving 
mechanically 
all 
true 
statements 
of 
number 
theory. 
The 
incompleteness 
theorem 
showed 
that 
this 
was 
wrong: 
there 
exist 
even 
fairly 
simple 
statements 
of,number 
theory 
that 
are 
true 
:hut 
not 
provable 
in 
PA. 
This 
holds 
not 
only 
for 
PA 
but 
for 
any 
reasonable 
extension 
of 
it. 
This 
revelation 
was 
a 
significant 
set 
back 
for 
the 
formalist 
program 
and 
sent 
shock 
waves 
throughout 
the 
mathematical 
world. 
Gödel 
proved 
the 
incompleteness 
theorem 
using 
self-reference. 
The 
basic 
observation 
needed 
here 
is 
that 
the 
language 
of 
number 
theory 
is 
sive 
enough 
to 
talk 
about 
itself 
and 
about 
proofs 
in 
PA. 
For 
example, 
one 
can 
write 
down 
a 
number-theoretic 
statement 
that 
says 
that 
a 
certain 
other 
number-theoretic 
statement 
has 
a 
proof 
in 
PA, 
and 
one 
can 
reason 
about 
such 
statements 
using 
PA 
itself. 
Now, 
by 
a 
tricky 
argument 
involving 
tutions, 
one 
can 
actually 
construct 
statementsthat 
talk 
about 
whether 
they 
themselves 
are 
provable. 
Gödel 
actually 
constructed 
a 
sentence 
that 
said, 
"I 
am 
not 
provable." 
This 
construction 
is 
presented 
in 
detail 
in 
Supplementary 
Lecture 
K. 
The 
consequences 
of 
universality 
are 
not 
only 
philosophical 
but 
also 
tical. 
U 
niversality 
was 
in 
a 
sense 
the 
germ 
of 
the 
idea 
that 
led 
to 
the 
opment 
of 
computers 
as 
we 
know 
them 
today: 
the 
not 
ion 
of 
a 
stored 
gram, 
a 
piece 
of 
software 
that 
can 
be 
read 
and 
executed 
by 
hardware. 
This 
programmability 
is 
what 
makes 
computers 
so 
versatile. 
Although 
it 
was 

_____________________________________________
210 
Lecture 
28 
only 
realized 
in 
physical 
form 
several 
years 
later, 
the 
notion 
was 
definitely 
present 
in 
Turing's 
theoretical 
work 
in 
the 
1930s. 
Informal 
Description 
of 
Turing 
Machines 
We 
describe 
here 
a 
deterministic, 
one-tape 
Turing 
machine. 
This 
is 
the 
standard 
off-the-shelf 
model. 
There 
are 
many 
variations, 
apparently 
more 
powerful 
or 
less 
powerful 
but 
in 
reality 
not. 
We 
will 
consider 
some 
of 
these 
in 
Lecture 
30. 
A 
TM 
has 
a 
finite 
set 
of 
states 
Q, 
a 
semi-infinite 
tape 
that 
is 
delimited 
on 
the 
left 
end 
by 
an 
endmarker 
I-
and 
is 
infinite 
to 
the 
right, 
and 
a 
head 
that 
can 
move 
left 
and 
right 
over 
the 
tape, 
reading 
and 
writing 
symbols. 
The 
input 
string 
is 
of 
finite 
length 
and 
is 
initially 
written 
on 
the 
t.ape 
in 
contiguous 
tape 
cells 
snug 
up 
against 
the 
left 
endmarker. 
The 
infinitely 
many 
cells 
to 
the 
right 
of 
the 
input 
all 
contain 
a 
special 
blank 
symbol 
u. 
The 
machine 
starts 
in 
its 
start 
state 
s 
with 
its 
head 
scanning 
the 
left 
marker. 
In 
each 
step 
it 
reads 
the 
symbol 
on 
the 
tape 
under 
its 
head. 
pending 
on 
that 
symbol 
and 
the 
current 
state, 
it 
writes 
a 
new 
symbol 
on 
that 
tape 
cell, 
moves 
its 
head 
either 
left 
or 
right 
one 
cell, 
and 
enters 
a 
new 
state. 
The 
action 
it 
takes 
in 
each 
situation 
is 
determined 
by 
a 
transition 
function 
8. 
Itcaccepts 
its 
input 
by 
entering 
a 
special 
accept 
state 
t 
and 
rejects 
by 
entering 
a 
special 
reject 
state 
r. 
On 
so 
me 
inputs 
it 
may 
run 
infinitely 
without 
ever 
accepting 
or 
rejecting, 
in 
which 
case 
it 
is 
said 
to 
loop 
on 
that 
input. 
Formal 
Definition 
of 
Turing 
Machines 
Formally, 
a 
deterministic 
one-tape 
Turing 
machine 
is 
a 
9-tuple 
M 
= 
(Q, 
r, 
1-, 
u, 
8, 
s, 
t, 
r), 
where 
Ł Q 
is 
a 
finite 
set 
(the 
states); 
Ł E 
is 
a 
finite 
set 
(the 
input 
alphabet); 

_____________________________________________
Turing 
Machines 
and 
Effective 
Computability 
211 
Ł r 
is 
a 
finite 
set 
(the 
tape 
alphabet) 
containing 
as 
a 
subset; 
Ł u 
E 
r -
the 
blank 
symbol; 
Ł 
r-
E 
r -
the 
left 
endmarker; 
Ł 
6: 
Q 
x 
r 
-+ 
Q 
x 
r 
x 
{L, 
R}, 
the 
transition 
function; 
Ł s 
E 
Q, 
the 
start 
state; 
Ł t 
E 
Q, 
the 
accept 
state; 
and 
Ł r 
E 
Q, 
the 
reject 
state, 
r 
=I-
t. 
Intuitively, 
6 
(p, 
a) 
= 
(q, 
b, 
d) 
means, 
"When 
in 
state 
p 
scanning 
a, 
write 
b 
on 
that 
tape 
cell, 
move 
the 
head 
in 
direction 
d, 
and 
enter 
state 
q." 
The 
symbols 
Land 
R 
stand 
for 
left 
and 
right, 
respectively. 
We 
restrict 
TMs 
so 
that 
the 
left 
endmarker 
is 
never 
overwritten 
with 
other 
symbol 
and 
the 
machine 
never 
moves 
off 
the 
tape 
to 
the 
left 
of 
the 
endmarker; 
that 
is, 
we 
require 
that 
for 
all 
p 
E 
Q 
there 
exists 
q 
E 
Q 
such 
that 
6(p,r-) 
= 
(q,r-,R). 
(28.2) 
We 
also 
require 
that 
once 
the 
machine 
enters 
its 
accept 
state, 
it 
never 
leaves 
it, 
and 
similarly 
for 
its 
reject 
state; 
that 
is, 
for 
all 
b 
E 
r 
there 
exist 
c, 
c' 
E 
r 
and 
d, 
d' 
E 
{L, 
R} 
such 
that 
6(t,b) 
= 
(t,c,d), 
6(r, 
b) 
= 
(r, 
c', 
d'). 
(28.3) 
We 
sometimes 
reier 
to 
the 
state 
set 
and 
transition 
function 
collectively 
as 
the 
finite 
control. 
Example 
28.1 
Here 
is 
a 
TM 
that 
accepts 
the 
non-context-free 
set 
{anbnc
n 
I 
n 
O}. 
Informally, 
the 
machine 
starts 
in 
its 
start 
state 
s, 
then 
scans 
to 
the 
right 
over 
the 
in 
pu 
t 
string, 
checking 
that 
it 
is 
of 
the 
form 
a 
* 
b* 
c*. 
It 
doesn 
't 
write 
anything 
on 
the 
way 
ac 
ross 
(formally, 
it 
writes 
the 
same 
symbol 
it 
reads). 
When 
it 
sees 
the 
first 
blank 
symbol 
u, 
it 
overwrites 
it 
with 
a 
right 
endmarker 
-1. 
Now 
it 
scans 
left, 
erasing 
the 
first 
c 
it 
sees, 
then 
the 
first 
b 
it 
sees, 
then 
the 
first 
a 
it 
sees, 
until 
it 
comes 
to 
the 
r-. 
It 
then 
scans 
right, 
erasing 
one 
a, 
one 
b, 
and 
one 
c. 
It 
continues 
to 
sweep 
left 
and 
right 
over 
the 
input, 
erasing 
one 
occurrence 
of 
each 
letter 
in 
each 
pass. 
If 
on 
some 
pass 
it 
sees 
at 
least 
one 
occurrence 
of 
one 
of 
the 
letters 
and 
no 
occurrences 
of 
another, 
it 
rejects. 
Otherwise, 
it 
eventually 
erases 
all 
the 
letters 
and 
makes 
one 
pass 
between 
r-
and 
-l 
seeing 
only 
blanks, 
at 
which 
point 
it 
accepts. 
Formally, 
this 
machine 
has 
Q 
= 
{S,ql,'" 
,qlo,t,r}, 

_____________________________________________
212 
Lecture 
28 
8 
ql 
q2 
qs 
q4 
qs 
q6 
q7 
qs 
qg 
E 
= 
{a,b,c}, 
r= 
EU 
{f-,u,-i}. 
There 
is 
nothing 
special 
about 
-i; 
it 
is 
just 
an 
extra 
useful 
symbol 
in 
the 
tape 
alphabet. 
The 
transition 
function 
0 
is 
specified 
by 
the 
following 
table: 
f-
a b 
u 
-i 
(t, 
-, 
-) 
(r, 
-, 
-) 
(r,-,-) 
(q4,u,L) 
(qs,u,L) 
(r,-,-) 
(r, 
-, 
-) 
(qs,u,L) 
(q4,c,L) 
(q4,u,L) 
(r,-,-) 
(q6,u,L) 
(qs,b,L) 
(qs,u,L) 
(q7,f-,R) 
(q6, 
a, 
L) 
(q6,u,L) 
(qS,u, 
R) 
(r,-,-) 
(r, 
-, 
-) 
(q7, 
u, 
R) 
(t,-,-) 
(qs,a,R) 
(q9,u,R) 
(r, 
-, 
-) 
(qs,u,R) 
(r,-,-) 
(qg, 
b, 
R) 
(qlQ,u,R) 
(q9,u,R) 
(r,-,-) 
ql0 
(ql0,c,R) 
(qlQ,u,R) 
(q3,-i,L) 
The 
symbol 
-
in 
the 
table 
above 
means 
"don't 
care." 
The 
transitions 
for 
t 
and 
r 
are 
not 
included 
in 
the 
table-just 
define 
them 
to 
be 
anything 
satisfying 
the 
restrictions 
(28.2) 
and 
(28.3). 
0 
Configurations 
and 
Acceptance 
At 
any 
point 
in 
time, 
the 
read/write 
tape 
of 
the 
Turing 
machine 
M 
contains 
a 
semi-infinite 
string 
of 
the 
form 
yu
W
, 
where 
y 
E 
r* 
(y 
is 
a 
finite-Iength 
string) 
and 
U
W 
denotes 
the 
sem 
i-infinite 
string 
uuuuuuuu 
.... 
(Here 
w 
denotes 
the 
smallest 
infinite 
ordinal. 
) 
Although 
the 
string 
is 
nite, 
it 
always 
has 
a 
finite 
representation, 
since 
all 
but 
finitely 
many 
of 
the 
symbols 
are 
the 
blank 
symbol 
u. 
We 
define 
a 
configu.ration 
to 
be 
an 
element 
of 
Q 
x 
{yu
W 
I 
y 
E 
r*} 
x 
N, 
where 
N 
= 
{O, 
1, 
2, 
... 
}. 
A 
configuration 
is 
aglobai 
state 
giving 
a 
snapshot 
of 
all 
relevant 
information 
about 
a 
TM 
computation 
at 
some 
instant 
in 
time. 
The 
configuration 
(p, 
z, 
n) 
specifies 
a 
current 
state 
p 
of 
the 
finite 
control, 
current 
tape 
contents 
z, 
and 
current 
position 
of 
the 
read/write 
head 
n 
O. 
We 
usually 
denote 
configurations 
by 
a, 
ß, 
"Y. 
The 
start 
configu.ration 
on 
input 
x 
E 
I:* 
is 
the 
configuration 
(8, 
f-xu
W
, 
0). 

_____________________________________________
Turing 
Machines 
and 
Effective 
Computability 
213 
The 
last 
component 
0 
means 
that 
the 
machine 
is 
initially 
scanning 
the 
left 
endmarker 
1-. 
One 
can 
define 
a 
next 
eonfiguration 
relation 
as 
with 
PDAs. 
For 
astring 
M 
Z 
E 
pol, 
let 
Zn 
be 
the 
nth 
symbol 
of 
Z 
(the 
leftmost 
symbol 
is 
zo), 
and 
let 
sb'(z) 
denote 
the 
string 
obtained 
from 
z 
by 
substituting 
b 
for 
Zn 
at 
position 
n. 
For 
example, 
st(1-
ba 
a a 
c 
abc 
a 
.. 
-). 
= 
I-
ba 
abc 
abc 
a 
.... 
The 
relation 
.2..... 
is 
defined 
by 
M 
( ) 
1 
{(q,si:(z),n-l) 
p,z,n 
17 
(q,sb'(z),n 
+ 
1) 
if 
15(p,zn) 
= 
(q,b,L), 
if 
15(p,zn) 
= 
(q,b,R). 
Intuitively, 
if 
the 
tape 
contains 
z 
and 
if 
M 
is 
in 
state 
p 
scanning 
the 
nth 
tape 
cell, 
and 
15 
says 
to 
print 
b, 
go 
left, 
and 
enter 
state 
q, 
then 
after 
that 
step 
the 
tape 
will 
contain 
si:(z), 
the 
head 
will 
be 
scanning 
the 
n 
-1st 
tape 
cell, 
and 
the 
new 
state 
will 
be 
q. 
We 
define 
the 
reflexive 
transitive 
closure 
of 
inductively, 
as 
usual: 
M M 
o 
Ł 
M 
n+I. 
nIß 
f 
Ł a 
ß 
If 
a 
'Y 
or 
some 
'Y, 
and 
M 
M M 
Ł a 
ß 
if 
a 
ß 
for 
some 
n 
> 
O. 
M M 
-
The 
machine 
M 
is 
said 
to 
aeeept 
input 
X 
E 
if 
(s,1-
XL,lw,O) 
-i! 
(t,y,n) 
for 
some 
y 
and 
n, 
and 
rejeet 
x 
if 
(8,1-
XUW,O) 
(r,y,n) 
M 
for 
some 
y 
and 
n. 
It 
is 
said 
to 
halt 
on 
input 
x 
if 
it 
either 
accepts 
x 
or 
rejects 
x. 
As 
with 
PDAs, 
this 
is 
just 
a 
mathematical 
definition; 
the 
machine 
doesn't 
really 
grind 
to 
a 
halt 
in 
the 
literal 
sense. 
It 
is 
possible 
that 
it 
neither 
accepts 
nor 
rejects, 
in 
which 
case 
it 
is 
said 
to 
loop 
on 
input 
x. 
A 
Turing 
machine 
is 
said 
to 
be 
total 
if 
it 
halts 
on 
all 
inputs; 
that 
is, 
if 
for 
all 
inputs 
it 
either 
accepts 
or 
rejects. 
The 
set 
L(M) 
denotes 
the 
set 
of 
strings 
accepted 
by 
M. 
We 
call 
a 
set 
of 
strings 
Ł 
reeursively 
enumerable 
(r.e.) 
if 
it 
is 
L(M) 
for 
some 
Turing 
machine 
M, 
Ł 
eo-r.e. 
if 
its 
complement 
is 
r.e., 
and 

_____________________________________________
214 
Lecture 
28 
Ł 
recursive 
if 
it 
is 
L( 
M) 
for 
some 
total 
Turing 
machine 
M. 
In 
common 
parlance, 
the 
term 
"recursive" 
usually 
refers 
to 
an 
algorithm 
that 
calls 
itself. 
The 
definition 
above 
has 
not 
hing 
to 
do 
with 
this 
usage. 
used 
here, 
it 
is 
just 
a 
name 
for 
a 
set 
accepted 
by 
a 
Turing 
machine 
that 
always 
halts. 
We 
will 
see 
lots 
of 
examples 
next 
time. 
Historical 
Notes 
Church's 
thesis 
is 
often 
referred 
to 
as 
the 
Church-Turing 
thesis, 
although 
Alonzo 
Church 
was 
the 
first 
to 
formulate 
it 
explicitly 
[25]. 
The 
thesis 
was 
based 
on 
Church 
and 
Kleene's 
observation 
that 
the 
>.-calculus 
and 
the 
recursive 
functions 
of 
Gödel 
and 
Herbrand 
were 
computationally 
equivalent 
[25]. 
Church 
was 
apparently 
unaware 
of 
Turing's 
work 
at 
the 
time 
of 
the 
writing 
of 
[25], 
or 
if 
he 
was, 
he 
failed 
to 
mention 
it. 
Turing, 
on 
the 
other 
hand, 
cited 
'Church's 
paper 
[25] 
explicitly 
in 
[120], 
and 
apparently 
ered 
his 
machines 
to 
be 
a 
much 
more 
compelling 
definition 
of 
ity. 
In 
an 
appendix 
to 
[120], 
Turing 
outlined 
a 
proof 
of 
the 
computational 
equivalence 
of 
Turing 
machines 
and 
the 
>.-calculus. 

_____________________________________________
Lecture 
29 
More 
on 
Turing 
Machines 
Last 
time 
we 
defined 
deterministic 
one-tape 
Turing 
machines: 
M 
= 
(Q, 
E, 
r, 
u, 
6, 
. 
stateJ 
r 
1 
mput 
alphabet 
tape 
alphabet 
s, 
t, 
r) 
1 
1 
treject 
state 
accept 
state 
start 
state 
left 
endmarker 
transition 
function 
blank 
symbol 
In 
each 
step, 
based 
on 
the 
current 
tape 
symbol 
it 
is 
reading 
and 
its 
current 
state, 
it 
prints 
a 
new 
symbol 
on 
the 
tape, 
moves 
its 
head 
either 
left 
or 
right, 
and 
enters 
a 
new 
state. 
This 
action 
is 
specified 
formally 
by 
the 
transition 
function 
6: 
Q 
x 
r 
-+ 
Q 
x 
r 
x 
{L, 
R}. 

_____________________________________________
216 
Lecture 
29 
Intuitively, 
8(p,a) 
= 
(q,b,d) 
means, 
"When 
in 
state 
p 
scanning 
symbol 
a, 
write 
b 
on 
thai 
tape 
cell, 
move 
the 
head 
in 
direction 
d, 
and 
enter 
state 
q." 
We 
defined 
a 
eonfiguration 
to 
be 
a 
tripIe 
(p, 
z, 
n) 
where 
p 
is 
astate, 
zis 
a 
semi-infinite 
string 
of 
the 
form 
yu
W
, 
y 
E 
E*, 
describing 
the 
contents 
of 
the 
tape, 
and 
n 
is 
a 
natural 
number 
denoting 
a 
tape 
head 
position. 
The 
transition 
function 
8 
was 
used 
to 
define 
the 
next 
eonfiguration 
relation 
on 
configurations 
and 
its 
reflexive 
transitive 
closure 
. 
The 
machine 
M 
M 
M 
aeeepts 
input 
x 
E 
E* 
if 
(s,1-
XUW,O) 
(t,y,n) 
M 
for 
some 
y 
and 
n, 
and 
rejeets 
input 
x 
if 
(s,1-
XUW,O) 
-i; 
(r,y,n) 
for 
some 
y 
and 
n. 
The 
left 
configuration 
above 
is 
the 
start 
eonfiguration 
on 
input 
x. 
Recall 
that 
we 
restricted 
TMs 
so 
that 
once 
a 
TM 
enters 
its 
accept 
state, 
it 
may 
never 
leave 
it, 
and 
similarly 
for 
its 
reject 
state. 
If 
M 
never 
enters 
its 
accept 
or 
r?ject 
state 
on 
input 
x, 
it 
is 
said 
to 
loop 
on 
input 
x. 
It 
is 
said 
to 
halt 
on 
input 
x 
if 
it 
either 
accepts 
or 
rejects. 
A 
TM 
that 
halts 
on 
all 
inputs 
is 
called 
total. 
Define 
the 
set 
L(M) 
= 
{x 
E 
E* 
IM 
accepts 
xl. 
This 
is 
called 
the 
set 
aeeepted 
by 
M.-
A 
subset 
of 
E* 
is 
called 
reeursively 
enumerable 
(r.e.) 
if 
it 
is 
L(M) 
for 
some 
M. 
A 
set 
is 
called 
recursive 
if 
it 
is 
L(M) 
for 
some 
total 
M. 
For 
now, 
the 
terms 
r.e. 
and 
reeursive 
are 
just 
technical 
terms 
describing 
the 
sets 
accepted 
by 
TMs 
and 
total 
TMs, 
respectively; 
they 
have 
no 
other 
significance. 
We 
will 
discuss 
the 
origin 
of 
this 
terminology 
in 
Lecture 
30. 
Example 
29.1 
Consider 
the 
non-CFL 
{ww 
I 
w 
E 
{a,b}*}. 
It 
is 
a 
recursive 
set, 
because 
we 
can 
give 
a 
total 
TM 
M 
for 
it. 
The 
machine 
M 
works 
as 
folIows. 
On 
input 
x, 
it 
scans 
out 
to 
the 
first 
blank 
symbol 
u, 
counting 
the 
number 
of 
symbols 
mod 
2 
to 
make 
sure 
x 
is 
of 
even 
length 
and 
rejecting 
immediately 
if 
not. 
It 
lays 
down 
a 
right 
endmarker 
-1, 
then 
repeatedly 
scans 
back 
and 
forth 
over 
the 
input. 
In 
each 
pass 
from 
right 
to 
left, 
it 
marks 
the 
first 
unmarked 
a 
or 
bit 
sees 
with 
'. 
In 
each 
pass 
from 
left 
to 
right, 
it 
marks 
the 
first 
unmarked 
a 
or 
b 
it 
sees 
with 
'. 
It 
continues 
this 
until 
all 
symbols 
are 
marked. 
For 
example, 
on 
input 
aabbaaabba 
the 
initial 
tape 
contents 
are 
I-aabbaaabbauuu··· 

_____________________________________________
More 
on 
Turing 
Machines 
217 
and 
the 
following 
are 
the 
tape 
contents 
after 
the 
first 
few 
passes. 
I-aab 
b 
aaab 
b 
a-luuu··· 
I-aab 
b 
aaab 
b 
a-luuu··· 
I-aab 
b 
aaab 
b 
a-luuu··· 
I-aab 
b 
aaab 
b 
I-aab 
b 
aaab 
b 
a-luuu··· 
Marking 
a 
with 
' 
formally 
means 
writing 
the 
symbol 
a 
E 
rj 
thus 
r 
= 
{a, 
b, 
1-, 
u, 
-I, 
a, 
b, 
a, 
b}. 
When 
all 
symbols 
are 
marked, 
we 
have 
the 
first 
half 
of 
the 
input 
string 
marked 
with 
' 
and 
the 
second 
half 
marked 
with 
I-aab 
b 
aaab 
b 
a-luuu··· 
The 
reason 
we 
did 
this 
was 
to 
find 
the 
center 
of 
the 
input 
string. 
The 
machine 
then 
repeatedly 
scans 
left 
to 
right 
over 
the 
input. 
In 
each 
pass 
it 
erases 
the 
first 
symbol 
it 
sees 
marked 
with 
' 
but 
remembers 
that 
symbol 
in 
its 
finite 
control 
(to 
"erase" 
really 
means 
to 
write 
the 
blank 
symbol 
u). 
It 
then 
scans 
forward 
until 
itsees 
the 
first 
symbol 
marked 
with 
" 
checks 
that that 
symbol 
is 
the 
same, 
and 
erases 
it. 
If 
the 
two 
symbols 
are 
not 
the 
same, 
it 
rejects. 
Otherwise, 
when 
it 
has 
erased 
all 
the 
symbols, 
it 
accepts. 
In 
our 
example, 
the 
following 
would 
be 
the 
tape 
contents 
after 
each 
pass. 
aabbaaabba-luui.J··· 
uabbauabba-luuu··· 
uub 
b 
a 
uub 
b 
a 
-Iuuu··· 
uuub 
a 
uuub 
a 
-1uuu··· 
uuuua 
uuuua 
-Iuuu··· 
uuuuuuuuuu-luuu·· 
. 
o 
Example 
29.2 
We 
want 
to 
construct 
a 
total 
TM 
that 
accepts 
its 
input 
striIig 
if 
the 
length 
of 
the 
string 
is 
prime. 
This 
language 
is 
not 
regular 
or 
context-free. 
We 
will 
give 
a 
TM 
implementation 
of 
the 
sieve 
0/ 
Eratosthenes, 
which 
can 
be 
described 
informally 
as 
folIows. 
Say 
we 
want 
to 
check 
whether 
n 
is 
prime. 
We 
write 
down 
all 
the 
numbers 
from 
2 
to 
n 
in 
order, 
then 
repeat 
the 
following: 
find 
the 
smallest 
number 
in 
the 
list, 
declare 
it 
prime, 
then 
cross 
off 
all 
multiples 
of 
that 
number. 
Repeat 
until 
each 
number 
in 
the 
list 
has 
been 
either 
declared 
prime 
or 
crossed 
off 
as 
a 
multiple 
of 
a 
smaller 
prime. 
For 
example, 
to 
check 
whether 
23 
is 
prime, 
we 
would 
start 
with 
all 
the 
numbers 
from 
2 
to 
23: 
2 3 4 5 6 7 8 9 
10 
11 
12 
13 14 15 
16 17 
18 
19· 
20 
21 
22 
23 

_____________________________________________
218 
Lecture 
29 
In 
the 
first 
pass, 
we 
cross 
off 
multiples 
of 
2: 
X 
3 
X 
5 
)( 
7 
)( 
9 
)<l 
11 
X 
13 
)( 
15 
17 
)«{ 
19 
21 
23 
The 
smallest 
number 
remaining 
is 
3, 
and 
this 
is 
prime. 
In 
the 
second 
pass 
we 
cross 
off 
multiples 
of 
3: 
X 
)( 
X 
5 
)( 
7 
)()()<l 
11 
X 
13 
X 
)fl 
17 
)«{ 
19 
)( 
23 
Then 
5 
is 
the 
next 
prime, 
so 
we 
cross 
off 
multiples 
of 
5; 
and 
so 
forth. 
·Since 
23 
is 
prime, 
it 
will 
never 
be 
crossed 
off 
as 
a 
multiple 
of 
anything 
smaller, 
and 
eventually 
we 
will 
discover 
that 
fact 
when 
everything 
smaller 
has 
been 
crossed 
off. 
Now 
we 
show 
how 
to 
implement 
this 
on 
a 
TM. 
Suppose 
we 
have 
a
P 
written 
on 
the 
tape. 
We 
illustrate 
the 
algorithm 
with 
p 
= 
23. 
I-aa 
aaa 
a 
aa 
a a 
aaa 
aa 
a 
aa aa 
aa 
a 
uuu··· 
If 
p 
= 
0 
or 
p 
= 
1, 
reject. 
We 
can 
determine 
this 
by 
looking 
at 
the 
first 
three 
cells 
of 
the 
tape. 
Otherwise, 
there 
are 
at 
least 
two 
a's. 
Erase 
the 
first 
a, 
scan 
right 
to 
the 
end 
of 
the 
input, 
and 
replace 
the 
last 
a 
in 
the 
input 
string 
with 
the 
symbol 
$. 
We 
now 
have 
an 
a 
in 
positions 
2,3,4, 
... 
,p 
-
1 
and 
$ 
at 
position 
p. 
I-ua 
a a 
aa 
aaa 
aaaa 
aa 
a a a 
aa 
a a 
$ 
uuu··· 
Now 
we 
repeat 
the 
following 
loop. 
Starting 
from 
the 
left 
endmarker 
1-, 
scan 
right 
and 
find 
the 
first 
non 
blank 
symbol, 
say 
occurring 
at 
position 
m. 
Then 
m 
is 
prime 
(this 
is 
an 
invariant 
of 
the 
loop). 
If 
this 
symbol 
is 
the 
$, 
we 
are 
done: 
p 
= 
m 
is 
prime, 
so 
we 
halt 
and 
accept. 
Otherwise, 
the 
symbol 
is 
an 
a. 
Mark 
it 
with 
a 
.... 
and 
everything 
between 
there 
and 
the 
left 
endmarker 
with 
'. 
I-
ua 
a a a a a a a a a a a a a a a a a a a a 
$ 
u u 
u· 
.. 
We 
will 
now 
enter 
an 
inner 
loop 
to 
erase 
a11 
the 
symbols 
occurring 
at 
tions 
that 
are 
multiples 
of 
m. 
First, 
erase 
the 
a 
under 
the 
"'. 
(Formally, 
just 
write 
the 
symbol 
u.) 
I-uuaa 
a a 
aaa 
aa aa 
aa 
a a a 
aa aa 
$ 
uuu··· 
Shift 
the 
marks 
to 
the 
right 
one 
at 
a 
time 
a 
distance 
equal 
to 
the 
number 
of 
marks. 
This 
can 
be 
done 
by 
shuttling 
back 
and 
forth, erasing 
marks 
on 
the 
left 
and 
writing 
them 
on 
the 
right. 
We 
know 
when 
we 
are 
done 
because 
the 
.... 
is 
the 
last 
mark 
mOJled. 
I-uuaa 
aaa 
aa aa 
a a 
aa 
a a a 
aa aa 
$ 
uuu··· 
When 
this 
is 
done, 
erase 
the 
symbol 
under 
the 
..... 
This 
is 
the 
symbol 
occurring 
at 
position 
2m. 
I-uua 
ua 
a a a a a a a a a a a a a a a a a 
$ 
uuu··· 

_____________________________________________
220 
Lecture 
29 
M' 
simultaneously 
on 
two 
different 
tracks 
of 
its 
tape. 
Formally, 
the 
tape 
alphabet 
of 
N 
contains 
symbols 
where 
a 
is 
a 
tape 
symbol 
of 
M 
and 
c 
is 
a 
tape 
symbol 
of 
M'. 
Thus. 
N's 
tape 
may 
contain 
astring 
of 
the 
form 
b 
a 
b 
a 
b 
a 
b 
a 
c c c 
d 
d 
c 
C 
d 
for 
example. 
The 
extra 
marks 
are 
placed 
on 
the 
tape 
to 
indicate 
the 
rent 
positions 
ofthe 
simulated 
read/write 
heads 
of 
M 
and 
M'. 
The 
machine 
N 
alternately 
performs 
a 
step 
of 
M 
and 
a 
step 
of 
M', 
shuttling 
back 
and 
forth 
between 
the 
two 
simulated 
tape 
head 
positions 
of 
M 
and 
M' 
and 
dating 
the 
tape. 
The 
current 
states 
and 
transition 
information 
of 
M 
and 
M' 
can 
be 
stored 
in 
N's 
finite 
control. 
If 
the 
machine 
M 
ever 
accepts, 
then 
N 
immediately 
accepts. 
If 
M' 
ever 
accepts, 
then 
N 
immediately 
rejects. 
actly 
one 
of 
those 
two 
events 
must 
eventually occur, 
depending 
on 
whether 
x 
E 
A 
or 
xE'" 
A, 
since 
L(M) 
= 
A 
and 
L(M
'
) 
= 
'" 
A. 
Then 
N 
halts 
on 
all 
inputs 
and 
L(N) 
= 
A. 
Decidability 
and 
Semidecidability 
A 
property 
P 
of 
strings 
is 
said 
to 
be 
decidable 
if 
the 
set 
of 
all 
strings 
having 
property 
P 
is 
a 
recursive 
set; 
that 
is, 
if 
there 
is 
a 
total 
Turing 
machine 
that 
accepts 
input 
strings 
that 
have 
property 
P 
and 
rejects 
those 
that 
do 
not. 
A 
property 
P 
is 
said 
to 
be 
semidecidable 
if 
the 
set 
of 
strings 
having 
property 
Pis 
an 
r.e. 
set; 
that 
is, 
if 
there 
is 
a 
Turing 
machine 
that 
on 
input 
x 
accepts 
if 
x 
has 
property 
P 
and 
rejects 
or 
loops 
if 
not. 
For 
example, 
it 
is 
decidable 
whether 
a 
given 
string 
x 
is 
of 
the 
form 
WW, 
because 
we 
can 
construct 
a 
Turing 
machine 
that 
halts 
on 
all 
inputs 
and 
accepts 
exactly 
the 
strings 
of 
this 
form. 
Although 
you 
often 
hear 
them 
switched, 
the 
adjectives 
recursive 
and 
r.e. 
are 
best 
applied 
to 
sets 
and 
decidable 
and 
semidecidable 
to 
properties. 
The 
two 
notions 
are 
equivalent, 
since 
P 
is 
decidable 
{:::::} 
{x 
I 
P( 
x)} 
is 
recursive. 
A 
is 
recursive 
{:::::} 
"x 
E 
A" 
is 
decidable, 
Pis 
semidecidable 
{:::::} 
{x 
I 
P(x)} 
is 
r.e., 
A 
is 
r.e. 
{:::::} 
"x 
E 
A" 
is 
semidecidable. 

_____________________________________________
Lecture 
30 
Equivalent 
Models 
As 
mentioned, 
the 
concept 
of 
computability 
is 
remarkably 
robust. 
As 
idence 
of 
this, 
we 
will 
present 
several 
different 
flavors 
of 
Turing 
machines 
that 
at 
first 
glance 
appear 
to 
be 
significantly 
more 
or 
less 
powerful 
than 
the 
basic 
model 
defined 
in 
Lecture 
29 
but 
in 
fact 
are 
computationally 
equivalent. 
Multiple 
Tapes 
First, 
we 
show 
how 
to 
simulate 
multitape 
Turing 
machines 
with 
tape 
Turing 
machines. 
Thus 
extra 
tapes 
don't 
add 
any 
power. 
A 
three-tape 
machine 
is 
similar 
to 
a 
one-tape 
TM 
except 
that 
it 
has 
three 
semi-infinite 
tapes 
and 
three 
independent 
read/write 
heads. 
Initially, 
the 
input 
occupies 
the 
first 
tape 
and 
the 
other 
two 
are 
blank. 
In 
each 
step, 
the 
machine 
reads 
the 
three 
symbols 
under 
its 
heads, 
and 
based 
on 
this 
information 
and 
the 
current 
state, 
it 
prints 
a 
symbol 
on 
each 
tape, 
moves 
the 
heads 
(they 
don't 
all 
have 
to 
move 
in 
the 
same 
direction), 
and 
enters 
a 
new 
state. 

_____________________________________________
224 
Lecture 
30 
The 
bot 
tom 
track 
ia 
used 
to 
simulate 
the 
original 
machine 
when 
ita 
head 
ia 
to 
the 
right 
of 
the 
fold, 
and 
the 
top 
track 
is 
uaed 
to 
simulate 
the 
machine 
when 
its 
head 
is 
to 
the 
left 
of 
the 
fold, 
moving 
in 
the 
opposite 
direction. 
Two 
Stacks 
A 
machine 
with 
a 
two-way, 
read-only 
input 
head 
and 
two 
stacks 
ia 
as 
erful 
as 
a 
Thring 
machine. 
Intuitively, 
the 
computation 
of 
a 
one-tape 
TM 
can 
be 
simulated 
with 
two 
stacks 
by 
storing 
the 
tape 
contents 
to 
the 
left 
of 
the 
head 
on 
one 
stack 
and 
the 
tape 
contenta 
to 
the 
right 
of 
the 
head 
on 
the 
other 
stack. 
The 
motion 
of 
the 
head 
is 
simulated 
by 
popping 
a 
symbol 
off 
one 
stack 
and 
pushing 
it 
onto 
the 
other. 
For 
example, 
I 
r 
I 
alb 
I 
a 
I 
alb 
I 
alb 
I 
b 
I 
b 
I 
b 
I 
b 
I 
b 
I 
b 
la 
la 
I 
alb 
I 
f 
is 
simulated 
by 
I 
r 
I 
alb 
I 
a 
I 
alb 
I 
alb 
I 
b 
I 
b 
I 
f 
stack 
1 
Counter 
Automata 
A 
k-counter 
automaton 
is 
a 
machine 
equipped 
with 
a 
two-way 
read-only 
input 
head 
and 
k 
integer 
counters. 
Each 
counter 
can 
store 
an 
arbitrary 
negative 
integer. 
In 
each 
step, 
the 
automaton 
can 
independently 
increment 
or 
decrement 
its 
counters 
and 
test 
them 
for 
0 
and 
can 
move 
its 
input 
head 
one 
cell 
in 
either 
direction. 
It 
cannot 
write 
on 
the 
tape. 
A 
stack 
can 
be 
simulated 
with 
two 
counters 
as 
folIows. 
We 
can 
assume 
without 
10ss 
of 
generality 
that 
the 
stack 
alphabet 
of 
the 
stack 
to 
be 
lated 
contains 
only 
two 
symbols, 
say 
0 
and 
1. 
This 
is 
because 
we 
can 
encode 
finitely 
many 
stack 
symbols 
as 
binary 
numbers 
of 
fixed 
length, 
say 
mj 
then 
pushing 
or 
popping 
one 
stack 
symbol 
is 
simulated 
by 
pushing 
or 
popping 
m 
binary 
digits. 
Then 
the 
contents 
of 
the 
stack 
can 
be 
regarded 
as 
a 
binary 
number 
whose 
least 
significant 
bit 
is 
on 
top 
of 
the 
stack. 
The 
simulation 
maintains 
this 
number 
in 
the 
first 
of 
the 
two 
counters 
and 
uses 
the 
second 
to 
effect 
the 
stack 
operations. 
To 
simulate 
pushing 
a 0 
onto 
the 
stack, 
we 
need 
to 
double 
the 
value 
in 
the 
first 
counter. 
This 
is 
done 
by 
entering 
a 
loop 
that 
repeatedly 
subtracts 
one 
from 
the 
first 
counter 
and 
adds 
two 
to 
the 

_____________________________________________
Equivalent 
Models 
225 
second 
until 
the 
first 
counter 
is 
O. 
The 
value 
in 
the 
second 
counter 
is 
then 
twice 
the 
original 
value 
in 
the 
first 
counter. 
We 
can 
then 
transfer 
that 
value 
back 
to 
the 
first 
counter, 
or 
just 
switch 
the 
roles 
of 
the 
two 
counters. 
To 
push 
1, 
the 
operation 
is 
the 
same, 
except 
the 
value 
of 
the 
second 
counter 
is 
incremented 
once 
at 
the 
end. 
To 
simulate 
popping, 
we 
need 
to 
divide 
the 
counter 
value 
by 
tWOj 
this 
is 
done 
by 
decrementing 
one 
counter 
while 
incrementing 
the 
other 
counter 
hery 
second 
step. 
Testing 
the 
parity 
of 
the 
original 
counter 
cor:tents 
tells 
whether 
a 
simulated 
1 
or 
0 
was 
popped. 
Since 
a 
two-stack 
machine 
can 
simulate 
an 
arbitrary 
TM, 
and 
since 
two 
counters 
can 
simulate 
a 
stack, 
it 
follows 
that 
a 
four-counter 
automaton 
can 
simulate 
an 
arbitrary 
TM. 
However, 
we 
can 
do 
even 
better: 
a 
two-counter 
automaton 
can 
simulate 
a 
four-counter 
automaton. 
When 
the 
four-counter 
automaton 
has 
the 
ues 
i, 
j, 
k, 
l 
in 
its 
counters, 
the 
two-counter 
automaton 
will 
have 
the 
value 
2
i
3
i 
5
k
7
l 
in 
its 
first 
counter. 
It 
uses 
its 
second 
counter 
to 
effect 
the 
counter 
operations 
of 
the 
four-counter 
automaton. 
For 
example, 
if 
the 
four·counter 
automaton 
wanted 
to 
add 
one 
to 
k 
(the 
value 
of 
the 
third 
counter), 
then 
the 
two-counter 
automaton 
would 
have 
to 
multiply 
the 
value 
in 
its 
first 
counter 
by 
5. 
This 
is 
done 
in 
the 
same 
way 
as 
above, 
adding 
5 
to 
the 
second 
counter 
for 
every 
1 
we 
subtract 
from 
the 
first 
counter. 
To 
simulate 
a 
test 
for 
zero, 
the 
two-counter 
automaton 
has 
to 
determine 
whether 
the 
value 
in 
its 
first 
counter 
is 
divisible 
by 
2, 
3, 
5, 
or 
7, 
respectively, 
depending 
on 
which 
counter 
of 
the 
four-counter 
automaton 
is 
being 
tested. 
Combining 
these 
simulations, 
we 
see 
that 
two-counter 
automata 
are 
as 
erful 
as 
arbitrary 
Turing 
machines. 
However, 
as 
you 
can 
imagine, 
it 
takes 
an 
enormous 
number 
of 
steps 
of 
the 
two-counter 
automaton 
to 
simulate 
one 
step 
of 
the 
Turing 
machine. 
One-counter 
automata 
are 
not 
as 
powerful 
as 
arbitrary 
TMs, 
although 
they 
can 
accept 
non-CFLs. 
For 
exarfiple, 
the 
set 
{anbnc
n 
I 
n 
O} 
can 
be 
accepted 
by 
a 
one-counter 
automaton. 
Enumeration 
Machines 
We 
defined 
the 
recursively 
enumerable 
(r.e.) 
sets 
to 
be 
those 
sets 
accepted 
by 
Turing 
machines. 
The 
term 
recursively 
enumerable 
comes 
from 
a 
different 
but 
equivalent 
formalism 
embodying 
the 
idea 
that 
the 
elements 
of 
an 
r.e. 
set 
can 
be 
enumerated 
one 
at 
a 
time 
in 
a 
mechanical 
fashion. 
Define 
an 
enumeration 
machme 
as 
follow8. 
It 
has 
a 
finite 
control 
and 
two 
tapes, 
a 
read/write 
work 
tape 
and 
a 
write-only 
output 
tape. 
The 
work 
tape 
head 
can 
move 
in 
either 
direction 
and 
can 
read 
and 
write 
any 
element 
of 
r. 
The 
output 
tape 
head 
moves 
right 
one 
cell 
when 
it 
writes 
a. 
symbol, 
a.nd 

_____________________________________________
Lecture 
31 
Universal 
Machines 
and 
Diagonalization 
A 
Universal 
Turing 
Machine 
Now 
we 
come 
to 
a 
crucial 
observation 
about 
the 
power 
of 
Turing 
machines: 
there 
exist 
Turing 
machines 
that 
can 
other 
Turing 
machines 
whose 
descriptions 
are 
presented 
as 
part 
of 
the 
input. 
There 
is 
nothing 
mysterious 
about 
this; 
it 
is 
the 
same 
as 
writing 
a 
LISP 
interpreter 
in 
LISP. 
First 
we 
need 
to 
fix 
a 
reasonable 
encoding 
scheme 
for 
Turing 
machines 
over 
the 
alphabet 
{O, 
I}. 
This 
encoding 
scheme 
should 
be 
simple 
enough 
that 
all 
the 
data 
associated 
with 
a 
machine 
M 
-the 
set 
of 
states, 
the 
transition 
function, 
the 
input 
and 
tape 
alphabets, 
the 
endmarker, 
the 
blank 
symbol, 
and 
the 
start, 
accept, 
and 
reject 
states-can 
be 
determined 
easily 
by 
other 
machine 
reading 
the 
encoded 
description 
of 
M. 
For 
example, 
if 
the 
string 
begins 
with 
the 
prefix 
onlO
m
lO
k
lO"1O!lOr'lO
u
IOtl1, 
this 
might 
indicate 
that 
the 
machine 
has 
n 
states 
represented 
by 
the 
bers 
0 
to 
n 
-
1; 
it 
has 
m 
tape 
symbols 
represented 
by 
the 
numbers 
0 
to 
m -
1, 
of 
which 
the 
first 
k 
represent 
input 
symbols; 
the 
start, 
accept, 
and 
reject 
states 
are 
s, 
t, 
and 
T, 
respectively; 
and 
the 
end 
marker 
and 
blank 
symbol 
are 
u 
and 
v, 
respectively. 
The 
remainder 
of 
the 
string 
can 
consist 

_____________________________________________
Universal 
Machines 
and 
Diagonalization 
229 
of 
a 
sequence 
of 
substrings 
specifying 
the 
transitions 
in 
C. 
For 
example, 
the 
substring 
OPIO"lOqlOblO 
might 
indicate 
that 
6 
contains 
the 
transition 
((p,a), 
(q,b,L)), 
the 
direction 
to 
move 
the 
head 
encoded 
by 
the 
final 
digit. 
The 
exact 
details 
of 
the 
encoding 
scheme 
are 
not 
important. 
The 
only 
requirements 
are 
that 
it 
should 
be 
easy 
to 
interpret 
and 
able 
to 
encode 
a11 
Turing 
machines 
up 
to 
isomorphism. 
Once 
we 
have 
a 
suitable 
encoding 
of 
Turing 
machines, 
we 
can 
construct 
a 
universal 
Turing 
machine 
U 
such 
that 
L(U) 
I 
x 
E 
L(M)}. 
In 
other 
words, 
presented 
with 
(an 
encoding 
over 
{O,l} 
of) 
a 
Turing 
chine 
M 
and 
(an 
encoding 
over 
{O, 
l} 
of) 
astring 
x 
over 
M's 
input 
bet, 
the 
machine 
U 
accepts 
M#x 
iff 
M 
accepts 
x.
1 
The 
symbol 
# 
is 
just 
a 
symbol 
in 
U's 
input 
alphabet 
other 
than 
0 
or 
1 
used 
to 
delimit 
M 
and 
x. 
The 
machine 
U 
first 
checks 
its 
input 
M 
#x 
to 
make 
sure 
that 
M 
is 
a 
valid 
encoding 
of 
a 
Turing 
machine 
and 
x 
is 
a 
valid 
encoding 
of 
astring 
over 
M's 
input 
alphabet. 
If 
not, 
it 
immediately 
rejects. 
If 
the 
encodings 
of 
M 
and 
x 
are 
valid, 
the 
machine 
U 
does 
a 
step-by-step 
simulation 
of 
M. 
This 
might 
work 
as 
fo11ows. 
The 
tape 
of 
U 
is 
partitioned 
into 
three 
tracks. 
The 
description 
of 
M 
is 
copied 
to 
the 
top 
track 
and 
the 
string 
x 
to 
the 
middle 
track. 
The 
middle 
track 
will 
be 
used 
to 
hold 
the 
simulated 
contents 
of 
M's 
tape. 
The 
bottom 
track 
will 
be 
used 
to 
remember 
the 
current 
state 
of 
M 
and 
the 
current 
position 
of 
M's 
readfwrite 
head. 
The 
machine 
U 
then 
simulates 
M 
on 
input 
x 
one 
step 
at 
a 
time, 
shuttling 
back 
and 
forth 
between 
the 
description 
of 
M 
on 
its 
top 
track 
and 
the 
simulated 
contents 
of 
M's 
tape 
on 
the 
middle 
track. 
In 
each 
step, 
it 
updates 
M's 
state 
and 
simulated 
tape 
contents 
as 
dictated 
by 
M's 
transition 
function, 
which 
U 
can 
read 
from 
the 
description 
of 
M. 
If 
ever 
M 
halts 
and 
accepts 
or 
halts 
and 
rejects, 
then 
U 
does 
the 
same. 
As 
we 
have 
observed, 
the 
string 
x 
over 
the 
input 
alphabet 
of 
M 
and 
its 
encoding 
over 
the 
input 
alphabet 
of 
U 
are 
two 
different 
things, 
since 
the 
two 
machines 
may 
have 
different 
input 
alphabets. 
If 
the 
input 
alphabet 
of 
1 
Note 
that 
we 
are 
using 
the 
metasymbol 
M 
for 
both 
a 
Turing 
machine 
and 
its 
encoding 
over 
{O, 
I} 
and 
the 
metasymbol 
x 
for 
both 
astring 
over 
M's 
input 
alphabet 
and 
its 
encoding 
over 
{O, 
I}. 
This 
is 
for 
notational 
convenience. 

_____________________________________________
232 
Lecture 
31 
The 
machine 
U 
doesn't 
do 
any 
faney 
analysis 
on 
the 
machine 
M 
to 
try 
to 
determine 
whether 
or 
not 
it 
will 
halt. 
It 
just 
blindly 
simulates 
M 
step 
by 
step. 
If 
M 
doesn't 
halt 
on 
x, 
then 
U 
will 
just 
go 
on 
happily 
simulating 
M 
forever. 
It 
is 
natural 
to 
ask 
wh 
ether 
we 
ean 
do 
better 
than 
just 
a 
blind 
simulation. 
Might 
there 
be 
some 
way 
to 
analyze 
M 
to 
determine 
in 
advanee, 
before 
doing 
the 
simulation, 
whether 
M 
would 
eventually 
halt 
on 
x? 
If 
U 
could 
say 
for 
sure 
in 
advance 
that 
M 
would 
not 
halt 
on 
x, 
then 
it 
could 
skip 
the 
simulation 
and 
save 
itself 
a 
lot 
of 
useless 
work. 
On 
the 
other 
hand, 
if 
U 
could 
ascertain 
that 
M 
would 
eventually 
halt 
on 
x, 
then 
it 
could 
go 
ahead 
with 
the 
simulation 
to 
determine 
whether 
M 
accepts 
or 
rejects. 
We 
could 
then 
build 
a 
machine 
U' 
that 
takes 
as 
input 
an 
encoding 
of 
a 
Turing 
machine 
M 
and 
astring 
x, 
and 
Ł 
halts 
and 
accepts 
if 
M 
halts 
and 
accepts 
x, 
Ł 
halts 
and 
rejects 
if 
M 
halts 
and 
rejects 
,x, 
and 
Ł 
halts 
and 
rejects 
if 
M 
loops 
on 
x. 
This 
would 
say 
that 
L(U') 
= 
L(U) 
= 
MP 
is 
a 
recursive 
set. 
U 
nfortunately, this 
is 
not 
possible 
in 
general. 
There 
are 
certainly 
machines 
for 
whieh 
it 
is 
possible 
to 
determine 
halting 
by 
some 
heuristic 
or 
other: 
machines 
for 
which 
the 
start 
state 
is 
the 
accept 
state, 
for 
example. 
However, 
there 
is 
no 
general 
method 
that 
gives 
the 
right 
answer 
for 
all 
machines. 
We 
can 
prove 
this 
using 
Cantor's 
diagonallzation 
technique. 
For 
x 
E 
{O, 
I} 
* , 
let 
M
z 
be 
the 
Turing 
machine 
with 
input 
alphabet 
{O, 
I} 
whose 
encoding 
over 
{O, 
1}* 
is 
x. 
(If 
x 
is 
not 
a 
legal 
description 
of 
a 
TM 
with 
input 
alphabet 
{O, 
I} 
* 
according 
to 
our 
encoding 
scheme, 
we 
take 
M
z 
to 
be 
some 
arbitrary 
but 
fixed 
TM 
with 
input 
alphabet 
{O, 
1}, 
say 
a 
trivial 
TM 
with 
one 
state 
that 
immediately 
halts.) 
In 
this 
way 
we 
get 
a 
list 
(31.1) 
containing 
aJ.l 
possible 
Turing 
machines 
with 
input 
alphabet 
{O, 
1} 
indexed 
by 
strings 
in 
{0,1}*. 
We 
make 
sure 
that 
the 
encoding 
scheme 
is 
simple 
enough 
that 
a 
universal 
machine 
can 
determine 
M. 
from 
x 
for 
the 
purpose 
of 
simulation. 
Now 
consider 
an 
infinite 
two·dimensional 
matrix 
indexed 
along 
the 
top 
by 
strings 
in 
{O,l}* 
and 
down 
the 
left 
by 
TMs 
in 
the 
list 
(31.1). 
The 
matrix 

_____________________________________________
Universal 
Machines 
and 
Diagonalization 
233 
contains 
an 
H 
in 
position 
x, 
y 
if 
M
z 
halts 
on 
input 
y 
and 
an 
L 
if 
M
z 
loops 
on 
input 
y. 
f 
0 1 
00 
01 
10 
11 
000 
001 
010 
M. 
H L L H 
H 
L 
H 
L 
H 
H 
Mo 
L 
L 
H 
H 
L 
H H 
L L 
H 
MI 
L 
H H 
L 
L 
L 
H H 
L 
H 
Mo
o 
L 
H 
L 
H 
H 
L 
H 
H 
L L 
MOl 
H 
L 
H 
L L 
H 
L 
L 
H H 
M
lO 
H 
L 
H H 
L 
H 
H 
H 
L 
H 
Mn 
L 
L 
H 
L 
H 
H 
L 
L 
H 
H 
Mooo 
H H H 
L 
H 
H H 
L 
H 
L 
Moo
l 
L 
L 
H 
L L L L 
H H L 
Mo
1O 
H H 
L L 
H 
L L 
H 
L L 
The 
xth 
row 
of 
the 
matrix 
describes 
for 
each 
input 
string 
y 
whether 
or 
not 
M
z 
halts 
on 
y. 
For 
example, 
in 
the 
above 
picture, 
M. 
halts 
on 
inputs 
f, 
00, 
01,11,001,010, 
... 
and 
does 
not 
halt 
on 
inputs 
0,1,10,000, 
.... 
Suppose 
(for 
a 
contradiction) 
that 
there 
existed 
a 
total 
machine 
K 
accepting 
the 
set 
HP; 
that 
is, 
a 
machine 
that 
for 
any 
given 
x 
and 
y 
could 
determine 
the 
x, 
yth 
entry 
of 
the 
above 
table 
in 
finite 
time. 
Thus 
on 
input 
M 
#x, 
Ł K 
halts 
and 
accepts 
if 
M 
halts 
on 
x, 
and 
Ł K 
halts 
and 
rejects 
if 
M 
loops 
on 
x. 
Consider 
a 
machine 
N 
that 
on 
input 
x 
E 
{O,1}* 
(i) 
constructs 
M
z 
from 
x 
and 
writes 
Mz#x 
on 
its 
tape; 
(ii) 
runs 
K 
on 
input 
Mz#x, 
accepting 
if 
K 
rejects 
and 
going 
into 
a 
trivial 
loop 
if 
K 
accepts. 
Note 
that 
N 
is 
essentially 
complementing 
the 
diagonal 
of 
the 
above 
matrix. 
Then 
for 
any 
xE 
{0,1}*, 
N 
halts 
on 
x 
{::::::} 
K 
rejects 
Mz#x 
definition 
of 
N 
{::::::} 
M
z 
loops 
on 
x 
assumption 
about 
K. 
This 
says 
that 
N's 
behavior 
is 
different 
from 
every 
M
z 
on 
at 
least 
one 
string, 
namely 
x. 
But 
the 
list 
(31.1) 
was 
supposed 
to 
contain 
aB 
Turing 
machines 
over 
the 
input 
alphabet 
{O, 
1}, 
including 
N. 
This 
is 
a 
contradiction. 
0 
The 
fallacious 
assumption 
that 
led 
to 
the 
contradiction 
was 
that 
it 
was 
possible 
to 
determine 
the 
entries 
of 
the 
matrix 
effectively; 
in 
other 
words, 

_____________________________________________
Lecture 
32 
Decidable 
and 
Undecidable 
Problems 
Here 
are 
some 
examples 
of 
decision 
problems 
involving 
Turing 
machines. 
Is 
it 
decidable 
whether 
a 
given 
Turing 
machine 
(a) 
has 
at 
least 
481 
states? 
(b) 
takes 
more 
than 
481 
steps 
on 
input 
•? 
(c) 
takes 
more 
than 
481 
steps 
on 
80me 
input? 
(d) 
takes 
more 
than 
481 
steps 
on 
all 
inputs? 
(e) 
ever 
moves 
its 
head 
more 
than 
481 
tape 
cells 
away 
from 
the 
left 
endmarker 
on 
input 
•7 
(f) 
accepts 
the 
null 
string 
•? 
(g) 
accepts 
any 
string 
at 
all? 
(h) 
accepts 
every 
string? 
(i) 
accepts 
a 
finite 
set? 
(j) 
accepts 
a 
regular 
set? 
(k) 
accepts 
a 
CFL? 
(I) 
accepts 
a 
recursive 
set? 

_____________________________________________
236 
Lecture 
32 
(m) 
is 
equivalent 
to 
a 
Turing 
machine 
with 
a 
shorter 
description? 
Problems 
(a) 
through 
(e) 
are 
decidable 
and 
problems 
(f) 
through 
(m) 
are 
undecidable 
(proofs 
below). 
We 
will 
show 
that 
problems 
(f) 
through 
(1) 
are 
undecidable 
by 
showing 
that 
a 
procedure 
for 
one 
of 
these 
problems 
could 
be 
used 
to 
construct 
adecision 
procedure 
for 
the 
halting 
problem, 
which 
we 
know 
is 
impossible. 
Problem 
(m) 
is 
a 
little 
more 
diflicult, 
and 
we 
will 
leave 
that 
as 
an 
exercise 
(Miscellaneous 
Exercise 
131). 
Translated 
into 
modern 
terms, 
problem 
(m) 
is 
the 
same 
as 
determining 
whether 
there 
exists 
a 
shorter 
PASCAL 
program 
equivalent 
to 
a 
given 
one. 
The 
best 
way 
to 
show 
that 
a 
problem 
is 
decidable 
is 
to 
give 
a 
total 
Turing 
machine 
that 
accepts 
exactly 
the 
"yes" 
instances. 
Because 
it 
must 
be 
total, 
it 
must 
also 
reject 
the 
"no" 
instances; 
in 
other 
words, 
it 
must 
not 
loop 
on 
any 
input. 
Problem 
(a) 
is 
easily 
decidable, 
since 
the 
number 
of 
states 
of 
M 
can 
be 
read 
off 
from 
the 
encoding 
of 
M. 
We 
can 
build 
a 
Turing 
machine 
that, 
given 
the 
encoding 
of 
M 
written 
on 
its 
input 
tape, 
coq.nts 
the 
number 
of 
states 
of 
M 
and 
accepts 
or 
rejects 
depending 
on 
whether 
the 
number 
is 
at 
least 
481. 
Problem 
(b) 
is 
decidable, 
since 
we 
can 
simulate 
M 
on 
input 
f 
with 
a 
versal 
machine 
for 
481 
steps 
(counting 
up 
to 
481 
on 
a 
separate 
track) 
and 
accept 
or 
reject 
depending 
on 
whether 
M 
has 
halted 
by 
that 
time. 
Problem 
(c) 
is 
decidable: 
we 
can 
just 
simulate 
M 
on 
all 
inputs 
of 
length 
at 
most 
481 
for 
481 
steps. 
If 
M 
takes 
more 
than 
481 
steps 
on 
some 
input, 
then 
it 
will 
take 
more 
than 
481 
steps 
on 
some 
input 
of 
length 
at 
most 
481, 
since 
in 
481 
steps 
it 
can 
read 
at 
most 
the 
first 
481 
symbols 
of 
the 
input. 
The 
argument 
for 
problem 
(d) 
is 
similar. 
If 
M 
takes 
more 
than 
481 
steps 
on 
all 
inputs 
of 
length 
at 
most 
481, 
then 
it 
will 
take 
more 
than 
481 
steps 
on 
all 
inputs. 
For 
problem 
(e), 
if 
M 
never 
moves 
more 
than 
481 
tape 
cells 
away 
from 
the 
left 
endmarker, 
then 
it 
will 
either 
halt 
or 
loop 
in 
such 
a 
way 
that 
we 
can 
detect 
the 
looping 
after 
a 
finite 
time. 
This 
is 
because 
if 
M 
has 
k 
states 
and 
m 
tape 
symbols, 
and 
never 
moves 
more 
than 
481 
tape 
cells 
away 
from 
the 
left 
endmarker, 
then 
there 
are 
only 
482km
481 
configurations 
it 
could 
possibly 
ever 
be 
in, 
one 
for 
each 
choice 
of 
head 
position, 
state, 
and 
tape 
contents 
that 
fit 
within 
481 
tape 
cells. 
If 
it 
runs 
for 
any 
longer 
than 
that 
without 
moving 
more 
than 
481 
tape 
cells 
away 
from 
the 
left 
endmarker, 
then 
it 
must 
be 
in 
a 
loop, 
because 
it 
must 
have 
repeated 
a 
configuration. 
This 
can 
be 
detected 
by 
a 
machine 
that 
simulates 
M, 
counting 
the 
number 
of 
steps 
M 
takes 
on 
a 
separate 
track 
and 
declaring 
M 
to 
be 
in 
a 
loop 
if 
the 
bound 
of 
482km
481 
steps 
is 
ever 
exceeded. 

_____________________________________________
Decidable 
and 
Undecidable 
Problems 
237 
Problems 
(f) 
through 
(I) 
are 
undecidable. 
To 
show 
this, 
we 
show 
that 
the 
ability 
to 
decide 
any 
one 
of 
these 
problems 
could 
be 
used 
to 
decide 
the 
halting 
problem. 
Since 
we 
know 
that 
the 
halting 
problem 
is 
undecidable, 
these 
problems 
must 
be 
undecidable 
too. 
This 
is 
called 
a 
reduction. 
Let 
's 
consider 
(f) 
first 
(although 
the 
same 
construction 
will 
take 
care 
of 
(g) 
through 
(i) 
as 
well). 
We 
will 
&how 
that 
it 
is 
undecidable 
whether 
a 
given 
machine 
accepts 
f, 
because 
the 
ability 
to 
decide 
this 
question 
would 
give 
the 
ability 
to 
decide 
the 
halting 
problem, 
which 
we 
know 
is 
impossible. 
Suppose 
we 
could 
decide 
whether 
e\. 
given 
machine 
a.ccepts 
E. 
We 
could 
then 
decide 
the 
halting 
problem 
as 
folIows. 
Say 
we 
are 
given 
a 
Turing 
machine 
M 
and 
string 
x, 
and 
we 
wish 
to 
determine 
whether 
M 
halts 
on 
X. 
Construct 
from 
M 
and 
x 
a 
new 
machine 
M' 
that 
does 
the 
following 
on 
input 
y: 
(i) 
erases 
its 
input 
y; 
(ii) 
writes 
x 
on 
its 
tape 
(M' 
has 
x 
hard-wired 
in 
its 
finite 
control); 
(iii) 
runs 
M 
on 
input 
x 
(M' 
also 
has 
a 
description 
of 
M 
hard-wired 
in 
its 
finite 
control); 
(iv) 
accepts 
if 
M 
halts 
on 
X. 
I 
t 
M' 
erase 
input 
+ 
I 
write 
x 
I 
t 
x 
M 
Note 
that 
M' 
does 
the 
same 
thing 
on 
all 
inputs 
y: 
if 
M 
halts 
on 
x, 
then 
M' 
accepts 
its 
input 
y; 
and 
if 
M 
does 
not 
halt 
on 
x, 
then 
M' 
does 
not 
halt 
on 
y, 
therefore 
does 
not 
accept 
y. 
Moreover, 
this 
is 
true 
for 
every 
y. 
Thus 
if 
M 
halts 
on 
x, 
if 
M 
does 
not 
halt 
on 
X. 
L(M
'
) 
= { 
;* 
Now 
if 
we 
could 
decide 
whether 
a 
given 
machine 
accepts 
the 
null 
f:tring 
E, 
we 
could 
apply 
this 
decision 
procedure 
to 
the 
M' 
just 
constructed, 
and 
this 
would 
tell 
whether 
M 
halts 
on 
X. 
In 
other 
words, 
we 
could 
obtain 
adecision 
procedure 
for 
halting 
as 
folIows: 
given 
M 
and 
x, 
construct 
M', 
then 
ask 
whether 
M' 
accepts 
E. 
The 
answer 
to 
t.he 
latter 
quest 
ion 
is 
"yes" 
iff 
M 
halts 

_____________________________________________
238 
Lecture 
32 
on 
x. 
Since 
we 
know 
the 
halting 
problem 
is 
undecidable, 
it 
must 
also 
be 
undecidable 
whether 
a 
given 
machine 
accepts 
E. 
Similarly, 
if 
we 
could 
decide 
whether 
a 
given 
machine 
accepts 
any 
string 
at 
all 
, 
or 
whether 
it 
accepts 
every 
string, 
or 
wh 
ether 
the 
set 
of 
strings 
it 
accepts 
is 
finite, 
we 
could 
apply 
any 
of 
these 
decision 
procedures 
to 
M' 
and 
this 
would 
tell 
whether 
M 
halts 
on 
x. 
Since 
we 
know 
that 
the 
halting 
problem 
is 
undecidable, 
all 
of 
these 
problems 
must 
be 
undecidable 
too. 
To 
show 
that 
(j), 
(k), 
and 
(1) 
are 
undecidable, 
pick 
your 
favorite 
r.e. 
but 
nonrecursive 
set 
A 
(HP 
or 
MP 
will 
do) 
and 
modify 
the 
above 
construction 
as 
follows. 
Given 
M 
and 
x, 
build 
a 
new 
machine 
M" 
that 
does 
the 
following 
on 
input 
y: 
(i) 
saves 
y 
on 
a 
separate 
track 
of 
its 
tape; 
(ii) 
writes 
x 
on 
a 
different 
track 
(x 
is 
hard-wired 
in 
the 
finite 
control 
of 
M"); 
(iii) 
runs 
M 
on 
input 
x 
(M 
is 
also 
hard-wired 
in 
the 
finite 
control 
of 
M"); 
(iv) 
if 
M 
halts 
on 
x, 
then 
M" 
runs 
a 
machine 
accepting 
A 
on 
its 
original 
input 
y, 
and 
accepts 
if 
that 
machine 
accepts. 
Either 
M 
does 
not 
halt 
on 
x, 
in 
which 
case 
the 
simulation 
in 
step 
(iii) 
never 
halts 
and 
M" 
never 
accepts 
any 
string; 
or 
M 
does 
halt 
on 
x, 
in 
which 
case 
M" 
accepts 
its 
input 
y 
iff 
Y 
E 
A. 
Thus 
L(M") 
= { 
if 
M 
halts 
on 
x, 
if 
M 
does 
not 
halt 
on 
x. 
Since 
A 
is 
neither 
recursive, 
CFL, 
nor 
regular, 
and 
0 
is 
all 
three 
of 
these 
things, 
if 
one 
could 
decide 
whether 
a 
given 
TM 
accepts 
a 
recursive, 
free, 
or 
regular 
set, 
then 
one 
could 
apply 
this 
decision 
procedure 
to 
M" 
and 
this 
would 
tell 
whether 
M 
halts 
on 
x. 

_____________________________________________
Lecture 
33 
Reduction 
There 
are 
two 
main 
techniques 
for 
showing 
that 
problems 
are 
able: 
diagonalization 
and 
reduction. 
We 
saw 
examples 
of 
di'a.gonalization 
in 
Lecture 
31 
and 
reduction 
in 
Lecture 
32. 
Once 
we 
have 
established 
that 
a 
problem 
such 
as 
HP 
is 
undecidable, 
we 
can 
show 
that 
another 
problem 
B 
is 
undecidable 
by 
reducing 
HP 
to 
B. 
Intuitively, 
this 
means 
we 
can 
manipulate 
instances 
of 
HP 
to 
make 
them 
look 
like 
instances 
of 
the 
problem 
B 
in 
such 
a 
way 
that 
"yes" 
instances 
of 
HP 
become 
"yes" 
instances 
of 
Band 
"no" 
instances 
of 
HP 
become 
"no" 
instances 
of 
B. 
Although 
we 
cannot 
tell 
effectively 
whether 
a 
given 
instance 
of 
HP 
is 
a 
"yes" 
instance, 
the 
manipulation 
preserves 
"yes"-ness 
and 
ness. 
If 
there 
existed 
adecision 
procedure 
for 
B, 
then 
we 
could 
apply 
it 
to 
the 
disguised 
instances 
of 
HP 
to 
decide 
membership 
in 
HP. 
In 
other 
words, 
combining 
adecision 
procedure 
for 
B 
with 
the 
manipulation 
pro 
dure 
would 
give 
adecision 
procedure 
for 
HP. 
Since 
we 
have 
already 
shown 
that 
no 
such 
decision 
procedure 
for 
HP 
can 
exist, 
we 
can 
conclude 
that 
no 
decision 
procedure 
for 
B 
can 
exist. 
We 
can 
give 
an 
abstract 
definition 
of 
reduction 
and 
prove 
a 
general 
theorem 
that 
will 
save 
us 
a 
lot 
of 
work 
in 
undecidability 
proofs 
from 
now 
on. 
Given 
sets 
A 
r;* 
and 
B 
!:l. 
*, 
a 
(many-one) 
reduction 
of 
A 
to 
B 
is 
a 
computable 
function 
q: 
r;* 
-+!:l.* 

_____________________________________________
240 
Lecture 
33 
such 
that 
for 
all 
x 
E 
I:*, 
xE 
A 
<==> 
a(x) 
E 
B. 
(33.1 
) 
In 
other 
words, 
strings 
in 
A 
must 
go 
to 
strings 
in 
B 
under 
a, 
and 
strings 
not 
in 
A 
must 
go 
to 
strings 
not 
in 
B 
under 
a. 
y 
a{y) 
I:* 
The 
function 
a 
need 
not 
be 
one-to-one 
or 
onto. 
It 
must, 
however, 
be 
total 
and 
effectively 
computable. 
This 
means 
a 
must 
be 
computable 
by 
a 
total 
Turingmachine 
that 
on 
any 
input 
x 
halts 
with 
a(x) 
written 
on 
itR 
tape. 
When 
such 
a 
reduction 
exists, 
we 
say 
that 
A 
is 
reducible 
to 
B 
via 
the 
map 
a, 
and 
we 
write 
A 
B. 
The 
subscript 
m, 
which 
stands 
for 
"many-one," 
is 
used 
to 
distinguish 
this 
relation 
from 
other 
types 
of 
reducibility 
relations. 
The 
relation 
::;m 
of 
reducibility 
between 
languages 
is 
transitive: 
if 
A 
::;m 
B 
and 
B 
::;m 
C, 
then 
A 
::;m 
C. 
This 
is 
because 
if 
areduces 
A 
to 
Band 
T 
reduces 
B 
to 
C, 
then 
T 
0 
a, 
the 
composition 
of 
a 
and 
T, 
is 
computable 
and 
reduces 
A 
to 
C. 
Although 
we 
have 
not 
mentioned 
it 
explicitly, 
we 
have 
used 
reductions 
in 
the 
last 
few 
lectures 
to 
show 
that 
various 
problems 
are 
undecidable. 
Example 
33.1 
In 
showing 
that 
it 
is 
undecidable 
whether 
a 
given 
TM 
accepts 
the 
null 
string, 
we 
constructed 
from 
a 
given 
TM 
M 
and 
string 
x 
a 
TM 
M' 
that 
accepted 
the 
null 
string 
iff 
M 
halts 
on 
x. 
In 
this 
example, 
A 
= 
{M#x 
I 
M 
halts 
on 
x} 
= 
HP, 
B 
= 
{M 
I 
• 
E 
L{M)}, 
and 
a 
is 
the 
computable 
map 
M#x 
H 
M'. 
o 
Example 
33.2 
In 
showing 
that 
it 
is 
undecidable 
whether 
a 
given 
TM 
accepts 
a 
regular 
set, 
we 
constructed 
from 
a 
given 
TM 
M 
and 
string 
x 
a 
TM 
M" 
such 
that 

_____________________________________________
Theorem 
33.3 
Reduction 
241 
L(MI/) 
is 
a 
nonregular 
set 
if 
M 
halts 
on 
x 
and 
0 
otherwise. 
In 
this 
example, 
A 
= 
{M#x 
I 
M 
halts 
on 
x} 
= 
HP, 
B 
= 
{M 
I 
L(M) 
is 
regular}, 
and 
a 
is 
the 
computable 
map 
M#x 
f-+ 
MI/. 
o 
Here 
is 
a 
general 
theorem 
that 
will 
save 
us 
some 
work. 
(i) 
If 
A 
:Sm 
Band 
B 
is 
r.e., 
then 
so 
A. 
Equi'valently, 
if 
A 
:Sm 
Band 
A 
is 
not 
r. 
e., 
then 
neither 
i,q 
B. 
(ii) 
If 
A 
:Sm 
Band 
B 
is 
recursive, 
then 
$0 
is 
A. 
Equivalently, 
if 
A 
:Sm 
B 
and 
A 
is 
not 
recursive, 
then 
neither 
is 
B. 
Proof. 
(i) 
Suppose 
A 
:Sm 
B 
via 
the 
map 
a 
and 
B 
is 
r.e. 
Let 
M 
be 
a 
TM 
such 
that 
B 
= 
L(M). 
Build 
a 
machine 
N 
for 
A 
as 
iollows: 
on 
input 
x, 
first 
compute 
a(x), 
then 
run 
M 
on 
input 
a(x), 
accepting 
if 
M 
accepts. 
Then 
N 
accepts 
x 
<=} 
M 
accepts 
a(x) 
definition 
of 
N 
<=} 
a(x) 
E 
B 
definition 
of 
M 
by 
(33.1). 
(ü) 
Reca.1l 
from 
Lecture 
29 
that 
a 
set 
is 
recursive 
iff 
both 
it 
and 
its 
plement 
are 
r.e. 
Suppose 
A 
:Sm 
B 
via 
the 
map 
q 
and 
B 
is 
recursive. 
Note 
that 
'" 
A 
Sm 
'" 
B 
via 
the 
same 
q 
(Check 
the 
definition!). 
If 
B 
is 
recursive, 
then 
both 
B 
and 
'" 
Bare 
r.e. 
By 
(i), 
both 
A 
and 
'" 
Aare 
r.e., 
thus 
A 
is 
recursive. 
0 
We 
can 
use 
Theorem 
33.3(i)· 
to 
show 
that 
certain 
sets 
are 
not 
r.e. 
and 
Theorem 
33.3(ü) 
to 
show 
that 
certain 
sets 
are 
not 
recursive. 
To 
show 
that 
a 
set 
Bis 
not 
r,e., 
we 
need 
only 
give 
a 
reductionfrom 
a 
set 
A 
we 
already 
know 
is 
not 
r.e. 
(such 
as 
'" 
HP) 
to 
B. 
By 
Theorem 
33.3(i), 
B 
cannot 
be 
r.e. 
Example 
33.4 
Let's 
illustrate 
by 
showing 
that 
neither 
the 
set 
FIN 
= 
{M 
I 
L(M) 
ie 
finite} 
nor 
its 
complement 
is 
r.e. 
We 
show 
that 
neither 
of 
these 
sets 
is 
r.e. 
by 
reducing 
'" 
HP 
to 
each 
of 
them, 
where 
",HP 
= 
{M#x 
I 
M 
does 
not 
halt 
on 
x} 
: 
(a) 
'" 
HP 
:Sm 
FIN, 
(b) 
'" 
HP 
Sm 
'" 
FIN. 
Since 
we 
already 
know 
that 
N 
HP 
is 
not 
r.e., 
it 
follows 
from 
Theorem 
33.3(i) 
that 
neither 
FIN 
nor 
'" 
FIN 
is 
r.e. 

_____________________________________________
242 
Lecture 
33 
For 
(a), 
we 
want 
to 
give 
a 
computable 
map 
a 
such 
that 
M#x 
E 
"'HP 
{::::} 
a(M#x) 
E 
FIN. 
In 
other 
words, 
from 
M 
#x 
we 
want 
to 
construct 
a 
Turing 
machine 
M' 
= 
a(M#x) 
such 
that 
M 
does 
not 
halt 
on 
x 
<=> 
L(M
'
) 
is 
finite. 
(33.2) 
Note 
that 
the 
description 
of 
M' 
can 
depend 
on 
M 
and 
x. 
In 
particular, 
M' 
can 
have 
a 
description 
af 
M 
and 
the 
string 
x 
hard-wired 
in 
its 
finite 
control 
if 
desired. 
We 
have 
actually 
already 
given 
a 
construction 
satisfying 
(33.2). 
Given 
M 
#x, 
construct 
M' 
such 
that 
on 
all 
inputs 
V, 
M' 
takes 
the 
following 
actions: 
(i) 
erases 
its 
input 
vj 
(ii) 
writes 
x 
on 
its 
tape 
(M' 
has 
x 
hard-wired 
in 
its 
finite 
control)j 
(iii) 
runs 
M 
on 
input 
x 
(M' 
also 
has 
a 
description 
of 
M 
hard-wired 
in 
its 
finite 
control)j 
(iv) 
accepts 
if 
M 
halts 
on 
x. 
If 
M 
does 
not 
halt 
on 
input 
x, 
then 
the 
simulation 
in 
step 
(iii) 
never 
halts, 
and 
M' 
never 
reaches 
step 
(iv). 
In 
this 
case 
M' 
does 
not 
accept 
its 
input 
v. 
This 
happens 
the 
same 
way 
for 
all 
inputs 
y, 
therefore 
in 
this 
case, 
L(M) 
= 
0. 
On 
the 
other 
hand, 
if 
M 
does 
halt 
on 
x, 
then 
the 
simulation 
in 
step 
(iii) 
halts, 
and 
V 
is 
accepted 
in 
step 
(iv). 
Moreover, 
this 
is 
true 
for 
all 
y. 
In 
this 
case, 
L(M) 
= 
E*. 
Thus 
M 
halts 
on 
x 
=> 
L(M
'
) 
= 
E* 
M 
does 
not 
halt 
on 
x 
=> 
L(M
'
) 
= 
0 
=> 
L(M
'
) 
is 
infinite, 
=> 
L(M
'
) 
is 
finite. 
Thus 
(33.2) 
is 
satisfied. 
Note 
that 
this 
is 
all 
we 
have 
to 
do 
to 
show 
that 
FIN 
is 
not 
r.e.: 
we 
have 
given 
the 
reduction 
(a), 
so 
by 
Theorem 
33.3(i) 
we 
are 
done. 
There 
is 
a 
common 
pitfall 
here 
that 
we 
should 
be 
careful 
to 
avoid. 
It 
is 
portant 
to 
observe 
that 
the 
computable 
map 
a 
that 
produces 
a 
description 
of 
M' 
from 
M 
and 
x 
does 
not 
need 
to 
execute 
the 
program 
(i) 
through 
(iv). 
It 
only 
produces 
the 
description 
of 
a 
machine 
M' 
that 
does 
so. 
The 
tation 
of 
a 
is 
quite 
simple-it 
does 
not 
involve 
the 
simulation 
of 
any 
other 
machines 
or 
anything 
complicated 
at 
all. 
It 
merely 
takes 
a 
description 
of 
a 
Turing 
machine 
M 
and 
string 
x 
and 
plugs 
them 
into 
a 
general 
description 
of 
a 
machine 
that 
executes 
(i) 
through 
(iv). 
This 
can 
be 
done 
quite 
easily 
by 
a 
total 
TM, 
so 
a 
is 
total 
and 
effectively 
computable. 

_____________________________________________
Reduction 
243 
Now 
(b). 
By 
definition 
of 
reduction, 
a 
map 
reducing 
""' 
HP 
to 
""' 
FIN 
also 
reduces 
HP 
to 
FIN, 
so 
it 
suffices 
to 
give 
a 
computable 
map 
7 
such 
that 
M#x 
E 
HP 
{::::::} 
7(M#x) 
E 
FIN. 
In 
other 
words, 
from 
M 
and 
x 
we 
want 
to 
construct 
a 
Turing 
machine 
M" 
= 
7( 
M 
#x) 
such 
that 
M 
halts 
on 
x 
{::::::} 
L(M") 
is 
finite. 
(33.3) 
Given 
M 
#x, 
construct 
a 
machine 
M" 
that 
on 
input 
y 
(i) 
saves 
y 
on 
a 
separate 
track; 
(ii) 
writes 
x 
on 
the 
tape; 
(iii) 
simulates 
M 
on 
x 
for 
lyl 
steps 
(it 
erases 
one 
symbol 
of 
y 
for 
each 
step 
of 
M 
on 
x 
that 
it 
simulates); 
(iv) 
accepts 
if 
M 
has 
not 
halted 
within 
that 
time, 
otherwise 
rejects. 
Now 
if 
M 
never 
halts 
on 
x, 
then 
M" 
halts 
and 
accepts 
y 
in 
step 
(iv) 
after 
lyl 
steps 
of 
the 
and 
this 
is 
true 
for 
all 
y. 
In 
this 
case 
L( 
M") 
= 
On 
the 
other 
hand, 
if 
M 
does 
halt 
on 
x, 
then 
it 
does 
so 
after 
some 
finite 
number 
of 
steps, 
say 
n. 
Then 
M" 
accepts 
y 
in 
(iv) 
if 
lyl 
< 
n 
(since 
the 
simulation 
in 
(iii) 
has 
not 
finished 
by 
lyl 
steps) 
and 
rejects 
y 
in 
(iv) 
if 
lyl 
2: 
n 
(since 
the 
simulation 
in 
(iii) 
does 
have 
time 
to 
complete). 
In 
this 
case 
M
II 
accepts 
all 
strings 
of 
length 
less 
than 
n 
and 
rejects 
all 
strings 
of 
length 
n 
or 
greater, 
so 
L( 
M
II
) 
is 
a 
finite 
set. 
Thus 
M 
halts 
on 
x 
L(M") 
= 
{y 
Ilyl 
< 
rUnning 
time 
of 
M 
on 
x} 
L(M") 
is 
finite, 
M 
does 
not 
halt 
on 
x 
L(M") 
= 
L( 
M") 
is 
infinite. 
'Then 
(33.3) 
is 
satisfied. 
It 
is 
important 
that 
the 
functions 
a 
and 
7 
in 
these 
two 
reductions 
can 
be 
computed 
by 
Turing 
machines 
that 
always 
halt. 
0 
Historical 
Notes 
The 
technique 
of 
diagonalization 
was 
first 
used 
by 
Cantor 
[16J 
to 
show 
that 
there 
were 
fewer 
real 
algebraic 
numbers 
than 
real 
numhers. 

_____________________________________________
244, 
Lecture 
33 
Universal 
Turing 
machines 
and 
the 
application 
of 
Cantor's 
diagonalization 
technique 
to 
prove 
the 
undecidabilitv 
of 
the 
halting 
problem 
appear 
in 
Turing's 
original 
paper 
[120]. 
Reducibility 
relations 
are 
discussed 
by 
Post 
[101]; 
see 
[106, 
116]. 

_____________________________________________
Lecture 
34 
Rice's 
Theorem 
Rice's 
theorem 
says 
that 
undecidability 
is 
the 
rule, 
not 
the 
exception. 
It 
is 
a 
very 
powerful 
theorem, 
subsuming 
many 
undecidability' 
results 
that 
we 
have 
seen 
as 
special 
cases. 
Theorem 
34.1 
(Rice's 
theorem) 
Every 
nontrivial 
property 
01 
the 
r.e. 
sets 
is 
able. 
Yes, 
you 
heard 
right: 
that's 
every 
nontrivial 
property 
of 
the 
r.e. 
sets. 
So 
as 
not 
to 
misinterpret 
this, 
let 
us 
clarify 
a 
few 
things. 
First, 
fix 
a 
finite 
alphabet 
1:. 
A 
property 
01 
the 
r. 
e. 
sets 
is 
a 
map 
P: 
{r.e. 
subsets 
of 
1:*} 
-> 
{T, 
.l..}, 
where 
T 
and 
.l.. 
represent 
truth 
and 
falsity, 
respectively. 
For 
example, 
the 
property 
of 
emptiness 
is 
represented 
by 
the 
map 
P(A) 
= { : 
if 
A 
= 
0, 
if 
A;6 
0. 
To 
ask 
whether 
such 
a 
property 
P 
is 
decidable, 
the 
set 
has 
to 
be 
presented 
in 
a 
finite 
form 
suitable 
for 
input 
to 
a 
TM. 
We 
assume 
that 
r.e. 
sets 
are 
presented 
by 
TMs 
that 
accept 
them. 
But 
keep 
in 
mind 
that 
the 
property 
is 
a 
property 
of 
sets, 
not 
of 
Turing 
machines; 
thus 
it 
must 
be 
true 
or 
false 
independent 
of 
the 
particular 
TM 
chosen 
to 
represent 
the 
set. 

_____________________________________________
246 
Lecture 
34 
Here 
are 
some 
other 
examples 
of 
properties 
of 
r.e. 
sets: 
L(M) 
is 
finite; 
L(M) 
is 
regular; 
L(M) 
is 
a 
CFL; 
M 
accepts 
101001 
(Le., 
101001 
E 
L(M)); 
L(M) 
= 
Each 
of 
these 
properties 
is 
a 
property 
of 
the 
set 
accepted 
by 
the 
Turing 
machine. 
Here 
are 
some 
examples 
of 
properties 
of 
Turing 
machines 
that 
are 
not 
erties 
of 
r.e. 
sets: 
M 
has 
at 
least 
481 
states; 
M 
halts 
on 
all 
inputs; 
M 
rejects 
101001; 
there 
exists 
a 
smaller 
machine 
equivalent 
to 
M. 
These 
are 
not 
erties 
of 
sets, 
because 
in 
each 
case 
one 
can 
give 
two 
TMs 
that 
accept 
the 
same 
set, 
one 
of 
which 
satisfies 
the 
property 
and 
the 
other 
of 
which 
doesn't. 
For 
Rice's 
theorem 
to 
apply, 
the 
property 
also 
has 
to 
be 
nontrivial. 
This 
just 
means 
that 
the 
property 
is 
neither 
universally 
true 
nor 
universally 
false; 
that 
is, 
there 
must 
be 
at 
least 
one 
r.e. 
set 
that 
satisfies 
the 
property 
and 
at 
least 
one 
that 
does 
not. 
There 
are 
only 
two 
trivial 
properties, 
and 
they 
are 
both 
trivially 
decidable. 
Prool 
01 
Rice 
's 
theorem. 
Let 
P 
be 
a 
nontrivial 
property 
of 
the 
r.e. 
sets. 
sume 
without 
loss 
of 
generality 
that 
P(0) 
= 
..L 
(the 
argument 
is 
symmetrie 
if 
P(0) 
= 
T). 
Since 
P 
is 
nontrivial, 
there 
must 
exist 
an 
r.e. 
set 
A 
such 
that 
P(A) 
= 
T. 
Let 
K 
be 
a 
TM 
accepting 
A. 
We 
reduce 
HP 
to 
the 
set 
{M 
I 
P(L(M)) 
= 
T}, 
thereby 
showing 
that 
the 
latter 
is 
undecidable 
(Theorem 
33.3(ii)). 
Given 
M 
Ix, 
construct 
a 
machine 
M' 
= 
cr(M#x) 
that 
on 
input 
y 
(i) 
saves 
y 
on 
a 
separate 
track 
someplace; 
(ii) 
writes 
x 
on 
its 
tape 
(x 
is 
hard-wired 
in 
the 
finite 
control 
of 
M'); 
(iii) 
runs 
M 
on 
input 
x 
(a 
description 
of 
M 
is 
also 
hard-wired 
in 
the 
finite 
control 
of 
M'); 
(iv) 
if 
M 
halts 
on 
x, 
M' 
runs 
K 
on 
y 
and 
accepts 
if 
K 
accepts. 
Now 
either 
M 
halts 
on 
x 
or 
not. 
If 
M 
does 
not 
halt 
on 
x, 
then 
the 
simulation 
in 
(iii) 
will 
never 
halt, 
and 
the 
input 
y 
of 
M' 
will 
not 
be 
accepted. 
This 
is 
true 
for 
every 
y, 
so 
in 
this 
case 
L(M
/
) 
= 
0. 
On 
the 
other 
hand, 
if 
M 
does 
halt 
on 
x, 
then 
M' 
always 
reaches 
step 
(iv), 
and 
the 
original 
input 
y 
of 
M' 
is 
accepted 
iff 
y 
is 
accepted 
by 
K; 
that 
is, 
if 
y 
E 
A. 
Thus 
M 
halts 
on 
x 
L(M
'
) 
= 
A 
M 
does 
not 
halt 
on 
x 
L(M
'
) 
= 
0 
P(L(M
/
)) 
= 
P(A) 
= 
T, 
P(L(M
'
)) 
= 
P(0) 
= 
..L. 
This 
constitutes 
a 
reduction 
from 
HP 
to 
the 
set 
{M 
I 
P(L(M)) 
= 
T}. 
Since 
HP 
is 
not 
recursive, 
by 
Theorem 
33.3, 
neither 
is 
the 
latter 
set; 
that 
is, 
it 
is 
undecidable 
whether 
L(M) 
satisfies 
P. 
0 

_____________________________________________
Rice's 
Theorem 
247 
Rice's 
Theorem, 
Part 
11 
A 
property 
P 
: 
{r.e. 
sets} 
-+ 
{T,1..} 
of 
the 
r.e. 
sets 
is 
called 
monotone 
if 
for 
all 
r.e. 
sets 
A 
and 
B, 
if 
A 
B, 
then 
P(A) 
P(B). 
Here 
means 
less 
than 
or 
equal 
to 
in 
the 
order 
1.. 
T. 
In 
other 
words, 
P 
is 
monotone 
if 
whenever 
a 
set 
has 
the 
property, 
then 
all 
super$ets 
of 
that 
set 
have 
it 
as 
weIl. 
For 
example, 
the 
properties 
"L(M) 
is 
infinite" 
and 
"L(M) 
= 
L:*" 
are 
monotone 
but 
"L(M) 
is 
finite" 
and 
"L(M) 
= 
0" 
are 
not. 
Theorem 
34.2 
(Rice's 
theorem, 
part 
11) 
No 
nonmonotone 
property 
01 
the 
r. 
e. 
sets 
is 
semidecidable. 
In 
other 
words, 
il 
P 
is 
a 
nonmonotone 
property 
01 
the 
r.e. 
sets, 
then 
the 
set 
Tp 
= 
{M 
I 
P( 
L( 
M)) 
= 
T} 
is 
not 
r. 
e. 
Proof. 
Since 
P 
is 
nonmonotone, 
there 
exist 
TMs 
Mo 
and 
MI 
such 
that 
L(M
o
) 
L(M
I
), 
P(M
o
) 
= 
T, 
and 
P(Md 
= 
1... 
We 
want 
to 
reduce 
'" 
HP 
to 
T
p
, 
or 
equivalently, 
HP 
to 
'" 
T
p 
= 
{M 
I 
P(L(M)) 
= 
1..}. 
Since 
"'HP 
is 
not 
r.e., 
neither 
will 
be 
T
p
. 
Given 
M#x, 
we 
want 
to 
show 
how 
to 
construct 
a 
machine 
M' 
such 
that 
P(M') 
= 
1.. 
iff 
M 
halts 
on 
x. 
Let 
M' 
be 
a 
machine 
that 
does 
the 
following 
on 
input 
y: 
(i) 
writes 
its 
input 
y 
on 
the 
top 
and 
middle 
tracks 
of 
its 
tape; 
(ii) 
writes 
x 
on 
the 
bottom 
track 
(it 
has 
x 
hard-wired 
in 
its 
finite 
control); 
(iii) 
simulates 
Mo 
on 
input 
y 
on 
the 
top 
track, 
MI 
ori 
input 
y 
on 
the 
middle 
track, 
and 
M 
on 
input 
x 
on 
the 
bottom 
track 
in 
a 
round-robin 
fashion; 
that 
is, 
it 
simulates 
one 
step 
of 
each 
of 
the 
three 
machines, 
then 
another 
step, 
and 
so 
on 
(descriptions 
of 
Mo, 
MI, 
and 
Mare 
all 
hard-wired 
in 
the 
finite 
control 
of 
M'); 
(iv) 
accepts 
its 
input 
y 
if 
either 
of 
the 
following 
two 
events 
oecurs: 
(a) 
Mo 
accepts 
y, 
or 
(b) 
MI 
accepts 
y 
and 
M 
halts 
on 
x. 
Either 
M 
halts 
on 
x 
or 
not, 
independent 
of 
the 
input 
y 
to 
M'. 
If 
M 
does 
not 
halt 
on 
x, 
then 
event 
(b) 
in 
step 
(iv) 
will 
never 
occur, 
so 
M' 
will 
accept 
y 
iff 
event 
(a) 
occurs, 
thus 
in 
this 
case 
L(M') 
= 
L(M
o
). 
On 
the 
other 
hand, 
if 
M 
does 
halt 
on 
x, 
then 
y 
will 
be 
accepted 
iff 
it 
is 
accepted 
by 
either 
Mo 
or 
MI; 
that 
is, 
if 
y 
E 
L(M
o
) 
U 
L(M
I
). 
Since 
L(M
o
) 
L(M
I
), 
this 
is 
equivalent 
to 
saying 
that 
y 
E 
L(M
I
), 
thus 
in 
this 
ease 
L(M') 
= 
L(MI). 
We 
have 
shown 
M 
halts 
on 
x 
=> 
L(M') 
= 
L(M
I
) 
=> 
P(L(M')) 
= 
P(L(M
I
)) 
= 
1.., 

_____________________________________________
248 
Lecture 
34 
M 
does 
not 
halt 
on 
x 
=> 
L(M') 
= 
L(M
o
) 
=> 
P(L(M')) 
= 
P(L(M
o
)) 
= 
T. 
The 
construction 
of 
M' 
from 
M 
and 
x 
constitutes 
a 
reduction 
from 
,...., 
HP 
.to 
the 
set 
= 
{M 
I 
P(L(M)) 
= 
T}. 
By 
Theorem 
33.3(i), 
the 
latter 
set 
is 
not 
r.e. 
0 
Historical 
Notes 
Rice's 
theorem 
was 
proved 
by 
H. 
G. 
Rice 
[104, 
105]. 

_____________________________________________
Lecture 
35 
Undecidable 
Problems 
About 
CFLs 
In 
this 
lecture 
we 
show 
that 
a 
very 
simple 
problem 
about 
CFLs 
is 
able, 
namely 
the 
problem 
of 
deciding 
whether 
a 
given 
CFG 
generates 
all 
strings. 
It 
is 
decidable 
whether 
a 
given 
CFG 
generates 
any 
string 
at 
all, 
since 
we 
know 
by 
the 
pumping 
lemma 
that 
a 
CFG 
G 
that 
generates 
any 
string 
at 
all 
must 
generate 
a 
short 
string; 
and 
we 
can 
determine 
for 
all 
short 
strings 
x 
whether 
x 
E 
L( 
G) 
by 
the 
CKY 
algorithm. 
This 
decision 
procedure 
is 
rather 
inefficient. 
Here 
is 
a 
better 
one. 
Let 
G 
= 
(N, 
P, 
5) 
be 
the 
given 
CFG. 
To 
decide 
whether 
L(G) 
is 
nonempty, 
we 
will 
execute 
an 
inductive 
procedure 
that 
marks 
a 
nonterminal 
when 
it 
is 
determined 
that 
that 
nonterminal 
generates 
some 
string 
in 
-any 
string 
at 
all-and 
when 
we 
are 
done, 
ask 
whetl1er 
the 
start 
symbolS 
is 
marked. 
At 
stage 
0, 
mark 
all 
the 
symbols 
of 
1";. 
At 
each 
successive 
stage, 
mark 
a 
nonterminal 
A 
E 
N 
if 
there 
is 
a 
production 
A 
ß 
E 
P 
and 
all 
symbols 
of 
ß 
are 
marked. 
Quit 
when 
there 
are 
no 
more 
changes; 
that 
is, 
when 
for 
each 
production 
A 
ß, 
either 
A 
is 
marked 
or 
there 
is 
an 
unmarked 
symbol 
of 
ß. 
This 
must 
happen 
after 
a 
finite 
time, 
since 
there 
are 
only 
finitcly 
many 
symbols 
to 
mark. 
It 
can 
be 
shown 
that 
A 
is 
marked 
by 
this 
procedure 
if 
and 
only 
if 
there 
is 
astring 
x 
E 
such 
that 
A 
x. 
This 
can 
be 
proved 
by 
induction, 
G 

_____________________________________________
250 
Lecture 
35 
the 
implication 
* 
by 
induction 
on 
the 
stage 
that 
A 
is 
marked, 
and 
the 
implication 
<= 
by 
induction 
on 
the 
length 
of 
the 
derivation 
A 
-iJ 
x. 
Then 
L( 
G) 
is 
nonempty 
Hf 
there 
exists 
an 
x 
E 
E* 
such 
that 
S 
x 
iff 
S 
G 
is 
marked. 
Believe 
it 
or 
not, 
this 
procedure 
can 
be 
implemented 
in 
linear 
time, 
so 
it 
is 
in 
fact 
quite 
easy 
to 
decide 
whether 
L( 
G) 
= 
{3. 
See 
also 
Miscellaneous 
Exercise 
134 
for 
another 
approach. 
The 
finiteness 
problem 
for 
CFLs 
is 
also 
decidable 
(Miscellaneous 
Exercise 
135). 
Valid 
Computation 
Histories 
In 
contrast 
to 
the 
efficient 
algorithm 
just 
given, 
it 
is 
impossible 
to 
decide 
in 
general 
for 
a 
given 
CFG 
Gwhether 
L( 
G) 
= 
E*. 
We 
will 
show 
this 
by 
a 
reduction 
from 
the 
halting 
problem. 
The 
reduction 
will 
involve 
the 
set 
VALCOMPS(M,x) 
of 
valid 
computation 
his 
tori 
es 
of 
a 
Turing 
machine 
M 
on 
input 
x, 
defined 
below. 
This 
set 
is 
also 
useful 
in 
showing 
the 
undecidability 
of 
other 
problems 
involving 
CFLs, 
such 
as 
whether 
the 
intersection 
of 
two 
given 
CFLs 
is 
nonempty 
or 
wh 
ether 
the 
complement 
of 
a 
given 
CFL 
is 
a 
CFL. 
Recall 
that 
a 
conjiguration 
a 
of 
a 
Turing 
machine 
M 
is 
a 
tripie 
(q,y,n) 
where 
q 
is 
astate, 
y 
is 
a 
semi-infinite 
string 
describing 
the 
contents 
of 
the 
tape, 
and 
n 
is 
a 
nonnegative 
integer 
describing 
the 
head 
position. 
We 
can 
encode 
configurations 
as 
finite 
strings 
over 
the 
alphabet 
rX(Qu{-}), 
where 
Q 
is 
the 
set 
of 
states 
of 
M, 
r 
is 
the 
tape 
alphabet 
of 
M, 
and 
-
is 
a 
new 
symbol. 
A paIr 
in 
r 
x 
(Q 
U { -
}) 
is 
written 
vertically 
with 
the 
element 
of 
r 
on 
top. 
A 
typical 
configuration 
(q, 
y, 
k) 
might 
be 
encoded 
as 
the 
string 
f-
b
1 
b
2 
b3 
... 
blc 
... 
b
m 
----
... 
q 
... 
-
which 
shows 
the 
nonblank 
symbols 
of 
y 
on 
the 
top 
and 
indicates 
that 
the 
machine 
is 
in 
state 
q 
scanning 
the 
kth 
tape 
cello 
Recall 
that 
the 
start 
conjiguration 
of 
M 
on 
input 
x 
is 
f-
al 
a2 
... 
an 
s 
--
where 
s 
is 
the 
start 
state 
of 
M 
and 
x 
= 
ala2'" 
an. 

_____________________________________________
Undecidable 
Problems 
About 
CFLs 
251 
A 
valid 
computation 
history 
of 
M 
on 
xis 
a 
list 
of 
such 
encodings 
of 
urations 
of 
M 
separated 
by 
a 
special 
marker 
# 
r 
x 
(Q 
U { -} 
); 
that 
is, 
a 
string 
#0.0#0.1#0.'2#··· 
#aN# 
such 
that 
Ł 
0.0 
is 
the 
start 
configuration 
of 
M 
on 
x; 
Ł 
aN 
is 
a 
halting 
configuration; 
that 
is, 
the 
state 
appearing 
in 
aN 
is 
either 
the 
accept 
state 
t 
or 
the 
reject 
state 
r; 
and 
Ł 
ai+1 
follows 
inone 
step 
from 
ai 
according 
to 
the 
transition 
function 
6 
of 
M, 
for 
0 
=5 
i 
=5 
N 
-
1; 
that 
is, 
ai 
-i7 
ai+1, 
0 
=5 
i 
=5 
N 
-
1, 
where 
.2... 
is 
the 
next 
configuration 
relation 
of 
M. 
M 
In 
other 
words, 
the 
valid 
computation 
history 
describes 
a 
halting 
tation 
of 
the 
machine 
M 
on 
input 
x, 
if 
M 
does 
indeed 
halt. 
If 
M 
does 
not 
halt 
on 
x, 
then 
no 
such 
valid 
computation 
history 
exists. 
Let 
tl 
= 
{#} 
U 
(r 
x 
(Q 
U { -} 
)). 
Then 
a 
valid 
computation 
history 
01 
l{ 
on 
x, 
if 
it 
exists, 
is 
astring 
in 
tl 
*. 
Define 
VALCOMPS(M,x) 
{valid 
computation 
histories 
of 
M 
on 
x}. 
Then 
VALCOMPS(M,
x) 
tl*, 
and 
VALCOMPS(M,x) 
= 
0 
{:::::} 
M 
does 
not 
halt 
on 
x. 
(35.1) 
Thus 
the 
complement 
of 
VALCOMPS(M, 
31), 
namely 
""VALCOMPS(M,x) 
= 
tl* 
-
VALCOMPS(M,x), 
is 
equal 
to 
tl 
* 
Hf 
M 
does 
not 
halt 
on 
x. 
The 
key 
claim 
now 
is 
that 
""VALCOMPS(M,x) 
is 
a 
CFL. 
Moreover, 
out 
knowing 
whether 
or 
not 
M 
halts 
on 
x, 
we 
can 
construct 
a 
CFG 
G 
for 
""VALCOMPS(M,x) 
from 
a 
description 
of 
M 
and 
x. 
By 
(35.1), 
we 
will 
have 
L( 
G) 
= 
tl 
* 
{:::::} 
M 
does 
not 
halt 
on 
x. 
Since 
we 
can 
construct 
G 
effectively 
from 
M 
and 
x, 
this 
will 
constitute 
a 
reduction 
""HP 
=5m 
{G 
I 
Gis 
a 
CFG 
and 
L(G) 
= 
tl*}. 
By 
Theorem 
33.3(i), 
the 
latter 
set 
is 
not 
r.e., 
which 
is 
what 
we 
want 
to 
show. 

_____________________________________________
252 
Lecture 
35 
To 
show 
that 
"'VALCOMPS(M,x) 
is 
a 
CFL, 
let 
us 
carefully 
write 
down 
all 
the 
conditions 
for 
astring 
z 
E 
fj. 
* 
to 
be 
a 
valid 
computation 
history 
of 
M 
on 
x: 
(1) 
z 
must 
begin 
and 
end 
with 
a 
#; 
that 
iso 
it 
must 
be 
of 
the 
form 
#0:0#0:1#'" 
#O:N#, 
where 
each 
O:i 
is 
in 
(fj. 
-
#)*; 
(2) 
each 
O:i 
is 
astring 
of 
symbols 
of 
the 
form 
a 
a 
or 
q 
where 
exactly 
one 
symbol 
of 
O:i 
has 
an 
element 
of 
Q 
on 
the 
bottom 
and 
the 
others 
have 
-, 
and 
only 
the 
leftmost 
has 
a 
f-
on 
top; 
(3) 
0:0 
represents 
the 
start 
configuration 
of 
M 
on 
x; 
(4) 
a 
halt 
state, 
either 
t 
or 
r, 
appears 
somewhere 
in 
z 
(by 
our 
convention 
that 
Turing 
machines 
always 
remain 
in 
a 
halt 
state 
once 
they 
enter 
it, 
this 
is 
equivalent 
to 
that 
O:N 
is 
a 
halt 
configuration); 
and 
(5) 
O:i 
7 
O:i+! 
for 
0 
i 
N 
-
1. 
Let 
Ai 
= 
{x 
E 
fj. 
* 
I 
x 
satisfies 
condition 
(in, 
1 
i 
5. 
Astring 
in 
fj.* 
is 
in 
VALCOMPS(M,x) 
iff 
it 
satisfies 
all 
five 
conditions 
listed 
above; 
that 
is, 
VALCOMPS(M,
x) 
= 
n 
Ai. 
l:$i:$S 
Astring 
is 
in 
"'VALCOMPS(M,x) 
iff 
it 
fails 
to 
satisfy 
at 
least 
one 
of 
conditions 
(1) 
through 
(5); 
that 
is, 
if 
it 
is 
in 
at 
least 
one 
of 
the 
'" 
Ai, 
1 
i 
5. 
We 
show 
that 
each 
of 
the 
sets 
'" 
Ai 
is 
a 
CFL 
and 
show 
how 
to 
obtain 
a 
CFG 
Gi 
for 
it. 
Then 
"'VALCOMPS(M,x) 
is 
the 
union 
of 
the 
'" 
Ai, 
and 
we 
know 
how 
to 
construct 
a 
grammar 
G 
for 
this 
union 
from 
the 
Gi. 
The 
sets 
Al, 
A
2
, 
A
3
, 
and 
A
4 
are 
all 
regular 
sets, 
and 
we 
can 
easily 
construct 
right-linear 
CFGs 
for 
their 
complements 
from 
finite 
automata 
or 
regular 
expressions. 
The 
only 
difficult 
case 
will 
be 
A
s
. 
The 
set 
Al 
is 
the 
set 
of 
strings 
beginning 
and 
with 
a 
#. 
This 
is 
the 
regular 
set 
#fj. 
*#. 

_____________________________________________
Undecidable 
Problems 
About 
CFLs 
253 
To 
check 
that 
astring 
is 
in 
A
2
, 
we 
need 
only 
check 
that 
between 
every 
two 
#'s 
there 
is 
exactly 
one 
symbol 
with 
astate 
q 
on 
the 
bottom, 
and 
r 
occurs 
on 
the 
top 
immediately 
after 
each 
# 
(except 
the 
last) 
and 
nowhere 
else. 
This 
can 
easily 
be 
checked 
with 
a 
finite 
automaton. 
The 
set 
A3 
is 
the 
regular 
set 
s 
--
... 
-
To 
check 
that 
astring 
is 
in 
A
4
, 
we 
need 
only 
check 
that 
t 
or 
rappears 
someplace 
in 
the 
string. 
Again, 
this 
is 
easily 
checked 
by 
a 
finite 
automaton. 
Finally, 
we 
are 
left 
with 
the 
task 
of 
showing 
that 
'" 
A
s 
is 
a 
CFL. 
Consider 
a 
substring 
... 
#a#ß#··· 
of 
astring 
in 
ß 
* 
satisfying 
conditions 
(1) 
through 
(4). 
Note 
that 
if 
a 
2-. 
ß, 
then 
the 
two 
configurations 
must 
agree 
in 
most 
M 
symbols 
except 
for 
q. 
few 
near 
the 
position 
of 
the 
head; 
and 
the 
differences 
that 
can 
occur 
near 
the 
position 
of 
the 
head 
must 
be 
consistent 
with 
the 
action 
of 
b. 
For 
example, 
the 
substring 
might 
look 
like 
... 
# 
r 
a b a a 
b, 
a b b 
# 
r 
a b a b b a b b 
# 
... 
----
q 
----
---
p 
-----
This 
would 
occur 
if 
b( 
q, 
a) 
= 
(p, 
b, 
L). 
We 
can 
check 
that 
a 
ß 
by 
check-
M 
ing 
for 
all 
three-element 
substrings 
u 
of 
a 
that 
the 
corresponding 
three-
element 
substring 
v 
of 
ß 
differs 
from 
u 
in 
a 
way 
that 
is 
with 
the 
operation 
of 
b. 
Corresponding 
means 
occurring 
at 
the 
same 
distance 
from 
the 
dosest 
# 
to 
its 
left. 
For 
example, 
the 
pair 
a a b a b b 
-q-
p--
occurring 
at 
a 
distance 
4 
from 
the 
dosest 
# 
to 
their 
left 
in 
a 
and 
ß, 
respectively, 
are 
consistent 
with 
b, 
since 
b(q,a) 
= 
(p,b,L). 
The 
pair 
abb 
abb 
occurring 
at 
distance 
7 
are 
consistent 
(any 
two 
identicallength-three 
strings 
are 
consistent, 
since 
this 
would 
occur 
if 
the 
tape 
head 
were 
far 
away). 
The 
pair 
a 
b 
a a 
b 
a 
--
p 
occurring 
at 
distance 
2 
are 
consistent, 
because 
there 
exists 
a 
transition 
moving 
left 
and 
entering 
state 
p. 
We 
can 
write 
down 
all 
consistent 
pairs 
of 
strings 
of 
length 
three 
over 
ß. 
For 
any 
configurations 
a 
and 
ß, 
if 
a 
2-. 
ß, 
then 
all 
corresponding 
substrings 
M 

_____________________________________________
254 
Lecture 
35 
of 
length 
three 
of 
0: 
and 
ß 
are 
consistent. 
Conversely, 
if 
all 
corresponding 
dubstrings 
of 
length 
three 
of 
0: 
and 
ß 
are 
consistent, 
then 
0: 
ß. 
Thus, 
1 
M 
to 
check 
that 
0: 
----+ 
ß 
does 
not 
hold, 
we 
need 
only 
check 
that 
there 
exists 
M 
a 
substring 
of 
0: 
of 
length 
three 
such 
that 
the 
corresponding 
substring 
of 
ß 
of 
length 
three 
is 
not 
consistent 
with 
the 
action 
of 
o. 
WE' 
now 
describe 
a 
nondeterministic 
PDA 
that 
accepts 
'" 
A
s
. 
We 
need 
to 
check 
t 
hat 
there 
exists 
i 
such 
that 
O:i+1 
does 
not 
follow 
from 
O:i 
according 
to 
o. 
The 
PDA 
will 
scan 
ac 
ross 
z 
and 
guess 
(}:i 
nondeterministically. 
It 
then 
checks 
that 
O:i+1 
does 
not 
follow 
from 
(}:i 
by 
guessing 
some 
length-three 
substring 
U 
of 
(}:i, 
remembering 
it 
in 
its 
finite 
control, 
and 
checking 
that 
the 
corresponding 
length-three 
substring 
v 
of 
(}:i+1 
is 
not 
consistent 
with 
u 
under 
the 
action 
of 
o. 
It 
uses 
its 
stack 
to 
check 
that 
the 
distance 
of 
u 
from 
the 
last 
# 
is 
the 
same 
as 
the 
distance 
of 
v 
from 
the 
last 
#. 
It 
does 
this 
by 
pushing 
the 
prefix 
of 
(}:i 
in 
front 
of 
U 
onto 
the 
stack 
and 
then 
popping 
as 
it 
scans 
the 
prefix 
of 
(}:i+l 
in 
front 
of 
v, 
checking 
that 
these 
two 
prefixes 
are 
the 
same 
length. 
For 
example, 
suppose 
o(q,a) 
= 
(p,b,R) 
and 
z 
contains 
the 
following 
string: 
#f-abaababb#f-abaabbbb#··· 
------q--
-----p---
T 
hen 
;; 
does 
not 
satisfy 
condition 
(5), 
because 
0 
said 
to 
go 
right 
but 
z 
went 
left. 
We 
can 
check 
with 
a 
PDA 
that 
this 
condition 
is 
violated 
by 
guessing 
where 
the 
error 
is 
and 
checking 
that 
the 
corresponding 
length-three 
subsequences 
are 
not. 
consistent 
with 
the 
action 
of 
o. 
Scan 
right, 
pushing 
symbols 
from 
the 
# 
up 
to 
the 
substring 
b a b 
-q -
(we 
nondeterministically 
guess 
where 
this 
is). 
Scan 
these 
three 
symbols, 
remembering 
them 
in 
the 
finite 
control. 
Scan 
to 
the 
next 
# 
without 
altering 
the 
stack, 
then 
scan 
and 
pop 
the 
stack. 
When 
the 
stack 
is 
empty, 
we 
are 
about 
to 
scan 
the 
symbols 
b b b 
p 
--
We 
scan 
these 
and 
compare 
them 
to 
the 
symbols 
from 
the 
first 
configuration 
we 
remembered 
in 
the 
finite 
control, 
and 
then 
we 
discover 
the 
error. 
We 
have 
given 
a 
nondeterministic 
PDA 
accepting 
'" 
A
s
. 
From 
this 
and 
the 
finite 
autornata 
for 
'" 
Ai, 
1 
::; 
i 
::; 
4, 
we 
can 
construct 
a 
CFG 
G 
for 
their 
union'" 
VALCOI\IPS(Af,:r), 
and 
L( 
C) 
= 
* 
{:=:} 
AI 
does 
not 
halt 
on 
x. 

_____________________________________________
Undecidable 
Problems 
About 
CFLs 
255 
If 
we 
could 
decide 
wh 
ether 
G 
generates 
all 
strings 
over 
its 
terminal 
alphabet, 
it 
would 
answer 
the 
question 
ofwhether 
M 
halts 
on 
x. 
We 
have 
thus 
reduced 
the 
halting 
problem 
to 
the 
question 
of 
whether 
a 
given 
gramm 
ar 
generates 
all 
strings. 
Since 
the 
halting 
problem 
is 
undecidable, 
we 
have 
shown: 
Theorem 
35.1 
It 
is 
undecidable 
for 
a 
given 
CPG 
Gwhether 
or 
not 
L( 
G) 
= 
E* 
. 
Many 
other 
simple 
problems 
involving 
CFLs 
are 
undecidable: 
whether 
a 
given 
CFL 
is 
a 
DCFL, 
whether 
the 
intersection 
of 
two 
given 
CFLs 
is 
a 
CFL, 
whether 
the 
complement 
of 
a 
given 
CFL 
is 
a 
CFL, 
and 
so 
on. 
These 
lems 
can 
all 
be 
shown 
to 
be 
undecidable 
using 
valid 
computation 
histories. 
We 
leave 
these 
a.s 
exercises 
(Miscellaneous 
Exercise 
121). 
Historical 
Notes 
Undecidable 
properties 
of 
context-free 
languages 
were 
established 
by 
Hillel 
et 
al. 
[8J, 
Ginsburg 
and 
Rose 
[47J, 
and 
Hartmanis 
and 
Hopcroft 
[56J. 
The 
idea 
of 
valid 
computation 
histories 
is 
essentially 
from 
Kleene 
[67, 
68], 
where 
it 
is 
called 
the 
T 
-predicate. 

_____________________________________________
Lecture 
36 
Other 
Formalisms 
In 
this 
lecture 
and 
the 
next 
we 
take 
a 
brief 
look 
at 
some 
of 
the 
other 
tional 
formalisms 
that 
are 
computatiqnally 
equivalent 
to 
Turing 
machines. 
Each 
one 
of 
these 
formalisms 
embodies 
a 
notion 
of 
computation 
in 
one 
form 
or 
another, 
and 
each 
can 
simulate 
the 
others. 
In 
addition 
to 
Turing 
machines, 
we'll 
consider 
Ł 
Post 
systems; 
Ł 
type 
0 
grammars; 
Ł 
JL-recursive 
functions 
(JL 
= 
"mu", 
Greek 
for 
m); 
Ł 
>.-calculus 
(>. 
= 
"lambda", 
Greek 
for 
1); 
Ł 
combinatory 
logic; 
and 
Ł 
while 
programs. 
Post 
Systems 
By 
the 
1920s, 
mathematicians 
had 
realized 
that 
much 
of 
formallogic 
was 
just 
symbol 
manipulation 
d.nd 
strongly 
related 
to 
emerging 
notions 
of 
putability. 
Emil 
Post 
cameup 
with 
a 
general 
formalism, 
now 
called 
Post 
systems, 
for 
talking 
about 
rearranging 
strings 
of 
symbols. 
A 
Post 
system 

_____________________________________________
Other 
Formalisms 
257 
consists 
of 
disjoint 
finite 
sets 
N 
and 
of 
nonterminal 
and 
terminal 
bols, 
respectively, 
a 
special 
start 
symbol 
SEN, 
a 
set 
of 
variables 
X
o
, 
Xl, 
... 
ranging 
over 
(N 
U 
1;)*, 
and 
a 
finite 
set 
of 
productions 
of 
the 
form 
XOX1X1X2X2X3'" 
Xnx
n 
-+ 
Y
O
Y
1
Y1Y2Y2Ya··· 
YmYm, 
where 
the 
Xi 
and 
Yj 
are 
strings 
in 
(N 
U 
and 
each 
Yj 
is 
some 
Xi 
that 
occurs 
on 
the 
left-hand 
side. 
If 
astring 
in 
(N 
U 
matches 
the 
left-hand 
side 
for 
some 
assignment 
of 
strings 
to 
the 
variables 
Xi, 
then 
that 
string 
can 
be 
rewritten 
as 
specified 
by 
the 
right-hand 
side. 
Astring 
x 
E 
is 
generated 
by 
the 
system 
if 
x 
can 
be 
derived 
from 
S 
by 
a 
finite 
sequence 
of 
such 
rewriting 
steps. 
Post 
systems 
and 
Turing 
machines 
are 
equivalent 
in 
computational 
power. 
Any 
Post 
system 
can 
be 
simulated 
by 
a 
TM 
that 
writes 
the 
start 
symbol 
on 
a 
track 
of 
its 
tape, 
then 
does 
the 
pattern 
matcliing 
and 
string 
ing 
according 
to 
the 
productions 
of 
the 
Post 
system 
in 
all 
possible 
ways, 
accepting 
if 
its 
input 
x 
is 
ever 
generated. 
Conversely, 
given 
any 
TM 
M, 
a 
Post 
system 
P 
can 
be 
designed 
that 
mimics 
the 
action 
of 
M. 
The 
sentential 
forms 
of 
P 
encode 
configurations 
of 
M. 
One 
of 
Post's 
main 
theorems 
was 
that 
Post 
systems 
in 
WhlCh 
all 
productions 
are 
of 
the 
more 
restricted 
form 
xX 
-+ 
Xy 
are 
just 
as 
powerful 
as 
general 
Post 
systems. 
Productions 
of 
this 
form 
say, 
"Take 
the 
string 
x 
off 
the 
front 
of 
the 
sentential 
form 
if 
it's 
there, 
and 
put 
Y 
on 
the 
back." 
lf 
you 
did 
Miscellaneous 
Exercise 
99 
on 
queue 
machines, 
you 
may 
have 
already 
recognized 
that 
this 
is 
essentially 
the 
same 
result. 
rype 
0 
Grammars 
Grammars 
are 
a 
restricted 
dass 
of 
Post 
systems 
that 
arose 
in 
formal 
guage 
theory. 
There 
is 
a 
natural 
hierarchy 
of 
grammars, 
called 
the 
Chomsky 
hierarchy, 
which 
classifies 
grammars 
into 
four 
types 
named 
0, 
1, 
2, 
and 
3. 
The 
type 
2 
and 
type 
3 
grammars 
are 
just 
the 
context-free 
ar:.':: 
fight-linear 
grammars, 
respectively, 
which 
we 
have 
"already 
seen. 
A 
more 
general 
dass 
of 
grammars, 
called 
the 
type 
0 
or 
unrestricted 
grammars, 
are 
much 
like 
CFGs, 
except 
that 
produ('tions 
may 
be 
of 
the 
more 
general 
form 
Q 
-> 
ß, 
(36.1) 
where 
Q 
and 
ß 
are 
any 
strings 
of 
terminals 
and 
nonterminals 
whatsoever. 
A 
type 
0 
grammar 
consists 
of 
a 
finite 
set 
of 
such 
productions. 
If 
the 
hand 
side 
of 
a 
production 
matches 
a 
substring 
of 
a 
sentential 
form, 
then 
the 
substring 
ran 
be 
replaced 
by 
the 
right-hand 
side 
of 
the 
production. 
A 

_____________________________________________
258 
Lecture 
36 
string 
x 
of 
terminal 
symbols 
is 
generated 
by 
C, 
that 
is, 
x 
E 
L( 
C), 
if 
x 
can 
be 
derived 
from 
the 
start 
symbol 
S 
by 
some 
finite 
number 
of 
such 
applications; 
in 
symbols, 
S 
x. 
G 
Type 
0 
grammars 
are 
a 
special 
case 
of 
Post 
systems: 
the 
grammar 
tion 
(36.1) 
corresponds 
to 
the 
Post 
production 
XaY 
-+ 
XßY. 
Type 
0 
gramm 
ars 
are 
the 
most 
powerful 
grammars 
in 
the 
Chomsky 
archy 
of 
grammars 
and 
generate 
exactly 
the 
r.e. 
sets. 
One 
can 
easily 
build 
a 
Turing 
machine 
to 
simulate 
a 
given 
type 
0 
grammar. 
The 
machine 
saves 
its 
input 
x 
on 
a 
track 
of 
its 
tape. 
It 
then 
writes 
the 
start 
symbol 
S 
of 
the 
grammar 
on 
another 
track 
and 
applies 
productions 
nondeterministiCally, 
accepting 
if 
its 
input 
string 
x 
is 
ever 
generated. 
Conversely, 
type 
0 
gIGlnmars 
can 
simulate 
Turing 
machines. 
Intuitively, 
sentential 
forms 
of 
the 
grammar 
encode 
configurations 
of 
the 
machine, 
and 
the 
productions 
simulate 
8 
(Miscellaneous 
Exercise 
104). 
Type 
1 
grammars 
are 
the 
context-sensitive 
grammars 
(CSGs). 
These are 
like 
type 
0 
grammars 
with 
productions 
of 
the 
form 
(36.1), 
except 
that 
we 
impose 
the 
extra 
restriction 
that 
lai::; 
IßI. 
Context-sensitive 
grammars 
are 
equivalent 
(except 
for 
a 
trivial 
glitch 
involving 
the 
null 
string) 
to 
terministic 
linear 
bounded 
automata 
(LBAs), 
wh 
ich 
are 
TMs 
that 
cannot 
write 
on 
the 
blank 
portion 
of 
the 
tape 
to 
the 
right 
of 
the 
input 
string 
(see 
Exercise 
2 
of 
Homework 
8 
and 
Exercise 
2 
of 
Homework 
12). 
The 
bound 
on 
the 
tape 
in 
LBAs 
translates 
to 
the 
restrietion 
lai::; 
IßI 
for 
CSGs. 
The 
p,-Recursive 
Functions 
Gödel 
defined 
a 
collection 
of 
number-theoretic 
functions 
N
k 
-+ 
N 
that, 
according 
to 
his 
intuition, 
represented 
all 
the 
computable 
functions. 
His 
definition 
was 
as 
folIows: 
(1) 
Successor. 
The 
function 
s : N 
-> 
N 
given 
by 
s(x) 
= 
x+1 
is 
computable. 
(2) 
Zero. 
The 
function 
z : 
Nl 
-+ 
N 
given 
by 
z( 
) 
= 
0 
is 
computable. 
(3) 
Projections. 
The 
functions 
1rr:: 
Nn 
-+ 
N 
given 
by 
1r;:(X1,'" 
,x
n
) 
= 
Xk, 
1 
::; 
k 
::; 
n, 
are 
computable. 
(4) 
Composition. 
If 
f 
: 
N
k 
-+ 
N 
and 
gl,' 
.. 
,gk 
: 
Nn 
-+ 
N 
are 
computable, 
then 
so 
is 
the 
function 
f 
0 
(gI, 
... 
,gk) 
: 
N
n 
-+ 
N 
that 
on 
input 
x 
= 
Xl, 
... 
,X
n 
gives 

_____________________________________________
Example 
36.1 
Other 
Formalisms 
259 
(5) 
Primitive 
recursion. 
If 
hi 
: 
Nn-l 
-+ 
N 
and 
gi 
: 
-+ 
N 
are 
putable, 
1 
i 
k, 
then 
so 
are 
the 
functions 
fi 
: 
Nn 
-+ 
N, 
1 
i 
k, 
defined 
by 
mutual 
induction 
as 
follows: 
/i(0, 
x) 
hi(X), 
/i(x 
+ 
1,x) 
gi(X,x,/l(x,x), 
... 
,/k(x;x)), 
where 
x 
= 
X2, 
Ł
ŁŁ 
,x
n
. 
(6) 
Unbounded 
minimization. 
If 
9 
: 
-+ 
N 
is 
computable, 
then 
so 
is 
the 
function 
f 
: 
-+ 
N 
that 
on 
input 
x 
= 
Xl, 
... 
,X
n 
gives 
the 
least 
y 
such 
that 
g(z, 
x) 
is 
defined 
for 
all 
z 
y 
and 
g(y, 
x) 
= 
0 
if 
such 
a 
y 
exists 
and 
is 
undefined 
otherwise. 
We 
denote 
this 
by 
f(x) 
= 
p,y.(g(y, 
x) 
= 
0). 
The 
functions 
defined 
by 
(1) 
through 
(6) 
are 
called 
the 
p,-recursive 
tions. 
The 
functions 
defined 
by 
(1) 
through 
(5) 
only 
are 
called 
the 
primitive 
recursive 
functions. 
Ł 
The 
constant 
functions 
const
n
( ) 
= 
n 
are 
primitive 
recursive: 
def 
const
n 
= 
so··· 
0 
s 
oz. 
'--v--" 
n 
Ł 
Addition 
is 
primitive 
recursive, 
since 
we 
can 
define 
( 
) 
def 
add 
O,y 
= 
y, 
add(x 
+ 
1,y) 
s(add(x,y)). 
This 
is 
a 
bona 
fide 
definition 
by 
primitive 
recursion: 
in 
rule 
(5) 
above, 
take 
k 
= 
1, 
n 
= 
2, 
h 
= 
lI"f, 
and 
9 
= 
s 
0 
Then 
add(O,y) 
= 
h(y) 
= 
y, 
add(x 
+ 
l,y) 
= 
g(x,y,add(x,y)) 
= 
s(add(x,y)) 
. 
Ł 
Multiplication 
is 
primitive 
recursive, 
since 
mult(O,y) 
0, 
mult(x 
+ 
1, 
y) 
add(y, 
mult(x, 
y)). 
Note 
how 
we 
used 
the 
function 
add 
defined 
previously. 
We 
are 
allowed 
to 
build 
up 
primitive 
recursive 
functions 
inductively 
in 
this 
way 
. 
Ł 
Exponentiation 
is 
primitive 
recursive, 
since 
( 
) 
def 
exp 
x,O 
= 
1, 
exp(x,y 
+ 
1) 
mult(x,exp(x,y)). 

_____________________________________________
260 
Lecture 
36 
Ł 
The 
predecessor 
function 
{
X 
-1 
x..:.. 
1 
= 
0 
is 
primi"tive 
recursive: 
0..:.. 
1 
0, 
(x 
+ 
1) 
..:.. 
1 
x. 
Ł 
Proper 
subtraction 
if 
x> 
0, 
if 
x 
= 
0 
. { 
x-y 
x 
-
y 
= 
0 
if 
x 
2 
y, 
if 
x< 
y 
is 
primitive 
recursive, 
and 
can 
be 
defined 
from 
predecessor 
in 
exactly 
the 
same 
way 
that 
addition 
is 
defined 
from 
successor. 
Ł 
The 
sign 
function 
is 
primitive 
recursive: 
Ł " ( ) 
def 
1 . ( 
1· 
) 
slgn 
x 
= --
x 
{
I 
if 
x> 
0, 
= 
0 
if 
x 
= 
O. 
Ł 
The 
relatIOns 
<, 
:S, 
>, 
2, 
=, 
and 
I, 
considered 
as 
(O,l)-valued 
tions, 
are 
all 
primitive 
recursivej 
for 
example, 
compare::;(x,y) 
1..:.. 
sign(x..:.. 
y) 
= { 
ifx:Sy, 
ifx 
> 
y. 
Ł 
Functions 
can 
be 
defined 
by 
cases. 
For 
example, 
{ 
x+ 
1 
g(x,y)
= 
x 
is 
primitive 
recursive: 
if 
2'" 
< 
y, 
if 
2'" 
2 
y 
g(x,y) 
comp
are
d2"',y). 
(x 
+ 
1) 
+ 
x. 
Ł 
Inverses 
of 
certain 
fUllctions 
can 
be 
defined. 
For 
example, 
rlog2 
y 
1 
is 
primitive 
recursive:
1 
rlog2 
y 
1 
= 
!(y, 
y), 
where 
f(O,y) 
0, 
f(x 
+ 
1,y) 
g(f(x,y),y), 
1 
r 
x 
1 
= 
least 
integer 
not 
less 
than 
Xj 
log2 
= 
base 
2 
logarithm. 

_____________________________________________
Other 
Formalisms 
261 
and 
9 
is 
from 
the 
previous 
example. 
The 
function 
f 
just 
continues 
to 
add 
1 
to 
its 
first 
argument 
x 
until 
the 
condition 
2'" 
? 
y 
is 
satisfied. 
This 
must 
happen 
for 
some 
x 
:=:; 
y. 
Inverses 
of 
other 
common 
functions, 
such 
as 
square 
root, 
can 
be 
defined 
similarly. 
0 
Observe 
that 
all 
the 
primitive. 
recursive 
functions 
are 
total, 
whereas 
a 
recursive 
function 
may 
not 
be. 
There 
exist 
total 
computable 
functions 
that 
are 
not 
primitive 
recursive; 
one 
example 
is 
Ackerm.ann's 
function: 
A(x 
+ 
1,0) 
A(x, 
1), 
A(x,A(x+1,y)). 
(36.2) 

_____________________________________________
Lecture 
37 
The 
A-Cal.culus 
The 
A-calculus 
(A 
= 
"lambda," 
Greek 
for 
1) 
consists 
of 
a 
set 
of 
objects 
called 
A-terms 
and 
some 
rules 
for 
manipulating 
them. 
It 
was 
originally 
designed 
to 
capture 
formally 
the 
notions 
of 
functional 
abstraction 
and 
functional 
application 
and 
their 
interaction. 
The 
A-calculus 
has 
had 
a 
profound 
impact 
on 
computing. 
One 
can 
see 
the 
basic 
principles 
of 
the 
A-calculus 
at 
work 
in 
the 
functional 
programming 
language 
LISP 
and 
its 
more 
modern 
offspring 
SCHEME 
and 
DYLAN. 
In 
mathematics, 
A-notation 
is 
commonly 
used 
to 
represent 
functions. 
The 
expression 
Ax.E(x) 
denotes 
a 
function 
that 
on 
input 
x 
computes 
E(x). 
To 
apply 
this 
function 
to 
an 
input, 
one 
substitutes 
the 
input 
for 
the 
variable 
x 
in 
the 
body 
E( 
x) 
and 
evaluates 
the 
resulting 
expression. 
For 
example, 
the 
expression 
AX.(X 
+ 
1) 
might 
be 
used 
to 
denota 
the 
successor 
function 
on 
natural 
numbers. 
To 
apply 
this 
function 
to 
the 
input 
7, 
we 
would 
substitute 
7 
for 
x 
in 
the 
body 
and 
evaluate: 
(AX.(X 
+ 
1))7 
-+ 
7 
+ 
1 
= 
8. 
In 
the 
programming 
language 
DYLAN, 
one 
would 
write 

_____________________________________________
The 
>'-Calculus 
263 
(method 
(x) 
(+ 
xi)) 
for 
the 
same 
thing. 
The 
keyword 
method 
is 
really 
>. 
in 
disguise. 
If 
you 
typed 
((method 
(x) 
(+ 
xi)) 
7) 
at 
a 
DYLAN 
interpreter, 
it 
would 
print 
out 
8. 
For 
another 
example, 
the 
expression 
>.x·f(gx) 
denotes 
the 
composition 
of 
the 
functions 
fand 
gj 
that 
is, 
the 
function 
that 
on 
input 
x 
applies 
9 
to 
x, 
then 
applies 
f 
to 
the 
result. 
The 
expression 
>,f.>.g.>.x·f(gx) 
(37.1) 
denotes 
the 
function 
that 
takes 
functions 
fand 
gasinput 
and 
gives 
back 
their 
composition 
>.x:f(gx). 
In 
DYLAN 
one 
would 
write 
(method 
(f) 
(method 
(g) 
(method 
(x) 
(g 
x))))) 
To 
see 
how 
this 
works, 
let's 
apply 
(37.1) 
to 
the 
successor 
function 
twice. 
We 
use 
different 
variables 
in 
the 
successor 
functions 
below 
for 
clarity. 
The 
symbol 
-+ 
denotes 
one 
substitution 
step. 
(,\f.>.g.>.x.(f(gx))) 
(>.y.(y 
+ 
1)) 
(>.z.(z 
+ 
1)) 
substitute 
>.y.(y 
+ 
1) 
for 
f 
-+ 
(>.g.>.x.«>.y.(y 
+ 
1)) 
(gx))) 
(>.z.(z+ 
1)) 
substitute 
>.z.(z 
+ 
1) 
for 
9 
-+ 
>.x.(>.y.(y 
+ 
1)) 
«>.z.(z 
+ 
1))x)) 
substitute 
x 
for 
z 
-+ 
>.x.«>.y.(y 
+ 
1)) 
(x 
+ 
1)) 
substitute 
x 
+ 
1 
for 
y 
-+ 
>.x.«x 
+ 
1) 
+ 
1) 
We 
could 
have 
substituted 
gx 
for 
y 
in 
the 
second 
step 
or 
(>.z.(z 
+ 
1))x 
for 
y 
in 
the 
thirdj 
we 
would 
have 
arrived 
at 
the 
same 
final 
result. 
Functions 
represented 
by 
>'-terms 
have 
only 
one 
input. 
A 
function 
with 
two 
inputs 
x, 
y 
that 
returns 
a 
value 
M 
is 
modeled 
by 
a 
function 
with 
one 
input 
x 
that 
returns 
a 
function 
with 
one 
input 
y 
that 
returns 
a 
value 
M. 
The 
technical 
term 
for 
this 
trick 
is 
currying 
(after 
Haskell 
B. 
Curry). 
The 
Pure 
>.-Calculus 
In 
the 
pure 
>.-calculus, 
there 
are 
only 
variables 
{f, 
g, 
h, 
x, 
y, 
.
.. 
} 
and 
tors 
for 
A-abstraction 
and 
application. 
Syntactic 
objects 
called 
A-terms 
are 
buHt 
inductively 
from 
these: 

_____________________________________________
264 
Lecture 
37 
Ł 
any 
variable 
x 
is 
a 
>'-term; 
Ł 
if 
M 
and 
N 
are 
>.-terms, 
then 
MN 
is 
a 
>'-term 
(fuIl:ctional 
think 
of 
M 
as 
a 
funttion 
that 
is 
about 
to 
be 
applied 
to 
input 
N); 
and 
Ł 
if 
M 
is 
a 
>'-term 
and 
x 
is 
a 
variable, 
then 
>.x.M 
is 
a 
>'-term 
(functional 
ahstraction-think 
of 
>.x.M 
as 
the 
function 
that 
on 
input 
x 
computes 
M). 
The 
operation 
of 
application 
is 
not 
associative, 
and 
unparenthesized 
pressions 
are 
conventionally 
associated 
to 
the 
left; 
thus, 
MNP 
should 
be 
parsed 
(MN)P. 
In 
the 
pure 
>.-calculus, 
>'-terms 
serve 
as 
both 
functions 
and 
data. 
There 
is 
nothing 
like 
"+1" 
as 
we 
used 
it 
informally 
above, 
unless 
we 
encode 
it 
somehow. We'll 
show 
how 
to 
do 
this 
below. 
The 
substitution 
rule 
described 
informally 
above 
is 
called 
ß-reduction. 
mally, 
this 
works 
as 
follows. 
Whenever 
our 
>'-term 
contains 
a 
subterm 
of 
the 
form 
(>.x.M)N, 
we 
can 
replace 
this 
subterm 
by 
the 
term 
sN(M), 
where 
sN(M) 
denotes 
the 
term 
obtained 
by 
(i) 
renaming 
the 
bound 
variables 
of 
M 
(those 
y 
occurring 
in 
thc 
scope 
of 
so 
me 
>.y) 
as 
necessary 
so 
that 
neither 
x 
nor 
any 
variable 
of 
N 
occurs 
bound 
in 
M; 
and 
(ii) 
substituting 
N 
for 
all 
occurrences 
of 
x 
in 
the 
resulting 
term. 
Step 
(i) 
is 
necessary 
only 
to 
make 
sure 
that 
any 
free 
variables 
y 
of 
N 
will 
not 
be 
inadvertently 
captured 
by 
a 
>.y 
occurring 
in 
M 
when 
the 
substitution 
is 
done 
in 
step 
(ii). 
This 
is 
the 
same 
problem 
that 
comes 
up 
in 
first-order 
logic. 
We 
can 
rename 
bound 
variables 
in 
>'-terms 
anytime, 
since 
their 
behavior 
as 
functions 
is 
not 
changed. 
For 
example, 
we 
can 
rewrite 
>.y.xy 
as 
>.z.xz; 
intuitively, 
the 
function 
that 
on 
input 
y 
applies 
x 
to 
y 
is 
the 
same 
as 
the 
function 
that 
on 
input 
z 
applies 
x 
to 
z. 
The 
process 
of 
renaming 
bound 
variables 
is 
officially 
called 
Q-reduction. 
We 
denote 
Q-
and 
ß-reduction 
by 
....!.. 
and 
L, 
respectively. 
Thus 
(>.x.M)N 
L 
sN(M). 
Computation 
in 
the 
>.-calculus 
is 
performed 
by 
ß-reducing 
subterms 
ever 
possible 
and 
for 
as 
long 
as 
possible. 
The 
order 
of 
the 
reductions 
doesn't 
matter, 
since 
there 
is 
a 
theorem 
that 
says 
that 
if 
you 
can 
reduce 
M 
to 
NI 
by 
some 
sequence 
of 
reduction 
steps 
and 
M 
to 
N
2 
by 
some 
other 
sequence 
of 
reduction 
steps, 
then 
there 
exists 
a 
term 
P 
such 
that 
both 
NI 
and 
N2 
reduce 
to 
P. 

_____________________________________________
The 
A-Calculus 
265 
M 
P 
This 
is 
called 
the 
Church-Rosser 
property 
after 
Alonzo 
Church 
and 
J. 
Barkley 
Rosser. 
A 
term 
is 
said 
to 
be 
in 
normal 
form 
if 
no 
ß-reductions 
apply; 
that 
is, 
if 
it 
has 
no 
subterms 
of 
the 
form 
(Ax.M)N. 
A 
normal 
form 
corresponds 
roughly 
to 
a 
halting 
configuration 
of 
a 
Turing 
machine. 
By 
the 
Rosser 
property, 
if 
a 
A-term 
has 
a 
normal 
form, 
then 
that 
normal 
form 
is 
unique 
up 
to 
a-renaming. 
There 
are 
terms 
with 
no 
normal 
form. 
These 
correspond 
to 
nonhalting 
computations 
of 
Turing 
machines. 
For 
example, 
the 
A-term 
(AX.XX)(AX.XX) 
has 
no 
normal 
form-try 
to 
do 
a 
ß-reduetion 
and 
see 
what 
happens! 
The 
term 
AX.XX 
is 
analogou!': 
to 
a 
Turing 
maehine 
that 
on 
input 
x 
runs 
M., 
on 
x. 
Church 
Numerals 
To 
simulate 
the 
tt-recursive 
functions 
in 
the 
A-calculus, 
we 
must 
first 
encode 
the 
natural 
numbers 
as 
A-terms 
so 
tlwy 
can 
be 
used 
in 
computations. 
Alonzo 
Church 
came 
up 
with 
a 
niee 
way 
to 
do 
this. 
His 
encoding 
is 
kriown 
as 
the 
Church 
numerals: 
-
der 
o 
= 
Af.AX.X, 
-
der 
1 
= 
).j.AJ;.J 
X, 
-
der 
2 
= 
>'f.Ax.JUx), 
-
def 
3 
= 
>'f.>.x.f(f(fx)), 
-
def 
'I' 
In 
n 
= 
1\ 
.I\X. 
X, 

_____________________________________________
266 
Lecture 
37 
where 
fn
x 
is 
an 
abbreviation 
for 
the 
term 
f(l(··· 
(I 
x)·· 
.)). 
n 
In 
other 
words, 
n 
represents 
a 
function 
that 
on 
input 
f 
returns 
ihe 
n-fold 
composition 
of 
f 
with 
itself. 
The 
n 
are 
aH 
distinct 
and 
in 
normal 
form. 
Using 
this 
representation 
of 
the 
natural 
numbers, 
the 
successor 
function 
can 
be 
defined 
as 
s 
Am.Af.Ax.f(mfx). 
To 
see 
that 
this 
is 
correct, 
try 
applying 
it 
to 
any 
n: 
sn 
= 
(Am.Af.Ax.f(mfx)) 
(Af.AX·rX) 
(Am.Ag.Ay.g(mgy)) 
(Af.AX.rX) 
L 
Ag.Ay.g((Af.AX.rX)gy) 
L 
Ag.Ay.g((AX.gnX)Y) 
L 
Ag.Ay.g(gn
y
) 
= 
Ag.Ay.gn+l
y 
Af·AX·r+
1
X 
= 
n+ 
1. 
One 
can 
likewise 
define 
addition, 
multiplication, 
and 
all 
the 
other 
IL-recursive 
functions. 
Combinatory 
Logic 
Combinatory 
logic 
is 
a 
form 
of 
variable-free 
A-calculus. 
It 
was 
first 
invented 
to 
study 
the 
mathematics 
of 
symbol 
manipulation, 
especially 
substitution. 
The 
system 
consists 
of 
terms 
called 
combinators 
that 
are 
manipulated 
using 
reduction 
rules. 
There 
are 
two 
primitive 
combinators 
Sand 
K, 
which 
are 
just 
symbols, 
as 
weH 
as 
a 
countable 
set 
of 
variables 
{X, 
Y, 
.
.. 
}. 
More 
complicated 
tors 
are 
formed 
inductively: 
S, 
K, 
and 
variables 
are 
combinators; 
and 
if 
M 
and 
N 
are 
then 
so 
is 
MN. 
Here 
MN 
is 
just 
a 
term, 
a 
syntactic 
object, 
but 
we 
can 
think 
ofM 
as 
a 
function 
and 
N 
as 
its 
input; 
thus, 
MN 
represents 
the 
application 
of 
M 
to 
N. 
As 
with 
the 
>..-calculus, 
this 
operation 
is 
not 
associative, 
so 
we 
use 
parentheses 
to 
avoid 
ambiguity. 
By 
convention, 
astring 
of 
applications 
associates 
to 
the 
left; 
thus, 
XYZ 
should 
be 
parsed 
(XY)Z 
and 
not 
X( 
YZ). 

_____________________________________________
The 
A-Calculus 
267 
Computation 
proceeds 
according 
to 
two 
reduction 
ruIes, 
one 
for 
Sand 
one 
for 
K. 
For 
any 
terms 
M, 
N, 
and 
P, 
SMNP 
MP(NP), 
Computation 
in 
this 
system 
consists 
of 
a 
sequence 
of 
reduction 
steps 
applied 
to 
subterms 
of 
a 
term. 
Other 
combinators 
can 
be 
built 
from 
Sand 
K. 
For 
exampIe, 
the 
combinator 
I 
SKK 
acts 
as 
the 
identity 
function: 
for 
any 
X, 
IX 
= 
SKKX 
KX(KX) 
the 
S 
rule 
X 
the 
K 
rule. 
Let 
B 
= 
SK. 
Whereas 
K 
picks 
out 
the 
first 
element 
of 
a 
pair, 
B 
picks 
out 
the 
second 
element: 
BXY 
= 
SKXY 
KY(XY) 
...... 
Y. 
One 
can 
construct 
fancy 
combinators 
from 
Sand 
K 
that 
can 
rearrange 
symbols 
in 
every 
conceivable 
way. 
For 
exampIe, 
to 
take 
two 
inputs 
and 
apply 
the 
second 
to 
the 
first, 
use 
the 
combinator 
C 
= 
S(S(KS)B)K: 
CXY 
= 
S(S(KS)B)KXY 
S(KS)BX(KX)Y 
KSX(BX)(KX)Y 
S(BX)(KX)Y 
...... 
BXY(KXY) 
YX. 
There 
is 
a 
theorem 
that 
says 
that 
no 
matter 
how 
you 
want 
to 
rearrange 
your 
inputs, 
there 
is 
a 
combinator 
built 
from 
Sand 
K 
only 
that 
can 
do 
it. 
In 
other 
words, 
for 
any 
term 
M 
built 
from 
Xl, 
... 
, X
n 
and 
the 
application 
operator, 
there 
is 
a 
combinator 
D 
built 
from 
Sand 
K 
only 
such 
that 
DX
I
X
2 
ŁŁŁ 
X
n 
M. 
This 
theorem 
is 
called 
combinatorial 
completeness. 
There 
is 
a 
paradoxical 
combinator 
SII(SI!), 
which 
corresponds 
to 
the 
term 
(.xx.xx)( 
AX.XX). 
Like 
its 
counterpart, 
it 
has 
no 
normal 
form. 
Like 
the 
>.-ca1culus, 
combinatory 
logic 
is 
powerful 
enough 
to 
simulate 
Turing 
machines. 

_____________________________________________
268 
Lecture 
37 
Historical 
Notes 
The 
late 
1920s 
and 
1930s 
were 
a 
hectic 
time. 
Turing 
machines 
(Turing 
[120]), 
Post 
(Post 
f99, 
100]), 
",-recursive 
functiöns 
(Gödel 
[51J, 
.brand, 
Kleene 
[67]), 
the 
A-calculus 
(Church 
[23, 
24, 25, 
26J, 
Kleene 
[66J,' 
Rosser 
[107]), 
and 
combinatory 
logic 
(Schönfinkel 
[111J, 
Curry 
[29]) 
were 
all 
developed 
around 
this 
time. 
The 
),-calculus 
is 
a 
topic 
unto 
itself. 
Barendregt's 
book 
[9] 
is 
an 
able 
reference. 
The 
JL-recursive 
functions 
were 
formulated 
by 
Gödel 
and 
presented 
in 
a 
series 
of 
lectures 
at 
Princeton 
in 
1934. 
According 
to 
Church 
[25], 
Gödel 
acknowledged 
that 
he 
got 
the 
idea 
originally 
from 
Jacques 
Herbrand 
in 
conversation. 
A 
proof 
of 
the 
equivalence 
of 
the 
",-recursive 
functions 
and 
the 
),-calculus 
first 
appeared 
in 
Church 
[25], 
although 
Church 
attributes 
the 
proof 
chiefly 
to 
Kleene. 
The 
equivalence 
of 
TMs 
and 
the 
A-calculus 
was 
shown 
by 
Turing 
[120]. 
Various 
perspectives 
on 
this 
important 
period 
can 
be 
found 
in 
Kleene 
[69], 
Davis 
[31, 
32], 
Rogers 
[106], 
Yasuhara 
[124], 
Jones 
[63], 
Brainerd 
and 
Landweber 
[15], 
Hennie 
[58], 
and 
Machtey 
and 
Young 
[81J. 
Chomsky 
[18] 
defined 
the 
Chomsky 
hierarchy 
and 
proved 
that 
the 
type 
0 
grammars 
generate 
exactly 
the 
r.e. 
sets. 
The 
relationship 
between 
context-sensitive 
grammars 
and 
linear 
bounded 
automata 
was 
studiM 
by 
Myhill 
[92], 
Landweber 
[78], 
and 
Kuroda 
[77]. 

_____________________________________________
Supplementary 
Lecture 
I 
While 
Programs 
We 
can 
relate 
the 
primitive 
and 
{L-recursive 
functions 
of 
Gödel 
to 
more 
modern 
concepts. 
Consider 
a 
simple 
programming 
language 
with 
variables 
Val' 
= 
{x, 
y, 
.
.. 
} 
ranging 
over 
N 
containing 
the 
following 
constructs: 
(i) 
.simple 
assignments 
x 
:= 
0 
x:= 
y 
+ 
1 
x:= 
y 
(ii) 
iil'quential 
cumpo8itiun 
p; 
q 
(iii) 
fonditiunal 
if 
x 
< 
y 
then 
p 
else 
q 
(iv) 
for 
loop 
for 
y 
do 
p 
(v) 
while 
loop 
while 
x 
< 
y 
do 
p 
In 
(iii) 
and 
(v), 
the 
relation< 
can 
be 
by 
any 
one 
of 
>, 
2:, 
::;, 
=, 
or 
=1=. 
In 
(ii) 
we 
can 
parenthesize 
using 
begin 
... 
end 
if 
necessary. 
Programs 
built 
ind 
uctively 
from 
these 
constructs 
are 
called 
while 
grams. 
Prograrns 
built 
without 
the 
while 
construct 
(v) 
are 
called 
fOl' 
grams. 
We 
will 
show 
in 
Theorem 
1.1 
that 
while 
programs 
compute 
actly 
the 
{L-recursive 
funetions 
and 
t.hat 
for 
programs 
compute 
exactly 
the 
primitive 
recursive 
functions. 
The 
intuitive 
opemt 
ion 
of 
the 
for 
loop 
is 
as 
folIows: 
upon 
entering 
the 
loop 
for 
y 
do 
p, 
the 
current 
valuf' 
of 
variable 
y 
is 
determined, 
and 
the 
program 

_____________________________________________
270 
Supplementary 
Lecture 
I 
p 
is 
executed 
that 
many 
times. 
Assignment 
to 
the 
variable 
y 
within 
the 
body 
of 
the 
loop 
does 
not 
change 
the 
nu 
mb 
er 
of 
times 
the 
loop 
is 
executed, 
nor 
does 
execution 
of 
the 
body 
of 
the 
loop 
alone 
decrement 
y 
or 
change 
its 
value 
in 
any 
way 
except 
by 
explicit 
assignment. 
The 
intuitive 
operation 
of 
the 
while 
loop 
is 
as 
follows: 
upon 
entering 
the 
loop 
while 
x 
< 
y 
dü 
p, 
the 
condition 
x 
< 
y 
is 
tested 
with 
the 
current 
values 
of 
the 
variables 
x, 
y. 
If 
the 
condition 
is 
false, 
then 
the 
body 
of 
the 
loop 
is 
not 
executed, 
and 
control 
passes 
through 
to 
the 
statement 
following 
the 
while 
loop. 
If 
the 
condition 
is 
true, 
then 
the 
body 
p 
of 
the 
loop 
is 
executed 
once, 
and 
then 
the 
procedure 
is 
repeated 
with 
the 
new 
values 
of 
x, 
y. 
Thus 
the 
while 
loop 
repeatedly 
tests 
the 
condition 
x 
< 
y, 
and 
if 
true, 
executes 
the 
body 
p. 
The 
first 
time 
that 
the 
condition 
x 
<. 
y 
tests 
false 
(if 
ever), 
the 
body 
of 
the 
loop 
is 
not 
executed 
and 
control 
passes 
immediately 
to 
the 
statement 
following 
the 
loop. 
If 
the 
condition 
always 
tests 
true, 
then 
the 
while 
loop 
never 
halts, 
as 
for 
example 
with 
the 
program 
while 
x 
= 
x 
dü 
x 
:= 
X 
+ 
1. 
In 
the 
presence 
of 
the 
while 
loop, 
the 
für 
loop 
is 
redundant: 
für 
y 
dü 
pis 
simulated 
by 
the 
while 
program 
z 
:= 
0; 
w 
:= 
y 
; 
while 
z 
< 
w 
dü 
begin 
p; 
z 
:= 
z 
+ 
1 
end 
where 
z 
and 
ware 
variables 
not 
occurring 
in 
p. 
However, 
note 
that 
für 
pro 
grams 
always 
halt. 
Thus 
the 
only 
source 
of 
potential 
nontermination 
is 
the 
while 
loop. 
Semantics 
of 
While 
Programs 
In 
order 
to 
prove 
the 
equivalence 
of 
while 
programs 
and 
the 
p,-recursive 
functions, 
we 
must 
give 
formal 
semantics 
for 
while 
programs. 
Astate 
or 
environment 
a 
is 
an 
assignment 
of 
a 
nonnegative 
integer 
to 
each 
variable 
in 
Var; 
that 
is, 
a 
: 
Var 
----> 
N. 
The 
set 
of 
all 
such 
environments 
is 
denoted 
Env. 
If 
a 
program 
is 
started 
in 
an 
initial 
environment 
a, 
then 
in 
the 
course 
of 
execution, 
the 
values 
of 
variables 
will 
be 
changed, 
so 
that 
if 
and 
when 
the 
program 
halts, 
the 
final 
environment 
will 
in 
general 
be 
different 
from 
a. 
We 
thus 
interpret 
programs 
pas 
partial 
functions 
: 
Env 
----> 
Env. 
The 
value 
is 
the 
final 
environment 
after 
executing 
the 
program 
p 
with 
initial 
environment 
a, 
provided 
p 
halts. 
If 
p 
does 
not 
halt 
when 
started 
in 
initial 
environment 
a, 
then 
[PD 
(a) 
is 
undefined. 
Thus 
[PD 
: 
Env 
-+ 
Env 
is 
a 
partial 
function; 
its 
domain 
is 
the 
set 
of 
a 
causing 
p 
to 
halt. 
Note 
that 
whether 
or 
not 
p 
halts 
depends 
on 
the 
initial 
environment; 
for 
example, 
if 
a(x) 
= 
0, 
then 
the 
program 
while 
x 
> 
0 
dü 
x 
:= 
x 
+ 
1 
halts 
on 
initial 
environment 
a, 
whereas 
if 
a(x) 
= 
1, 
then 
it 
does 
not. 

_____________________________________________
While 
Programs 
271 
Formally, 
the 
meaning 
[PD 
of 
a 
while 
program 
p 
is 
defined 
inductively 
as 
follows. 
For 
u 
E 
Env, 
x 
E 
Var, 
and 
a 
E 
N, 
let 
u[x 
+-
a) 
denote 
the 
environment 
that 
is 
identical 
to 
u 
except 
for 
the 
value 
of 
x, 
wh 
ich 
is 
a. 
Formally, 
u[x 
+-
a)(y) 
u(y), 
if 
y 
is 
not 
x, 
u[x 
+-
a)(x) 
a. 
Let 
[pr 
denote 
the 
n-fold 
composition 
of 
the 
partial 
function 
[p]: 
[pr 
= 
[PD 
0 
···0 
[PD, 
---....-.. 
n 
where 
[P]o 
is 
the 
identity 
function 
on 
Env. 
Formally, 
[Pt(u) 
u, 
[pr+
1
(u) 
[p]([Pr(u)). 
Now 
define 
[x 
:= 
O](u) 
u[x 
+-
0), 
[x 
:= 
y](u) 
dg 
oix 
+-
u(y)), 
[x 
:= 
y 
+ 
l](u) 
u[x 
+-
u(y) 
+ 
1), 
[P 
j 
q](u) 
[q]([P](u)), 
or 
in 
other 
words, 
[p 
j 
q] 
[q] 
0 
[PD 
(here 
[q]([PD(u)) 
is 
undefined 
if 
[P](u) 
is 
undefined), 
[ir 
x< 
y 
then 
p 
else 
q](u) 
{[P](u) 
if 
u(x) 
< 
u(y), 
-
[q](u) 
otherwise, 
[ror 
y 
do 
p](u) 
[P]"(y) 
(u), 
[while 
x 
< 
y 
do 
p](u) 
{ 
[pr(u) 
def 
-
undefined 
if 
n 
is 
the 
least 
number 
such 
that 
[pr(u) 
is 
defined 
and 
[pr(u)(x) 
[pr(u)(y), 
if 
ilO 
such 
n 
exists. 
We 
are 
now 
ready 
to 
give 
a 
formal 
statement 
of 
the 
equivalence 
of 
while 
programs 
and 
Jt-recursive 
functions. 
Theorem 
1.1 
(i) 
For 
every 
Jt-
(respectively, 
primitive) 
recursive 
junction 
I 
: 
--+ 
N, 
there 
is 
a 
while 
(respectively, 
ror) 
program 
p 
such 
that 
lor 
any 

_____________________________________________
272 
Supplemel1tary 
Lecture 
I 
environment 
0-, 
[PD( 
0-) 
is 
defined 
ifJ 
1(0-(X1), 
.
.. 
, 
o-(x,,)) 
is 
defined; 
and 
if 
both 
are 
defined, 
then 
(ii) 
For 
every 
while 
(respectively, 
for) 
program 
p 
with 
variables 
Xl, 
Ł
.Ł 
, 
X
n 
only, 
there 
are 
JL-
(respectively, 
primitive) 
recursive 
functions 
fi 
: 
N" 
N, 
1 
i 
n, 
such 
that 
lor 
an1/ 
environment 
0-, 
[P](o-) 
is 
defined 
iilli(0-(X1),""0-(x,,)) 
is 
defined, 
1 
i 
ni 
and 
if 
all 
are 
defined, 
then 
Once 
we 
have 
stated 
the 
theorem, 
the 
proof 
is 
quite 
straightforward 
and 
proceeds 
by 
induction 
on 
the 
structure 
of 
the 
program 
or 
JL-reeursive 
tion. 
We 
argue 
one 
case 
explicitly. 
Suppose 
we 
are 
given 
a 
function 
f 
: 
N" 
N 
defined 
by 
primitive 
reeursion. 
For 
simplicity, 
assume 
that 
the 
k 
in 
the 
primitive 
recursive 
definition 
of 
I 
is 
1; 
then 
I 
is 
defined 
from 
h: 
N"-1 
N 
and 
g: 
N"+l 
N 
by 
1(0; 
x) 
= 
I{x 
+ 
1, 
x) 
= 
g(x, 
x, 
I(x, 
x)), 
where 
x 
= 
X2, 
ŁŁŁ 
,x
n
Ł 
We 
wish 
to 
give 
a 
program 
p 
that 
takes 
its 
inputs 
in 
variables 
Xl 
and 
X, 
computes 
f 
On 
these 
values, 
and 
leaves 
its 
result 
in 
Xo. 
By 
the 
inductionhypothesis, 
9 
and 
h 
are 
computed 
by 
programs 
q 
and 
T, 
respectively. 
These 
programs 
expect 
theu 
inputs 
in 
variables 
Xl, 
Ł.. 
, 
X"+1 
and 
X2, 
ŁŁŁ 
,X"' 
respectively, 
and 
leave 
their 
outputs 
in 
xo. 
Let 
1/1,'" 
, 
1/" 
be 
new 
variables 
not 
occurring 
in 
either 
q 
or 
r. 
Then 
we 
can 
take 
p 
to 
be 
the 
following 
program: 
1/1 
:= 
Xl 
; 
Ł. 
, ; 
1/" 
:= 
X" 
; 
r 
j 
Xl:= 
Oi" 
fQ" 
'Pt 
dQ 
begin 
1/1 
:= 
Xl; 
X2 
:= 
1/2 
; 
ŁŁ 
, ; 
X" 
:= 
1/n 
; 
X,,+l 
:= 
Xo; 
qj 
Xl 
:= 
111 
+ 
1 
end 
1* 
save 
va.lues 
of 
input 
variables 
111/ 
/* 
set 
Xo 
to 
h(x) 
.. 
/ 
1* 
initla.lize 
iteration 
Count 
* / 
/* 
at 
thia 
point 
Xo 
contalna 
f( 
Xl,!) 
.. 
/ 
/' 
iteration 
count 
* / 
1* 
reatore 
values 
of 
other 
variables 
Ł / 
/* 
from 
previous 
itera.tion 
111/ 
1* 
set 
Xo 
to 
g(Xl, 
... 
,Xn+I) 
* / 
/* 
increment 
iteration 
count 
* / 

_____________________________________________
While 
Programs 
273 
Historical 
Notes 
Gödel 
originally 
worked 
exclusively 
with 
the 
primitive 
recursive 
functions. 
Ackermann's 
[1] 
discovery 
ofthe 
non-primitive 
recursive 
yet 
intuitively 
putable 
total 
function 
(36.2) 
fOiced 
Gödel 
to 
rethink 
the 
foundations 
of 
his 
system 
and 
ultimately 
to 
include 
unbounded 
minimization, 
despite 
the 
fact 
that 
it 
led 
to 
partial 
functions. 
As 
we 
now 
know, 
this 
is 
inevitable: 
no 
r.e.list 
of 
total 
computable 
functions 
could 
contain 
all 
total 
computable 
functions, 
as 
can 
be 
shown 
by 
a 
straightforward 
diagonalization 
argument. 
The 
relationship 
between 
the 
primitive 
recursive 
functions 
and 
for 
grams 
was 
observed 
by 
Meyer 
and 
Ritchie 
[86]. 

_____________________________________________
Supplementary 
Lecture 
J 
Beyond 
Undecidability 
Oracle 
Machines 
and 
Relative 
(amputation 
We 
know 
that 
virtually 
all 
interesting 
questions 
about 
Turing 
whether 
a 
given 
TM 
halts 
on 
a 
given 
input, 
whether 
a 
given 
TM 
accepts 
a 
finite 
set, 
and 
so 
on-are 
undecidable. 
But 
are 
all 
these 
questions 
equally 
hard? 
For 
example, 
suppose 
by 
some 
magic 
we 
were 
given 
the 
power 
to 
decide 
the 
halting 
problem. 
Could 
we 
somehow 
use 
that 
power 
to 
decide 
if 
a 
given 
TM 
accepts 
a 
finite 
set? 
In 
other 
words, 
relative 
to 
the 
halting 
problem, 
is 
finiteness 
decidable? 
Questions 
about 
relative 
computability 
can 
be 
formalized 
and 
studied 
using 
oracle 
Turing 
machines. 
Intuitively, 
an 
orade 
TM 
is 
a 
TM 
equipped 
with 
an 
oracle, 
a 
set 
B 
to 
which 
the 
TM 
may 
pose 
membership 
questions 
and 
always 
receive 
correct 
answers 
after 
a 
finite 
time. 
The 
interesting· 
thing 
about 
this 
definition 
is 
that 
it 
mak.es 
sense 
even 
if 
B 
is 
not 
recursive. 
Formally, 
an 
oracle 
Turing 
machine 
is 
a 
TM 
that 
in 
addition 
to 
its 
ordinary 
read/write 
tape 
is 
equipped 
with 
a 
special 
one-way-infinite 
read-only 
input 
tape 
on 
which 
some 
infinite 
string 
is 
written. 
The 
extra 
tape 
is 
called 
the 
oracle 
tape, 
and 
the 
string 
written 
on 
it 
is 
called 
the 
oracle. 
The 
machine 
can 
move 
its 
orade 
tape 
head 
one 
cell 
in 
either 
direction 
in 
each 
step 
and 
make 
decisions 
based 
on 
the 
symbols 
written 
on 
the 
orade 
tape. 
Other 
than 
that, 
it 
behaves 
exactly 
like 
an 
ordinary 
Turing 
machine. 

_____________________________________________
Beyond 
Undecidability 
275 
We 
usually 
think 
of 
the 
oracle 
as 
a 
specification 
of 
a 
set 
of 
strings. 
If 
the 
oracle 
is 
an 
infinite 
string 
over 
{O, 
I}, 
then 
we 
can 
regard 
it 
as 
the 
teristic 
function 
of 
a 
set 
B 
N, 
where 
the 
nth 
bit 
of 
the 
oracle 
string 
is 
1 
iff 
nE 
B. 
In 
that 
way 
we 
can 
study 
computation 
relative 
to 
the 
set 
B. 
There 
is 
nothing 
mysterious 
about 
oracle 
TMs. 
They 
operate 
exactly 
like 
ordinary 
TMs, 
theonly 
difference 
being 
the 
oracle. 
Ordinary 
TMs 
are 
alent 
to 
oracle 
TMs 
with 
the 
null 
oracle.0, 
whose 
characteristic 
function 
is 
00000·· 
'j 
for 
such 
machines, 
the 
oracle 
gives 
no 
extra. 
information 
that 
the 
TM 
doesn't 
already 
have. 
For 
A, 
B 
E*, 
we 
say 
that 
A 
is 
recursively 
enumerable 
(r.e.) 
in 
B 
if 
there 
is 
an 
oracle 
TM 
M 
with 
oracle 
B 
such 
that 
A 
= 
L(M). 
In 
addition, 
if 
M 
is 
total 
(Le., 
halts 
on 
all 
inputs), 
we 
write 
A 
:5T 
Band 
say 
that 
A 
is 
recursive 
in 
B 
or 
that 
A 
Thring 
reduces 
to 
B. 
For 
example, 
the 
halting 
problem 
is 
recursive 
in 
the 
membership 
problem, 
since 
halting 
is 
decidable 
in 
the 
presence 
of 
an 
oracle 
for 
membership. 
Here's 
how: 
given 
a 
TM 
M 
and 
input 
x, 
first 
ask 
the 
oracle 
whether 
M 
accepts 
x. 
If 
the 
answer 
is 
yes, 
then 
M 
certainly 
halts 
on 
x. 
If 
the 
answer 
is 
no, 
switch 
accept 
and 
reject 
states 
of 
M 
to 
get 
the 
machine 
M', 
then 
ask 
the 
oracle 
whether 
M' 
accepts 
x. 
If 
the 
answer 
is 
yes, 
then 
M 
rejects 
x, 
therefore 
halts 
on 
x. 
If 
the 
answer 
is 
still 
no, 
then 
M 
neither 
accepts 
or 
rejects 
x, 
therefore 
loops 
on 
x. 
In 
all 
we 
can 
say 
definitively 
after 
a 
finite 
time 
whether 
M 
halts 
on 
x. 
Likewise, 
the 
membership 
problem 
is 
recursive 
in 
the 
halting 
problem, 
since 
lIVe 
can 
determine 
membership 
in 
the 
presence 
of 
an 
oracle 
for 
halting. 
Given 
a 
TM 
M 
and 
input 
x, 
modify 
M 
so 
as 
never 
to 
reject 
by 
making 
the 
reject 
state 
r 
into 
a 
nonreject 
state. 
You 
can 
add 
a 
new 
dummy 
inaccessible 
reject 
state 
if 
you 
like. 
Call 
this 
modified 
machine 
M'. 
Now 
on 
any 
input, 
M' 
accepts 
iff 
it 
halts, 
and 
L(M) 
= 
L(M
'
), 
so 
we 
can 
determine 
whether 
M 
accepts 
x 
by 
asking 
the 
oracle 
whether 
the 
modified 
machine 
M' 
halts 
o'n 
x. 
It 
is 
not 
hard 
to 
show 
that 
the 
relation 
:5T 
is 
transitivej 
that 
is, 
if 
A 
is 
recursive 
in 
Band 
B 
is 
recursive 
in 
C, 
then 
A 
is 
recursive 
in 
C. 
Moreover, 
the 
relation 
:5m 
refines 
:5Tj 
in 
other 
words, 
if 
A 
:5m 
B, 
then 
A 
B 
(Miscellaneous 
Exercise 
141). 
The 
relation 
:5T 
is 
strictly 
coarser 
than 
:5m, 
since 
'" 
HP 
im 
HP 
but 
"'HP 
:5T 
HP. 
In 
fact, 
any 
set 
A 
Turing 
reduces 
to 
its 
complement, 
since 
with 
an 
oracle 
for 
A, 
on 
input 
x 
one 
can 
simply 
ask 
the 
oracle 
whether 
x 
E 
A, 
acceoting 
if 
not 
and 
rejecting 
if 
so. 

_____________________________________________
276 
Supplementary 
Lecture 
J 
The 
Arith 
metic 
Hierarchy 
Once 
we 
have 
the 
notion 
of 
relative 
computation, 
we 
can 
define 
a 
archy 
of 
dasses 
as 
follows. 
Fix 
the 
alphabet 
{O, 
I} 
and 
identify 
strings 
in 
{O, 
I} 
* 
with 
the 
natural 
numbers 
according 
to 
the 
one-to-one 
dence 
(28.1). 
Define 
,,0 
def 
{ 
t } 
"-'1 
= 
r.e. 
se 
s , 
{recursive 
sets}, 

{sets 
r.e. 
in 
some 
B 
E 
{sets 
recursive 
in 
some 
B 
E 
{complements 
of 
sets 
in 
Thus 
is 
the 
dass 
of 
co-r.e. 
sets. 
The 
dass 
es 
and 
comprise 
what 
is 
known 
as 
the 
arithmetic 
hierarchy. 
Here 
is 
perhaps 
a 
more 
revealing 
characterization 
of 
the 
arithmetic 
chy 
in 
terms 
of 
alternation 
of 
Recall 
from 
Exercise 
1 
of 
work 
11 
that 
a 
set 
A 
is 
r.e. 
Hf 
there 
exists 
a 
decidable 
binary 
predicate 
R 
such 
that 
A 
= 
{x 
13y 
R(x,y)}. 
For 
example, 
HP 
= 
{M#x 
13t 
M 
halts 
on 
x 
in 
t 
steps}, 
MP 
= 
{M#x 
13t 
M 
accepts 
x 
in 
t 
steps}. 
(J.1 
) 
Note 
that 
the 
predicate 
"M 
halts 
on 
x" 
is 
not 
decidable, 
but 
the 
predicate 
"M 
halts 
on 
x 
in 
t 
steps" 
is, 
since 
we 
can 
just 
simulate 
M 
on 
input 
x 
with 
a 
universal 
machine 
for 
t 
steps 
and 
see 
if 
it 
halts 
within 
that 
time. 
Alternatively, 
HP 
= 
{M#x 
\311 
v 
is 
a 
halting 
computation 
history 
of 
M 
on 
x}, 
MP 
= 
{M#x 
13v 
v 
is 
an 
accepting 
computation 
history 
of 
M 
on 
x}. 
Thus 
the 
dass 
is 
the 
family 
of 
all 
sets 
that 
can 
be 
expressed 
in 
the 
form 
(J.1). 
Similarly, 
it 
follows 
from 
elementary 
logic 
that 
the 
family 
of 
co-r.e. 
sets, 
is 
the 
dass 
of 
all 
sets 
A 
for 
which 
there 
exists 
a 
decidable 
binary 
predicate 
R 
such 
that 
A 
= 
{x 
I 
'l:/y 
R(x,y)}. 
(J.2) 

_____________________________________________
Theorem 
J.l 
Beyond 
Undecidability 
277 
We 
argued 
in 
Lecture 
29 
that 
a 
set 
is 
recursive 
iff 
it 
is 
both 
r.e. 
and 
co-r.e. 
In 
terms 
of 
our 
new 
notation, 
= 
n 
These 
results 
are 
special 
cases 
of 
the 
following 
theorem. 
(i) 
A 
set 
A 
is 
in 
iff 
there 
exists 
a 
decidable 
(n 
+ 
1)-ary 
predicate 
R 
such 
that 
where 
Q 
= 
3 
if 
n 
is 
odd, 
'V 
if 
n 
ia 
even. 
(ii) 
A 
set 
A 
is 
in 
iff 
there 
exists 
a 
decidable 
(n 
+ 
l)-ary 
predicate 
R 
such 
that 
A 
= 
{x 
I 
'VYl 
3Y2 
'VYa 
... 
QYn 
R(X,Yl,'" 
,Yn)}, 
where 
Q 
= 
V 
if 
n 
ia 
odd, 
3 
if 
n 
ia 
even. 
(iii) 
= 
n 
Proof. 
Miscellaneous 
Exercise 
137. 
o 
Example 
J.2 
The 
set 
EMPTY 
{M 
I 
L(M) 
= 
0} 
is 
in 
since 
EMPTY 
= 
{M 
I 
'Vx 
'Vt 
M 
does 
not 
accept 
x 
in 
t 
steps}. 
The 
two 
universal 
quantifiers 
'Vx 
'Vt 
can 
be 
combined 
into 
one 
using 
the 
cöm'putable 
one-to-onepairing 
function 
N2 
-+ 
N 
given 
by 
(
i 
+ 
)2' 
+ 
1 ) 
+'. 
(i,j) 
1-+ 
Ł 
j 
0 
1 
2 
3 
4 5 
0 0 
1 3 
6 
10 
15 
1 2 4 7 
11 
16 
2 
5 
8 
12 
17 
3 
9 
13 
18 
4 
14 
19 
5 
20 
Example 
J.3 
The 
set 
TOTAL 
{M 
I 
M 
is 
total} 
is 
in 
IIg, 
since 
TOTAL 
= 
{M 
I 
'Vx 
3t 
M 
halts 
on 
x 
in 
t 
steps}. 
(J.3) 
o 
o 

_____________________________________________
278 
Supplementary 
Lecture 
J 
Example 
J.4 
The 
set 
FIN 
{M 
I 
L(M) 
is 
finite} 
is 
in 
Eg, 
since 
FIN 
= 
{M 
13n 
"Ix 
if 
lxi> 
n 
then 
x 
L(M)} 
=·{M 
13n 
"Ix 
vi 
lxi 
n 
or 
M 
does 
not 
accept 
x 
in 
t 
steps}. 
Again, 
the 
two 
universal 
quantifiers 
"Ix 
Vt 
can 
be 
combined 
into 
one 
using 
(J.3). 
0 
Example 
J.5 
A 
set 
is 
cofinite 
if 
its 
compleiU•nt 
is 
finite. 
The 
set 
COF 
{M 
I 
L(M) 
is 
cofinite} 
is 
in 
Eg, 
since 
COF 
= 
{M 
13n 
"Ix 
if 
lxi> 
n 
then 
XE 
L(M)} 
= 
{M 
13n 
"Ix 
3t 
lxi 
n 
or 
M 
accepts 
x 
in 
t 
steps}. 
0 
Figure 
J.1 
depicts 
the 
inclusions 
among 
the 
few 
lowest 
levels 
of 
the 
archy. 
Each 
level 
of 
theJlierarchy 
is 
strictly 
contained 
in 
the 
nextj 
that 
is, 
but 
i= 
We 
hiLve 
shown 
that 
there 
exist 
r.e. 
sets 
that 
are 
not 
co-r.e. 
(HP, 
for 
example) 
and 
co-r.e. 
sets 
that 
are 
not 
r.e. 
('"V 
HP, 
for 
example). 
Thus 
and 
are 
incomparable 
with 
respect 
to 
set 
inclusion. 
One 
can 
show 
in 
the 
same 
way 
that 
and 
are 
inclIlllpala.ble 
with 
respect 
to 
set 
indusion 
for 
any 
n 
(Miscellaneous 
Exercise 
138). 
Completeness 
The 
membership 
problem 
MP 
{M#x 
I 
M 
accepts 
x} 
is 
not 
only 
decidable 
but 
is 
in 
a 
sense 
a 
"hardest" 
r.e. 
set, 
since 
every 
other 
r.e. 
set 
to 
it: 
for 
any 
Turing 
machine 
M, 
the 
map 
X 
1-+ 
M#x 
(JA) 
is 
a 
trivially 
computable 
map 
reducing 
L(M) 
to 
MP. 
We 
say 
that 
a 
set 
is 
r.e.-hard 
if 
every 
r.e. 
set 
to 
it. 
In 
other 
words, 
the 
set 
B 
is 
r.e.-hard 
if 
for 
all 
r.e. 
sets 
A, 
A 
B. 
As 
just 
flerved, 
the 
membership 
problem 
MP 
is 
r.e.-hard. 
So 
is 
any 
other 
problem 
to 
which 
the 
membership 
problem 
(e.g., 
the 
halting 
problem 
HP), 
because 
the 
relation 
is 
transitive. 
A 
set 
B 
is 
said 
to 
be 
r.e.-complete 
if 
it 
is 
both 
an 
r.e. 
set 
and 
r.e.-hard. 
For 
example, 
both 
MP 
and 
HP 
are 
r.e.-complete. 
More 
generally, 
if 
C 
is 
a 
dass 
of 
sets, 
we 
say 
that 
a 
set 
B 
is 
-harn 
for 
C 
(or 
just 
C-hard) 
if 
A 
B 
for 
all 
A 
E 
C. 
We 
say 
that 
B 
is 
for 
C 
(or 
just 
C-complete) 
if 
B 
is 
for 
C 
and 
BE 
C. 

_____________________________________________
Beyond 
Undecidability 
279 
recursive 
sets 
Figure 
J.1. 
The 
Arithmetic 
Hierarchy 
One 
can 
prove 
a 
theorem 
corresponding 
to 
Theorem 
33.3 
that 
says 
that 
if 
A 
Sm 
Band 
B 
E 
then 
A 
E 
and 
if 
A 
Sm 
Band 
B 
E 
then 
A 
E 
Since 
we 
know 
that 
the 
hierarchy 
is 
strict 
(each 
level 
is 
properly 
contained 
in 
the 
next), 
if 
B 
is 
Sm-complete 
for 
then 
B 
(or 
or 
It 
turns 
out 
that 
each 
of 
the 
problems 
mentioned 
above 
is 
Sm-complete 
for 
the 
level 
of 
the 
hierarchy 
in 
which 
it 
naturally 
falls: 

_____________________________________________
280 
Supplementary 
Lecture 
J 
(i) 
HP 
is 
:5m-complete 
for 
(ii) 
MP 
is 
:5m-complete 
for 
(iii) 
EMPTY 
is 
:5m-complete 
for 
(iv) 
TOTAL 
is 
:5m-complete 
for 
ng, 
(v) 
FIN 
is 
:5m-complete 
for 
Eg, 
and 
(vi) 
COF 
is 
:5m-complete 
for 
Eg. 
Since 
the 
hierarchy 
is 
strict, 
none 
of 
these 
problems 
is 
contained 
in 
any 
dass 
lower 
in 
the 
hierarchy 
or 
:5T-reduces 
to 
any 
problem 
complete 
for 
any 
dass 
lower 
in 
the 
hierarchy. 
If 
it 
did, 
then 
the 
hierarchy 
would 
collapse 
at 
that 
level. 
For 
example, 
EMPTY 
does 
not 
reduce 
to 
HP 
and 
COF 
does 
not 
reduce 
to 
FIN. 
We 
prove 
(v)j 
the 
others 
we 
leave 
as 
exercises 
(Miscellaneous 
Exercise 
142). 
We 
have 
already 
argued 
that 
FIN 
E 
Eg. 
To 
show 
that 
it 
is 
:5m-hard 
for 
Eg, 
we 
need 
to 
show 
that 
any 
set 
in 
Eg 
reduces 
to 
it. 
We 
use 
the 
characterization 
of 
Theorem 
J.1. 
Let 
A 
= 
{x 
I 
3y 
'Vz 
R(x, 
y, 
z)} 
be 
an 
arbitrary 
set 
in 
Eg, 
where 
R( 
x, 
y, 
z) 
is 
a 
decidable 
ternary 
predicate. 
Let 
M 
be 
a 
total 
machine 
that 
decides 
R. 
We 
need 
to 
construct 
a 
machine 
N 
effectively 
from 
a 
given 
x 
such 
that 
N 
E 
FIN 
iff 
x 
E 
Ai 
in 
other 
words, 
N 
accepts 
a 
finite 
set 
iff 
3y 
'Vz 
R(x, 
y, 
z). 
Let 
N 
on 
input 
w 
(i) 
write 
down 
all 
strings 
y 
of 
length 
at 
most 
Iwl; 
(ii) 
for 
each 
such 
y, 
try 
to 
find 
a 
z 
such 
that 
....,R(x, 
y, 
z) 
(i.e., 
such 
that 
M 
rejects 
x#y#z), 
and 
accept 
if 
all 
these 
trials 
are 
successful. 
The 
machine 
N 
has 
x 
and 
a 
description 
of 
M 
hard-wired 
in 
its 
finite 
control. 
In 
step 
(ii), 
for 
each 
y 
of 
length 
at 
most 
Iwl, 
N 
can 
just 
enumerate 
strings 
z 
in 
some 
order 
and 
run 
M 
on 
x#y#z 
until 
some 
z 
is 
found 
causing 
M 
to 
reject. 
Since 
M 
is 
total, 
N 
need 
not 
worry 
about 
timesharing. 
If 
no 
such 
z 
is 
ever 
found, 
N 
just 
goes 
on 
forever. 
Surely 
such 
an 
N 
can 
be 
built 
effectively 
from 
M 
and 
x. 
Now 
if 
xE 
A, 
then 
there 
exists 
y 
such 
that 
for 
all 
z, 
R(x, 
y, 
z) 
(Le., 
for 
all 
z, 
M 
accepts 
x#y#z); 
thus 
step 
(ii) 
fails 
whenever 
Iwl 
lyl. 
In 
this 
case 
N 
accepts 
a 
finite 
set. 
On 
the 
other 
hand, 
if 
x 
(j. 
A, 
then 
fpr 
all 
y 
there 
exists 
a 
z 
such 
that 
-,R(x, 
y, 
z), 
and 
these 
are 
all 
found 
in 
step 
(ii). 
In 
this 
case, 
N 
accepts 
E*. 

_____________________________________________
Beyond 
Undecidability 
281 
We 
have 
shown 
that 
the 
machine 
N 
accepts 
a 
finite 
set 
iff 
x 
E 
A, 
therefore 
the 
map 
x 
N 
constitutes 
a 
:Sm-reduction 
from 
A 
to 
FIN. 
Since 
A 
was 
an 
arbitrary 
element 
of 
Eg, 
FIN 
is 
:Sm-hard 
for 
Eg. 
The 
Analytic 
Hierarchy 
and 
nt 
The 
arithmetic 
hierarchy 
is 
defined 
in 
terms 
of 
first-order 
quantification, 
or· 
quantification 
over 
natural 
numbers 
or 
strings. 
But 
it 
doesn't 
stop 
there: 
if 
we 
consider 
second-order 
quantification-quantification 
over 
functions 
and 
get 
the 
so-called 
analytic 
hierarchy 
consisting 
of 
classes 
The 
entire 
arithmetic 
hierarchy 
is 
strictly 
contained 
in 
Ät, 
the 
est 
dass 
in 
the 
analytic 
hierarchy. 
Elements 
of 
Ät 
are 
called 
hyperarithmetic 
sets. 
Aremarkable 
theorem 
due 
to 
Kleene 
says 
tha,t 
the 
sets 
of 
natural 
numbers 
definable 
by 
one 
universal 
secolld-order 
quantifier 
(i.e., 
the 
nt 
sets) 
are 
the 
sets 
definable 
by 
first-order 
induction. 
The 
dass 
rrt 
also 
has 
natural 
completeproblems. 
For 
example, 
suppose 
we 
'are 
given 
a 
recursive 
binary 
relation 
-00( 
on 
Nj 
that 
is, 
a 
recursive 
subset 
of 
N2. 
A 
natural 
question 
to 
ask 
is 
whether 
the 
relation 
is 
well 
joundedj 
that 
is, 
whether 
there 
exists 
no 
infinite 
descending 
chain 
no 
>-
nl 
>-
n2 
>-
.... 
This 
decision 
problem 
is 
:Sm-complete 
for 
fit. 
These 
results 
are 
getting 
a 
bit 
beyond 
our 
scope, 
so 
we'H 
stop 
here. 
Historical 
Notes 
Oracle 
Turing 
machines 
were 
first 
defined 
by 
Turing 
[121]. 
The 
arithmetic 
and 
analytic 
hierarchies 
were 
studied 
by 
Kleene 
[68]; 
see 
also 
Rogers 
[106], 
Shoenfield 
[115], 
Kleene 
[69], 
and 
Soare 
[116]. 
Modern-day 
complexity 
theory 
has 
its 
roots 
in 
the 
theory 
of 
recursive 
tions 
and 
effective 
computability. 
The 
:ST-
and 
:Sm-reducibility 
relations, 
the 
concepts 
of 
completeness 
and 
hardness, 
and 
the 
arithmetic 
hierarchy 
aH 
have 
their 
subrecursive 
counterpartsj 
See 
Karp 
[64], 
Cook 
[28], 
and 
meyer 
[118]. 
For 
an 
introduction 
to 
complexity 
theory, 
see 
Hartmanis 
and 
Stearns 
[57], 
Garey 
and 
Johnson 
[40], 
or 
Papadimitriou 
[97]. 

_____________________________________________
Lecture 
38 
Gödel's 
Incompleteness 
Theorem 
In 
1931 
Kurt 
Gödel 
[50, 
51] 
proved 
a 
momentous 
theort:m 
with 
far-reaching 
philosophical 
consequences: 
he 
showed 
that 
no 
reasonable 
formal 
proof 
tem 
for 
number 
theory 
can 
prove 
all 
true 
senten 
ces. 
This 
result 
set 
the 
logic 
community 
on 
its 
ear 
and 
left 
Hilbert's 
formalist 
program 
in 
shambles. 
This 
result 
is 
widely 
regarded 
as 
one 
of 
the 
greatest 
intellectual 
achievements 
of 
twentieth-century 
mathematics. 
With 
our 
understanding 
of 
reductions 
and 
r.e. 
sets, 
we 
are 
in 
a 
position 
to 
understand 
this 
theorem 
and 
give 
a 
complete 
proof.1t 
is 
thus 
a 
fitting 
note 
on 
which 
to 
end 
the 
course. 
The 
Language 
of 
Number 
Theory 
The 
first-order 
language 
of 
number 
theory 
L 
is 
a 
formal 
language 
for 
pressing 
properties 
of 
the 
natural' 
numbers 
N 
= 
{O, 
1, 
2, 
... 
}. 
The 
language 
is 
built 
from 
the 
following 
symbols: 
Ł 
variables 
x, 
y, 
z, 
... 
ranging 
over 
Nj 
Ł 
operator 
symbols 
+ 
(addition) 
and 
. 
(multiplication); 

_____________________________________________
Gödel's 
Incompleteness 
Theorem 
283 
Ł 
constant 
symbols 
0 
(additive 
identity) 
and 
1 
(multiplicative 
identitY)j 
Ł 
relation 
symbol 
= 
(other 
relation 
symbols 
<, 
$, 
>, 
and 
2:: 
are 
able)j 
Ł 
quantifiers 
V 
(for 
all) 
and'3 
(there 
exists)j 
Ł 
propositional 
operators 
V 
(or), 
/I. 
(and), 
-, 
(not), 
-+ 
(if-then), 
and 
+-+ 
(if 
and 
only 
if)j 
and 
Ł 
parentheses. 
Rather 
than 
give 
a 
formal 
definition 
of 
the 
well-formed 
formulas 
of 
this 
language 
(which 
we 
could 
easily 
do 
with 
a 
CFG), 
let's 
give 
some 
examples 
of 
formulas 
and 
their 
interpretations. 
We 
can 
define 
other 
comparison 
relations 
besides 
=j 
for 
example, 
def 
3 
x 
$ 
y 
= 
z x 
+ 
z 
= 
y, 
x 
< 
Y 
d,g 
3z 
x 
+ 
z 
= 
Y 
/I. 
-,(z 
= 
0). 
Many 
useful 
number-theoretic 
concepts 
can 
be 
formalized 
in 
this 
language. 
For 
example: 
Ł 
"q 
is 
the 
quotient 
and 
r 
the 
remainder 
obtained 
when 
dividing 
x 
by 
y 
using 
integer 
division": 
( 
) 
def 
INTDIV 
x,y,q,r 
= 
x 
= 
qy 
+ 
r 
/I. 
r 
< 
y 
Ł 
"y 
divides 
x": 
DIV(Y,X) 
3q 
INTDIV(X,y,q,O) 
Ł 
"x 
is 
even": 
EVEN(X) 
DIv(2,x) 
Here 
2 
is 
an 
abbreviation 
for 
1+1. 
Ł 
"x 
is 
odd": 
ODO(X) 
-'EVEN(X) 
Ł 
"x 
is 
prime": 
PRIME(X) 
d,g 
x 
2:: 
2 
/I. 
Vy 
(DIV(Y,X) 
-+ 
(y 
= 
1 
Vy 
= 
x)) 
Ł 
"x 
is 
apower 
of 
two": 
POWER2 
(x) 
Vy 
(DIV(y, 
x) 
/I. 
PRIME(y)) 
-+ 
Y 
= 
2 

_____________________________________________
284 
Lecture 
38 
Ł 
"V 
is 
apower 
of 
two, 
say 
2
lo
, 
and 
the 
kth 
bit 
of 
the 
binary 
representation 
of 
x 
is 
I": 
BIT 
(X, 
V) 
POWER2(V) 
" 
\::Iq 
\::Ir 
(INTDIV(x,v,q,r) 
-
ODD(q)) 
Here 
is 
an 
explanation 
ofthe 
formula 
BIT(X,V). 
Suppose 
x 
and 
V 
are 
bers 
satisfying 
BIT(X, 
V). 
Since 
y 
is 
apower 
of 
two, 
its 
binary 
representation 
consists 
of 
al 
followed 
by 
astring 
of 
zeros. 
The 
formula 
BIT(X,y) 
is 
true 
precisely 
when 
x's 
bit 
in 
the 
same 
position 
as 
the 
1 
in 
y 
is 
1. 
We 
get 
hold 
of 
this 
bit 
in 
x 
by 
dividing 
x 
by 
y 
using 
integer 
division; 
the 
quotient 
q 
and 
remainder 
r 
are 
the 
binary 
numbers 
illustrated. 
The 
bit 
we 
are 
interested 
in 
is 
1 
iff 
q 
is 
odd. 
y 
x 
= 
= 
1000000000000 
).10 
I} 
00 
lJl 
1 
000 
1 
.. 
0 
11 
0 
11, 
q 
r 
This 
formula 
is 
useful 
for 
treating 
numbers 
as 
bit 
strings 
and 
indexing 
into 
them 
with 
other 
numbers 
to 
extract 
bits. 
We 
will 
use 
this 
power 
below 
to 
write 
formulas 
that 
talk 
about 
valid 
computation 
histories 
of 
Turing 
machines. 
If 
there 
are 
no 
free 
(unquantified) 
variables, 
then 
the 
formula 
is 
called 
a 
sentence. 
Every 
sentence 
has 
a 
well-defined 
truth 
value 
under 
its 
natural 
interpretation 
in 
N. 
Examples 
are 
\::Ix 
3y 
y 
= 
x 
+ 
1 
Vx 
3y 
x 
= 
y+ 
1 
"Every 
number 
has 
a 
successor." 
"Every 
number 
haa 
a 
predecessor." 
Of 
these 
two 
sentences, 
the 
first 
is 
true 
and 
the 
second 
is 
false 
(0 
has 
no 
predecessor 
in 
N). 
The 
set 
of 
true 
sentences 
in 
this 
language 
is 
called 
(first-order) 
number 
theory 
and 
is 
denoted 
Th(N). 
The 
decision 
problem 
for 
number 
theory 
is 
to 
decide 
whether 
a 
given 
sentence 
is 
true; 
that 
is, 
whether 
a 
given 
sentence 
is'in 
Th(N). 
Peano 
Arithmetic 
The 
most 
popular 
proof 
system 
for 
number 
theory 
is 
called 
Peano 
metic 
(PA). 
This 
system 
t:onsists 
of 
some 
basic 
assumptions 
called 
axioms, 
which 
are 
asserted 
to 
be 
true, 
and 
some 
mIes 
01 
inlerence, 
which 
can 
be 
applied 
in 
a 
mechanical 
way 
to 
derive 
further 
theorems 
from 
the 
axioms. 
Among 
the 
axioms 
of 
PA, 
there 
are 
axioms 
that 
apply 
to 
first-order 
logic 
in 
general 
and 
are 
not 
particular 
to 
number 
theory, 
such 
as 
axioms 
for 
manipulating 

_____________________________________________
Gödel's 
Incompleteness 
Theorem 
285 
Ł 
propositional 
formulas, 
such 
as 
(<p 
A 
1/J) 
-+ 
<Pj 
Ł 
quantifiers, 
such 
as 
(Vx 
<p(x)) 
-+. 
<p(17)j 
and 
Ł 
equality, 
such 
as 
Vx 
Vy 
Vz 
(x 
= 
Y 
1\ 
Y 
= 
z 
-+ 
x 
= 
z). 
In 
addition, 
PA 
has 
the 
following 
axioms 
particular 
to 
number 
theory: 
Vx...,(O=x+l) 
Vx 
Vy 
(x 
+ 1 = 
y 
+ 1 
-+ 
x 
= 
y) 
Vx 
x+O= 
x 
Vx 
Vy 
x 
+ 
(y 
+ 
1) 
= 
(x 
+ 
y) 
+ 1 
Vx 
X· 
0 
= 
0 
Vx 
Vy 
x· 
(y 
+ 
1) 
= 
(x· 
y) 
+ 
x 
(<p(O) 
1\ 
Vx 
(<p(x) 
-+ 
<p(x 
+ 
1))) 
-+ 
Vx 
<p(x) 
o 
is 
not 
a 
successor 
successor 
is 
one-to-one 
o 
is 
an 
identity 
for 
+ 
+ 
is 
associative 
o 
is 
an 
annihilator 
for 
. 
. 
distributes 
over 
+ 
induction 
axiom 
where 
<p(x) 
denotes 
any 
formula 
with 
one 
free 
variable 
x. 
The 
last 
axiom 
is 
called 
the 
induction 
axiom. 
It 
is 
actually 
an 
axiom 
scheme 
because 
it 
resents 
infinitely 
many 
axioms, 
one 
for 
each 
<p(x). 
It 
is 
really 
the 
induction 
principle 
on 
N 
as 
you 
know 
it: 
in 
words, 
Ł 
if<p 
is 
true 
of 
0 
(basis), 
and 
Ł 
if 
for 
any 
x, 
from 
the 
assumption 
that 
<p 
is 
true 
of 
x, 
it 
follows 
that 
<p 
is 
true 
of 
x 
+ 1 
(induction 
step), 
then 
we 
can 
conclude 
that 
<p 
is 
true 
of 
all 
x. 
There 
are 
also 
two 
rules 
of 
inference 
for 
deriving 
new 
theorems 
from 
old: 
<p <p 
-+ 
1/J 
<p 
1/J 
Vx 
<p' 
These 
two 
rules 
are 
called 
modus 
ponens 
and 
generalization, 
respectively. 
A 
proof 
of 
<Pn 
is 
a 
sequence 
<Po, 
<PI 
, 
<P2, 
ŁŁŁ 
, 
<Pn 
of 
formulas 
sliCh 
that 
each 
<Pi 
either 
is 
an 
axiom 
or 
follows 
from 
formulas 
occurring 
earlier 
in 
the 
list 
by 
a 
rule 
of 
inference. 
A 
sentence 
of 
the 
language 
is 
a 
theorem 
of 
the 
system 
if 
it 
has 
a 
proof. 
A 
proof 
system 
is 
said 
to 
be 
sound 
if 
all 
theorems 
are 
truej 
is, 
i:f 
it 
is 
not 
possible 
to 
prove 
a 
false 
sentence. 
This 
is 
a 
basic 
requirement 
of 
all 
reasonable 
proof 
systemsj 
a 
proof 
system 
wouldn't 
be 
much 
good 
if 
its 
theorems 
were 
false. 
The 
system 
PA 
is 
sound, 
as 
one 
can 
show 
by 
induction 
on 
the 
length 
of 
proofs: 
all 
the 
axioms 
are 
true, 
and 
any 
conclusion 
derived 
by 
a 
rule 
of 
inference 
from 
true 
premises 
is 
true. 
Soundness 
means 
that 
the 
following 
set 
inclusions 
hold: 

_____________________________________________
286 
Lecture 
38 
Th(N) 
"'Th(N) 
true 
sentences 
false 
sentences 
all 
sentences 
A 
proof 
is 
said 
to 
be 
complete 
if 
all 
true 
statements 
are 
theorems 
of 
the 
system; 
that 
is, 
if 
the 
set 
of 
theorems 
coincides 
with 
Th(N). 

_____________________________________________
Lecture 
39 
Proof 
of 
the 
Incompleteness 
Theorem 
Gödel 
proved 
the 
incompleteness 
theorem 
by 
for 
any 
sonable 
proof 
system, 
a 
sentence 
of 
number 
theory 
cp 
that 
asserts 
its 
own 
unprovability 
in 
that 
system: 
cp 
is 
true 
{:::::> 
cp 
is 
not 
provable. 
(39.1) 
Any 
reasonable 
proof 
system, 
including 
PA, 
is 
sound; 
this 
means 
that 
for 
any 
sentence 
'Ij;, 
'Ij; 
is 
provable 
:::} 
'Ij; 
is 
true 
(39.2) 
(a 
proof 
system 
would 
not 
be 
worth 
much 
if 
some 
of 
its 
theorems 
were 
false). 
Then 
cp 
must 
be 
true, 
because 
otherwise 
cp 
is 
false 
:::} 
cp 
is 
provable 
by 
(39.1) 
:::} 
cp 
is 
true 
by 
(39.2), 
a 
contradiction. 
Since 
cp 
is 
true, 
by 
(39.1) 
cp 
is 
not 
provable. 
The 
construction 
of 
cp 
is 
quite 
interesting 
by 
itself, 
since 
it 
captures 
in 
no 
uncertain 
terms 
the 
not 
ion 
of 
self-reference. 
The 
power 
that 
one 
needs 
to 
construct 
such 
a 
self-referential 
sentence 
is 
present 
in 
Turing 
machines 
and 
all 
modern 
programming 
languages. 
For 
example, 
the 
following 
is 
a C 
program 
that 
prints 
itself: 

_____________________________________________
288 
Lecture 
39 
char 
*s="char 
*s=%c%s%c; 
%cmainO{printf(s 
,34,s 
,34 ,10 
,10) 
;}%c"; 
main(){printf(s,34,s,34,10,10);} 
Here 
34 
and. 
10 
are 
the 
ASCII 
codes 
for 
double 
quote 
(") 
and 
newline, 
respectively. 
Although 
it's 
a 
mind-bender 
to 
try 
to 
figure 
out 
what 
this 
program 
does, 
it's 
worth 
the 
attempt, 
because 
once 
you 
understand 
this 
you 
have 
understood 
the 
main 
idea 
behind 
Gödel's 
construction. 
We'H 
construct 
Gödel's 
self-referential 
sentence 
in 
Supplementary 
Lecture 
K. 
For 
now 
we 
take 
a 
simpler 
approach 
that 
still 
retains 
the 
most 
tant 
consequences. 
We 
will 
argue 
that 
in 
PA 
or 
any 
other 
reasonable 
proof 
system 
for 
number 
theory, 
(i) 
the 
set 
of 
tneorems 
(provable 
sentences) 
is 
r.e., 
but 
(ii) 
the 
set 
Th{N) 
of 
true 
sentences 
is 
not, 
therefore 
the 
two 
sets 
cannot 
be 
equal, 
and 
the 
proof 
system 
cannot 
be 
complete. 
This approach 
is 
due 
to 
Turing 
[1201. 
The 
set 
of 
theorems 
of 
PA 
is 
certainly 
r.e.: 
one 
can 
enumerate 
the 
theorems 
by 
enumerating 
all 
the 
axioms 
and 
systematically 
applying 
the 
rules 
of 
inference 
in 
all 
possib!e 
ways, 
emitting 
every 
sentence 
that 
is 
ever 
derived. 
This 
is 
true 
for 
any 
reasonable 
proof 
system. 
The 
crux 
then 
is 
to 
show: 
Lemma 
39.1 
Th{N) 
is 
not 
r.e. 
Proof. 
We 
prove 
this 
by 
a 
reduction.f'V 
Th{N). 
The 
result 
will 
then 
foHow 
from 
Theorem 
33.3{i) 
and 
the 
fact 
that 
,...., 
HP 
is 
not 
r.e. 
Recall 
that 
HP 
= 
{M#x 
I 
M 
halts 
on 
input 
x}. 
Given 
M 
#x, 
we 
show 
how 
to 
produce 
a 
sentence 
'Y 
in 
the 
language 
of 
number 
theory 
such 
that 
M#x 
E 
,....,HP 
'Y 
E 
Th{N)j 
that 
is, 
M 
does 
not 
halt 
on 
x 
'Y 
is 
true. 
In 
other 
words, 
given 
M 
and 
x, 
we 
want 
to 
construct 
a 
sentence 
'Y 
in 
the 
language 
of 
number 
theory 
that 
says, 
"M 
does 
not 
halt 
on 
x." 
This 
will 
be 
possible 
because 
the 
language 
of 
number 
theory 
is 
strong 
enough 
to 
talk 
about 
Turing 
machines 
and 
whether 
or 
not 
they 
halt. 
Recall 
the 
formula 
BIT{Y, 
x) 
constructed 
in 
Lecture 
38, 
which 
allows 
us 
to 
think 
of 
numbers 
as 
bit 
strings 
and 
extract 
bits 
from 
them. 
Using 
this 
as 

_____________________________________________
Proof 
of 
the 
Incompleteness 
Theorem 
289 
a 
starting 
point, 
we 
will 
be 
able 
to 
construct 
aseries 
of 
formulas 
nating 
in 
a 
formula 
VALCOMPM,,,,(y) 
that 
says 
that 
y 
represents 
a 
valid 
computation 
history 
of 
M 
on 
input 
Xi 
that 
is, 
y 
represents 
a 
sequence 
of 
conflgurations 
00,01, 
... 
,ON 
of 
M, 
encoded 
over 
some 
alphabet 
D., 
such 
that 
(i) 
00 
is 
the 
start 
conflguration 
of 
M 
on 
X, 
(ii) 
OiH 
follows 
from 
0i 
according 
to 
the 
transition 
function 
8 
of 
M, 
and. 
(iü) 
ON 
is 
a 
halt 
conflguration. 
These 
are 
the 
same 
valid 
computation 
histories 
we 
saw 
in 
Lecture 
34. 
Once 
we 
have 
the 
formula 
VALCOMPM,,,, 
(y), 
we 
can 
say 
that 
M 
does 
not 
halt 
on 
X 
by 
saying 
that 
there 
does 
not 
exist 
a 
valid 
computation 
history: 
def 
( ) 
"I 
= 
...,3y 
VALCOMP 
M,'" 
Y . 
This 
constitutes 
a 
reduction 
from 
'" 
HP 
to 
Th(N). 
It 
remains 
only 
to 
provide 
the 
gory 
details 
of 
the 
construction 
of 
"I 
from 
M 
and 
x. 
Here 
they 
are. 
Assume 
that 
configurations 
of 
Mare 
encoded 
over 
a 
finite 
alphabet 
D. 
of 
size 
p, 
where 
p 
is 
prime. 
Every 
number 
has 
a 
unique 
p-ary 
representation. 
We 
use 
this 
representation 
instead 
of 
the 
binary 
representation 
for 
convenience. 
Let 
the 
symbols 
of 
the 
start 
configuration 
of 
M 
on 
x 
= 
a1 
a2 
... 
an 
be 
encoded 
by 
the 
p-ary 
digits 
k
o
, 
... 
,k
n 
as 
shown: 
I-
a1 
a2 a3 a4 
an 
S -
k
o 
k
1 
k
2 
k3 
k
4 
k
n 
Let 
the 
blank 
symbol 
u 
be 
encoded 
by 
the 
p-ary 
digit 
k. 
Let 
C 
be 
the 
set 
of 
all 
sextuples 
(a, 
b, 
c, 
d, 
e, 
f) 
of 
p-ary 
digits 
such 
that 
if 
the 
three 
elements 
of 
D. 
represented 
by 
a, 
b, 
and 
c 
occur 
consecutively 
in 
a 
configuration 
Oi, 
and 
if 
d, 
e, 
and 
f 
occur 
in 
the 
corresponding 
locations 
in 
0i+1, 
then 
this 
would 
be 
consistent 
with 
the 
transition 
fllu.:tlOn 
8. 
For 
example, 
if 
8(q,a) 
= 
(p,b,R), 
then 
the 
sextuple 
a a 
b 
-q -
a 
b 
b, 
-p 
would 
be 
in 
C. 
N 
ow 
it 
's 
time 
to 
define 
some 
formulas. 

_____________________________________________
290 
Lecture 
39 
Ł 
"The 
number 
y 
is 
apower 
of 
p." 
Here 
p 
is 
a 
fixed 
prime 
that 
depends 
onM. 
POWERp(Y) 
'iz 
(DIV(Z,y) 
1\ 
PRIME(Z) 
-? 
Z 
= 
p) 
Ł 
"The 
number 
d 
is 
a 
of 
p 
and 
specifies 
the 
length 
of 
v 
as 
astring 
over 
A." 
LENGTH(V,d) 
POWERp(d) 
1\ 
v< 
d 
Ł 
"The 
p-ary 
digit 
of 
v 
at 
position 
y 
is 
b" 
(assuming 
y 
is 
apower 
of 
p). 
1\ 
a<y 
1\ 
b<p) 
Ł 
"The 
three 
p-ary 
digits 
of 
v 
at 
position 
y 
are 
b, 
c, 
and 
d" 
(assuming 
Y 
is 
apower 
of 
p). 
3DIGIT( 
v, 
y, 
b, 
c, 
d) 
3u 
3a 
(v 
= 
a 
+ 
by 
+ 
cpy 
+ 
dppy 
+ 
uppPY 
1\ 
a 
< 
y 
1\ 
b 
< 
P 
1\ 
c 
< 
p 
1\ 
d 
< 
p) 
Ł 
"The 
three 
p-ary 
digits 
of 
v 
at 
position 
y 
match 
the 
three 
p-ary 
digits 
of 
v 
at 
Z 
according 
to 
0" 
(assuming 
y 
and 
Z 
are 
powers 
of 
p). 
MATCH(V, 
y, 
z) 
def 
v 
3DIGIT(v,y,a,b,c) 
1\ 
3DIGIT(v,z,d,e,f) 
= 
(a,b,c,d,e'/)EC 
Ł 
"The 
string 
v 
represents 
astring 
of 
successive 
configurations 
of 
M 
of 
length 
c 
up 
to 
d" 
= 
"All 
pairs 
of 
three-digit 
sequences 
exactly 
c 
apart 
in 
v 
match 
according 
to 
0" 
(assuming 
c 
and 
d 
are 
powers 
of 
p). 
MOVE(V,c,d) 
(POWERp(Y) 
1\ 
yppc 
< 
d) 
-:-+ 
MATCH(V,y,yc) 
Ł 
"The 
string 
v 
starts 
with 
the 
start 
configuration 
of 
M 
on 
input 
x 
= 
ala2 
... 
an 
padded 
with 
blanks 
out 
to 
length 
c" 
(assuming 
c 
is 
apower 
of 
p; 
n 
and 
pi, 
0 
i 
n, 
are 
fixed 
constants 
depending 
only 
on 
M). 
n 
def 
1\ 
. 
START(V,C} 
= 
DIGIT(V,p',ki) 
1\ 
pn 
< 
c 
i=O 
1\ 
'iy 
(POWERp(Y) 
1\ 
pn 
< 
y 
< 
c 
-? 
DIGIT(V,y,k)) 
Ł 
"The 
string 
v 
has 
a 
halt 
state 
in 
it 
somewhere." 
HALT(V,d) 
3y 
(POWERp(Y) 
1\ 
Y 
< 
d 
1\ 
V 
DIGIT(v,y,a)) 
aEH 

_____________________________________________
Proof 
of 
the 
Incompleteness 
Theorem 
291 
Here 
H 
is 
the 
set 
of 
all 
p-ary 
digits 
corresponding 
to 
symbols 
of 
ß 
containing 
halt 
states 
. 
Ł 
"The 
string 
v 
is 
a 
valid 
computation 
history 
of 
M 
on 
x." 
VALCOMPM,.,(V) 
3c 
3d 
(POWERp(C) 
" 
c< 
d 
" 
LENGTH(V,dj 
" 
START(V,C) 
" 
MOVE(V,c,d) 
" 
HALT(V,d)) 
Ł 
"The 
machine 
M 
does 
not 
halt 
on 
x." 
...,3v 
VALCOMPM,.,(V) 
This 
concludes 
the 
proof 
of 
the 
incompleteness 
theorem. 
o 

_____________________________________________
Supplementary 
Lecture 
K 
Gödel's 
Proof 
Gödel 
proved 
the 
incompleteness 
theorem 
by 
constructing, 
for 
any 
sonable 
proof 
system, 
a 
sentence 
of 
number 
theory 
that 
asserts 
its 
own 
unprovability. 
Here 
is 
the 
essence 
of 
Gödel's 
construction. 
We 
use 
the·symbols 
rand 
1= 
for 
provability 
in 
Peano 
arithmetic 
(PA) 
and 
truth, 
respectively. 
That 
is, 
1= 
def 
ŁŁ 
N 
<p 
{::::::> 
sentence 
<p 
IS 
true 
In 
, 
r 
<p 
sentence 
<p 
is 
provable 
in 
PA. 
To 
say 
that 
PA 
is 
sound 
says 
that 
every 
theorem 
of 
PA 
is 
truej 
in 
other 
words, 
for 
any 
sentence 
<p, 
if 
r 
<p, 
then 
1= 
<p. 
The 
soundness 
of 
PA 
can 
be 
established 
by 
induction 
on 
the 
length 
of 
proofs, 
using 
the 
fact 
that 
all 
axioms 
of 
PA 
are 
true 
and 
the 
induction 
rules 
preserve 
truth. 
(We 
haven't 
defined 
truth, 
but 
we'll 
come 
back 
to 
this 
later.) 
Let 
formulas 
of 
number 
theory 
be 
coded 
as 
natural 
numbers 
in 
some 
sonable 
way. 
Fix 
this 
coding 
and 
let 
r 
<p' 
denote 
the 
code 
of 
the 
formula 
<po 
First 
we 
prove 
a 
lemma 
due 
to 
Gödel 
that 
is 
a 
kind 
of 
fixpoint 
theorem. 
Lemma 
K.l 
(Gödel's 
fixpoint 
lemma) 
For 
any 
formula 
'IjJ(x) 
with 
one 
free 
variable 
x, 
there 
exists 
a 
sentence 
T 
such 
that 

_____________________________________________
Gödel's 
Proof 
293 
T 
+-+ 
'I/I(r 
T'); 
that 
is, 
the 
sentences 
T 
and 
'I/I(rT,) 
are 
provably 
equivalent 
in 
PA. 
Note 
that 
T 
is 
heavily 
self-referential. 
It 
asserts 
that 
its 
own 
code 
satisfies 
the 
property 
'1/1. 
Proof. 
Let 
Xo 
be 
a 
fixed 
variable. 
One 
can 
construct 
a 
formula 
SUBST(X,y,Z) 
with 
free 
variables 
x, 
y, 
z 
asserting 
the 
following: 
The 
number 
z 
is 
the 
code 
of 
the 
formula 
obtained 
by 
substituting 
the 
constant 
whose 
value 
is 
x 
for 
all 
free 
occurrences 
of 
the 
variable 
Xo 
in 
the 
formula 
whose 
code 
is 
y. 
For 
example, 
if 
cp(xo) 
is 
a 
formula 
possibly 
containing 
a 
free 
occurrence 
of 
Xo 
but 
no 
other 
free 
variables, 
then 
the 
sentence 
SUBST(7, 
r 
cp(XO)', 
312) 
is 
true 
iff 
312 
= 
rcp(7)'. 
We 
omit 
the 
details 
of 
the 
construction 
of 
SUBST, 
but 
the 
crucial 
insight 
is 
that 
given 
a 
sufficiently 
nice 
encoding 
of 
formulas 
as 
numbers, 
the 
logical 
machinery 
is 
powerful 
enough 
to 
talk 
about 
formulas 
and 
substitution. 
One 
would 
presumably 
think 
of 
numbers 
as 
bit 
strings 
and 
use 
the 
formula 
BIT(X,y) 
constructed 
in 
Lecture 
37 
for 
this 
purpose. 
Now 
define 
O'(x) 
Vy 
SUBST(X,x,y) 
--
'I/I(y), 
T 
O'(r 
O'(Xo)'). 
Then 
T 
is 
the 
desired 
fixpoint 
of 
'1/1: 
T 
= 
O'(r 
0'( 
xo) 
,) 
= 
Vy 
SUBST(r 
O'(XO)', 
r 
O'(XO)', 
y) 
--
'I/I(y) 
{::=} 
Vy 
Y 
= 
r 
O'(r 
O'(XO)')' 
--
'I/I(y) 
= 
Vy 
y 
= 
r 
T' 
--
'I/I(y) 
{::=} 
'I/I(r 
T'). 
We 
have 
argued 
informally 
that 
T 
and 
'I/I(rT,) 
are 
equivalent, 
but 
the 
entire 
argument 
can 
be 
formalized 
in 
PA. 
0 
Now 
we 
observe 
that 
the 
language 
of 
number 
theory 
is 
also 
strong 
enough 
to 
talk 
about 
provability 
in 
Peano 
arithmetic. 
In 
particular, 
it 
is 
possible 

_____________________________________________
294 
Supplementary 
Lecture 
K 
to 
code 
sequences 
of 
formulas 
as 
numbers 
and 
write 
down 
a 
formula 
PROOF(X,y) 
that 
asserts 
that 
the 
sequence 
of 
formulas 
whose 
code 
is 
given 
by 
x 
is 
a 
legal 
proof 
in 
PA 
and 
constitutes 
a 
proof 
of 
the 
formula 
whose 
code 
is 
given 
by 
y. 
That 
is, 
for 
any 
sequence 
7r 
of 
formulas 
and 
formula 
cp, 
f-
PROOF(r
7r
',rcp') 
{:=} 
7r 
is 
a 
proofin 
PA 
of 
cp. 
Provability 
in 
PA 
is 
then 
encoded 
by 
the 
formula 
PROVABLE(y) 
3x 
PROOF(X,y). 
Then 
for 
any 
sentence 
cp 
of 
L, 
f-
cp 
{:=} 
1= 
PROVABLE(r 
cp 
'). 
(K.l) 
Moreover, 
(K.2) 
This 
says, 
"cp 
is 
provable 
iff 
it 
is 
provable 
that 
cp 
is 
provable." 
The 
direction 
(=> 
) 
holds 
because 
if 
cp 
is 
provable, 
then 
there 
exists 
a 
proof 
7r 
of 
cp, 
and 
PA 
is 
clever 
enough 
to 
recognize 
that 
7r 
is 
a 
proof 
of 
cp 
(Le., 
that 
PROOF(r 
7r', 
r 
cp')) 
and 
use 
this 
fact 
a 
proof 
that 
such 
a 
proof 
exists. 
The 
direction 
(<=) 
follows 
from 
(K.l) 
and 
the 
soundness 
of 
PA. 
Applying 
the 
fixpoint 
lemma 
(LemmaK.l) 
to 
the 
predicate 
""PROVABLE(X), 
we 
obtain 
a 
sentence 
p 
that 
asserts 
its 
own 
unprovability: 
(K.3) 
in 
other 
words, 
p 
is 
true 
iff 
it 
is 
not 
provable 
in 
PA. 
By 
the 
soundness 
of 
PA, 
we 
have 
Then 
the 
sentence 
p 
must 
be 
true, 
since 
if 
not, 
then 
1= 
...,p 
=> 
1= 
PROVABLE(r 
p') 
=> 
f-
p 
=> 
1= 
P 
by 
(K.4) 
by 
(K.l) 
by 
the 
soundness 
of 
PA, 
a 
contradiction. 
Therefore 
1= 
p. 
But 
again, 
1= 
p 
=> 
1= 
...,PROVABLE(r 
p') 
by 
(K.4) 
=> 
PROVABLE(r 
p') 
=> 
J.L 
p. 
Thus 
p 
is 
true 
but 
not 
provable. 
by 
the 
definition 
of 
truth 
by 
(K.l). 
(K.4) 

_____________________________________________
Gödel's 
Proof 
295 
The 
Second 
Incompleteness 
Theorem 
In 
the 
last 
section 
we 
constructed 
a 
sentence 
p 
such 
that 
(i) 
p 
is 
true, 
but 
(ii) 
p 
is 
not 
provable 
in 
PA. 
Now 
in 
a 
weak 
moment, 
we 
might 
reason 
as 
follows. 
If 
PA 
is 
so 
all-powerful, 
why 
can't 
we 
just 
encode 
the 
wh 
oie 
argument 
of 
the 
previous 
section 
in 
PA 
Then 
(i) 
and 
(ii) 
should 
be 
provable 
in 
PA. 
But 
this 
would 
say 
that 
pis 
provable 
in 
PA, 
yet 
provably 
not 
provable 
in 
PA. 
We 
would 
appear 
to 
have 
a 
paradox 
on 
our 
hands. 
Thinking 
about 
this 
is 
enough 
to 
make 
your 
head 
swim. 
To 
sort 
this 
out, 
observe 
that 
there 
is 
logic 
going 
on 
at 
two 
levels 
here. 
The 
object 
of 
our 
study 
is 
a 
logical 
system, 
namely 
the 
language 
of 
number 
theory 
Land 
its 
deductive 
system 
PA; 
but 
we 
are 
reasoning 
about 
it 
using 
another 
logical 
system, 
wh 
ich 
we 
will 
call 
the 
metasystem. 
The 
symbols 
1-, 
1=, 
and 
{=:} 
that 
we 
used 
in 
the 
previous 
section 
are 
not 
symbols 
of 
L, 
but 
metasymbols, 
or 
symbols 
of 
the 
metasystem. 
The 
statements 
we 
made 
about 
truth 
and 
provability 
of 
sentences 
of 
L 
are 
metastatements. 
For 
example, 
let 
I{) 
be 
a 
sentence 
of 
L. 
The 
statement 
I-
I{) 
is 
not 
a 
sentence 
of 
L 
but 
a 
metastatement 
that 
says, 
"I{) 
is 
provable 
in 
PA." 
Similarly, 
1= 
I{) 
is 
a 
metastatement 
that 
says, 
"I{) 
is 
true." 
Now 
certain 
metastatements 
about 
Land 
PA 
can 
be 
encoded 
in 
L 
ing 
the 
co 
ding 
scheme 
rcjJ' 
and 
reasoned 
about 
in 
PA. 
For 
example, 
the 
metastatement 
I-
I{) 
is 
encoded 
as 
the 
sentence 
PROVABLE(r 
I{) 
,) 
of 
L. 
The 
metastatement 
(K.l) 
expresses 
the 
correctness 
of 
this 
encoding. 
Other 
metastatements 
cannot 
be 
expressed 
in 
L. 
For 
example, 
the 
statement 
"I{) 
is 
true" 
cannot 
be 
expressed 
in 
L. 
You 
might 
think 
that 
the 
I{) 
itself 
does 
this,but 
it 
doesn't, 
at 
·least 
not 
in 
the 
way 
we 
want 
to 
use 
it: 
to 
encode 
meta-arguments 
in 
PA, 
we 
have 
to 
work 
with 
the 
code 
r 
I{)' 
of 
I{), 
so 
there 
would 
have 
to 
be 
a 
formula 
TRUE( 
x) 
of 
L 
such 
that 
for 
all 
sentences 
I{) 
of 
L, 
(K.5) 
But 
it 
follows 
from 
the 
fixpoint 
lemma 
(Lemma 
K.l) 
that 
no 
such 
formula 
can 
exist. 
If 
it 
did, 
then 
there 
would 
exist 
a 
sentence 
a 
such 
that 
but 
by 
(K.5), 

_____________________________________________
296 
Supplementary 
Lecture 
K 
t= 
U 
{=} 
t= 
TRUE(r 
u
'
), 
a 
contradiction. 
The 
language 
L 
is 
not 
powerful 
enough 
to 
express 
the 
truth 
of 
sentences 
of 
L 
or 
the 
soundness 
of 
PA 
(which 
refers 
to 
truth). 
These 
are 
external 
concepts, 
and 
we 
must 
deal 
with 
them 
in 
the 
metasystem. 
However, 
Land 
PA 
are 
powerful 
enough 
to 
express 
and 
reason 
about 
provability 
and 
consistency, 
which 
are 
the 
internal 
analogs 
of 
truth 
and 
soundness, 
respectively. 
sistency 
just 
means 
that 
no 
contradiction 
can 
be 
derivedj 
in 
other 
words, 
.1 
(falsity) 
is 
not 
a 
theorem. 
The 
consistency 
of 
PA 
is 
expressed 
in 
L 
as 
follows: 
CONSIS 
...,PROVABLE(r 
.1 
,). 
Meta-arguments 
involving 
only 
the 
concepts 
of 
provability 
and 
consistency 
can 
typically 
be-mapped 
down 
into 
PA. 
For 
example, 
the 
argument 
we 
gave 
in 
the 
last 
section 
for 
(K.2) 
can 
be mapped 
down 
into 
PA, 
giving 
f-
PROVABLE(rcp') 
+-+ 
PROVABLE(rpROVABLE(rcpl)'). 
(K.6) 
With 
this 
in 
mind, 
we 
can 
try 
to 
recreate 
the 
argument 
of 
the 
previous 
section 
without 
reference 
to 
truth 
or 
soundness. 
This 
leads 
to 
the 
following 
amazing 
consequence. 
Theorem 
K.2 
(Gödel's 
second 
incompleteness 
theorem) 
No 
sufficiently 
powerful 
deductive 
system 
can 
prove 
its 
own 
consistency, 
unless 
it 
is 
inconsistent. 
Of 
course, 
if 
the 
system 
is 
inconsistent 
(i.e., 
if 
it 
can 
prove 
.1), 
then 
it 
can 
prove 
anything, 
including 
its 
own 
consistency. 
We 
prove 
the 
second 
incompleteness 
theorem 
for 
PA. 
But 
it 
actually 
holds 
for 
any 
sufficiently 
powerful 
deductive 
system, 
where 
"sufficiently 
powerful" 
just 
means 
strong 
enough 
to 
encode 
and 
reason 
ab 
out 
certain 
simple 
tatements 
involving 
provability 
and 
consistency 
such 
as 
those 
discussed 
above. 
Proof. 
Let 
p 
be 
the 
formula 
of 
(K.3). 
If 
f-
p, 
then 
f-
PROVABLE(r 
pi) 
by 
(K.2), 
but 
also 
f-
p') 
by 
(K.3), 
so 
PA 
would 
be 
inconsistent. 
Furthermore, 
this 
argument 
can 
be 
mapped 
down 
into 
PA 
using 
(K.6), 
giving 
f-
PROVABLE(r 
pi) 
_ 
""CONSIS, 
or 
in 
contrapositive 
form, 
(K.7) 

_____________________________________________
Gödel's 
Proof 
297 
Now 
suppose 
that 
consistency 
were 
provablej 
that 
is, 
I-
CONSIS. 
By 
(K.7), 
we 
would 
have 
I-
-,PROVABLE( 
p'). 
Then 
by 
(K.3), 
we 
would 
have 
I-
p. 
But 
we 
have 
just 
shown 
that 
this 
implies 
that 
PA 
is 
inconsistent. 
Thus 
if 
PA 
is 
consistent, 
then 
j.t 
CONSIS. 
o 
The 
Consistency 
of 
Mathematics 
The 
metasystem 
we 
have 
been 
talking 
about 
so 
glibly 
is 
Zermelo-Fraenkel 
set 
theory 
(ZF), 
a 
modification 
of 
Cantor's 
original 
set 
theory 
that 
evolved 
after 
Russell 
discovered 
an 
inconsistency. 
In 
Cantor's 
system 
one 
could 
form 
the 
set 
of 
all 
elements 
satisfying 
any 
given 
property. 
But 
consider 
the 
property 
x 
rt 
x. 
If 
you 
could 
form 
the 
set 
then 
bEb 
iff 
b 
rt 
b, 
a 
contradiction. 
This 
is 
known 
as 
Russell's 
paradox. 
(Notice 
any 
similarities 
to 
anything 
in 
Lecture 
31?) 
Over 
the 
years, 
mathematicians 
and 
logicians 
have 
come 
to 
fairly 
sal 
agreement 
that 
ZF 
forms 
a 
reasonable 
foundation 
for 
most 
of 
classical 
mathematics 
(type 
theory 
and 
intuitionism 
excluded). 
Pure 
ZF 
set 
theory 
is 
a 
first-order 
theory 
that 
deals 
with 
nothing 
but 
sets, 
sets 
of 
sets, 
sets 
of 
sets 
of 
sets, 
and 
so 
on. 
There 
is 
an 
"element 
of" 
relation 
E, 
a 
basic 
::let 
0, 
and 
ways 
of 
constructing 
more 
complicateq 
sets 
inductively. 
The 
natural 
numbers 
are 
defined 
inductively 
as 
:ertain 
sets: 
o 
0, 
1 
{O}, 
2 
{O, 
1}, 
def 
{ } 
3 
= 
0,1,2, 
and 
so 
on. 
There 
are 
axioms 
and 
pr.oof 
rules 
for 
manipulating 
sets, 
and 
the 
Peano 
axioms 
for 
number 
theory 
can 
be 
derived 
from 
them. 
In 
ZF 
one 
can 
give 
formal 
for 
the 
language 
L 
of 
number 
theory, 
including 
adefinition 
of 
truth. 
Or,e 
can 
then 
prove 
that 
relative 
to 
that 
semantics, 
PA 
is 
sound, 
therefore 
consistent. 
But 
is 
ZF 
consistent? 
No 
one 
knows. 
And 
Gödel's 
theorem 
tha.t 
no 
one 
can 
ever 
know, 
short 
of 
discovering 
an 
inconsistency. 
A" 
far 
as 
we 
can 
tell, 
a 
new 
Russell's 
paradox 
could 
be 
discovered 
tomorrow, 
and 
much 
of 
the 
structure 
of 
mathematics 
that 
has 
been 
built 
over 
the 
last 
century 
would 
come 
crashing 
down. 
Gödel's 
theorem 
says 
that 
we 
cannot 
prove 
the 
consistency 
of 
ZF 
in 
ZF 
any 
more 
than 
we 
can 
prove 
the 
COi1siskncy 
of 
PA 
in 
PA. 
In 
order 
to 
prove 

_____________________________________________
298 
Supplementary 
Lecture 
K 
the 
consistency 
of 
ZF, 
we 
would 
have 
to 
go 
outside 
of 
ZF 
to 
an 
even 
larger 
metasystem. 
But 
then 
we 
would 
be 
faced 
with 
the 
same 
question 
about 
the 
consistency 
of 
that 
metasystem. 
At 
some 
point, 
we 
just 
have 
to 
stop 
and 
take 
consistency 
as 
a 
matter 
of 
faith. 
Most 
mathematicians 
would 
agree 
that 
ZF 
is 
just 
as 
good 
a 
place 
as 
any 
to 
do 
this. 
But the 
only 
assurance 
we 
have 
that 
the 
ice 
is 
solid 
under 
our 
feet 
is 
that 
no 
one 
has 
broken 
through 
yet-at 
least 
not 
since 
Cantor! 

_____________________________________________
Exercises 

_____________________________________________
Homework 
1 
Homework 
1 
301 
1. 
Design 
deterministic 
finite 
automata 
for 
each 
of 
the 
following 
sets: 
(a) 
the 
set 
of 
strings 
in 
{4,8, 
1}* 
containing 
the 
substring 
481; 
(b) 
the 
set 
of 
strings 
in 
{a} 
* 
whose 
length 
is 
divisible 
by 
either 
2 
or 
7; 
(c) 
the 
Set 
of 
strings 
xE 
{O,1}* 
such 
that 
#O(x) 
is 
even 
and 
#1(x) 
is 
a 
multiple 
of 
three; 
(d) 
the 
set 
of 
strings 
over 
the 
alphabet 
{a, 
b} 
containing 
at 
least 
three 
occurrences 
of 
three 
consecutive 
b's, 
overlapping 
permitted 
(e.g., 
the 
string 
bbbbb 
should 
be 
accepted); 
.(e) 
the 
set 
of 
strings 
in 
{O, 
1, 
2} 
* 
that 
are 
ternary 
(base 
3) 
tiQns, 
leading 
zeros 
permitted, 
of 
numbers 
that 
are 
not 
multiples 
of 
four_ 
(Consider 
the 
null 
string 
a 
representation 
of 
zero.) 
2. 
Consider 
the 
following 
two 
deterministic 
finite 
automata. 
a 
b 
2F 
I 
2 1 
a b 
1 
[IT3 
2 3 1 
3F 
1 2 
Use 
the 
product 
construction 
to 
produce 
deterministic 
automata 
cepting 
(a) 
the 
intersection 
and 
(b) 
the 
union 
of 
the 
two 
sets 
accepted 
by 
these 
automata. 
3. 
Let 
M 
= 
(Q, 
E, 
0, 
s, 
F) 
be 
an 
arbitrary 
DFA. 
Prove 
by 
induction 
on 
lyl 
that 
for 
all 
strings 
x, 
y 
E 
E* 
and 
q 
E 
Q, 
8(q,xy) 
= 
8(8(q,x),y), 
where 
8 
is 
the 
extended 
version 
of 
{j 
defined 
on 
all 
strings 
described 
in 
Lecture 
3. 
4 
For 
k 
:::: 
1 
and 
p 
:::: 
2, 
let 
Ak,p 
{x 
E 
{O, 
1, 
... 
,p 
-
1} 
* 
I 
x 
is 
a 
p-ary 
representation 
of 
a 
multiple 
of 
k}. 
In 
Lecture 
4 
we 
gave 
a 
DFA 
for 
the 
set 
A
3
,2, 
the 
multiples 
of 
three 
in 
binary, 
and 
proved 
it 
correct. 
Generalize 
the 
construction 
and 
proof 
to 
arbitrary 
k 
and 
p. 

_____________________________________________
302 
Exercises 
Homework 
2 
1. 
The 
foIlowing 
nondeterministic 
automaton 
accepts 
the 
set 
of 
strings 
in 
{a, 
b} 
* 
ending 
in 
aaa. 
Convert 
this 
automaton 
to 
an 
equivalent 
ministic 
one 
using 
the 
subset 
construction. 
Show 
clearly 
which 
subset 
of 
{s, 
t, 
u, 
v} 
corresponds 
to 
each 
state 
of 
the 
deterministic 
automaton. 
Omit 
inaccessible 
states. 
a,b 
Oa 
a a 
-® 
... 
.. 
. 
s 
t 
u 
v 
2. 
The 
reverse 
of 
astring 
x, 
denoted 
rev 
x, 
is 
x 
written 
backwards. 
FormaIly, 
def 
revt" 
= 
•, 
def 
rev 
xa 
= 
arev 
X. 
For 
example, 
revabbaaab 
= 
baaabba. 
For 
A 
E*, 
define 
rev 
A 
{rev 
x 
I 
x 
E 
A} 
. 
. 
For 
exam 
pIe, 
rev 
{a, 
ab, 
aab, 
aaab} 
= 
{a, 
ba, 
baa, 
baaa}. 
Show 
that 
for 
any 
A 
E*, 
if 
Ais 
regular, 
then 
so 
is 
rev 
A. 
3. 
The 
Hamming 
distance 
between 
two 
bit 
strings 
x 
and 
y 
(notation: 
H(x,y)) 
is 
the 
number 
of 
places 
at 
which 
they 
differ. 
For 
example, 
H(011, 
110) 
= 
2. 
(If 
lxi 
::j:. 
Iyl, 
then 
their 
Hamming 
distance 
is 
infinite.) 
If 
x 
is 
astring 
and 
A 
is 
a 
set 
of 
strings, 
the 
Hamming 
distance 
between 
x 
and 
A 
is 
the 
distance 
from 
x 
to 
the 
closest 
string 
in 
A: 
H(x,A) 
minH(x,y). 
lIEA 
For 
any 
set 
A 
{O, 
1}* 
and 
k 
0, 
define 
Nk(A) 
{x 
I 
H(x,A) 
:s 
k}, 
the 
set 
of 
strings 
of 
Hamming 
distance 
at 
most 
k 
from 
A. 
For 
ample, 
No( 
{OOO}) 
= 
{OOO}, 
N
1 
({OOO}) 
= 
{OOO, 
001, 010, 
lOO}, 
and 
N2({000}) 
= 
{O, 
1P 
-
{111} 
Prove 
that 
if 
A 
{O, 
I} 
* 
is 
regular, 
then 
so 
is 
N
2 
(A). 
(Hint: 
If 
A 
is 
accepted 
by 
a 
machine 
with 
states 
Q, 
build 
a 
machine 
for 
N
2 
(A) 
with 
states 
Q 
x 
{O, 
1, 
2}. 
The 
second 
component 
teIls 
how 
many 
errors 
you 
have 
seen 
so 
far. 
Use 
nondeterminism 
to 
guess 
the 
string 
y 
E 
A 
that 
the 
input 
string 
x 
is 
similar 
to 
and 
where 
the 
errors 
are.) 

_____________________________________________
Homework 
3 
Homework 
3 
303 
1. 
Give 
regular 
expressions 
for 
each 
of 
the 
following 
su 
bsets 
of 
{a, 
b} 
* 
(a) 
{x 
I 
x 
eontains 
an 
even 
number 
of 
a's} 
(b) 
{x 
I 
x 
eontains 
an 
odd 
number 
of 
b's} 
(e) 
{x 
I 
x 
eontains 
an 
even 
number 
of 
a's 
or 
an 
odd 
number 
of 
b's} 
(d) 
{x 
I 
x 
eontains 
an 
even 
number 
of 
a's 
and 
an 
odd 
number 
of 
b's} 
Try 
to 
simplify 
the 
expressions 
as 
mueh 
as 
possible 
using 
the 
algebraie 
laws 
of 
Leeture 
9. 
Reeall 
that 
regular 
expressions 
over 
{a, 
b} 
may 
use 
E, 
fj, 
a, 
b, 
and 
operators 
+, 
*, 
and 
. 
onlYi 
the 
other 
pattern 
operators 
are 
not 
allowed. 
2. 
Give 
deterministk: 
finite 
automata 
aeeepting 
the 
sets 
of 
strings 
mateh-
ing 
the 
following 
regular 
expressions. 
(a) 
(000*+ 
111*)* 
(b) 
(01 
+ 
10)(01 
+ 
10)(01 
+ 
10) 
(e) 
(0 
+ 
1(01 
*0)*1)*' 
Try 
to 
simplify 
as 
mueh 
as 
possible. 
3. 
For 
any 
set 
of 
strings 
A, 
define 
the 
set 
MiddleThirdsA 
= 
{y 
13x,z 
lxi 
= 
lyl 
= 
Izi 
and 
xyz 
EA}. 
For 
exam 
pIe, 
MiddleThirds{ 
E, 
a, 
ab, 
bab, 
bbab, 
aabbab} 
= 
{E, 
a, 
bb}. 
Show 
that 
if 
Ais 
regular, 
then 
so 
is 
MiddleThirds 
A. 

_____________________________________________
304 
Exercises 
Homework 
4 
1. 
Show 
that 
the 
following 
sets 
are 
not 
regular. 
(a) 
{anb
m 
1 
n 
= 
2m} 
(b) 
{x 
E 
{a,b,c}* 
1 
x 
is 
a 
palindrome; 
Le 
.Ł 
x 
= 
rev(x)} 
(c) 
{x 
E 
{a, 
b, 
c} 
* 
1 
the 
length 
of 
x 
is 
a 
square} 
(d) 
The 
set 
PAREN 
of 
balanced 
strings 
of 
parentheses 
( 
). 
For 
ex-
ample, 
the 
string 
« ( ) ( ) ) ( 
)) 
is 
in 
PAREN, 
but 
the 
string 
) ( 
() 
is 
not. 
2. 
The 
operation 
of 
shuffte 
is 
important 
in 
the 
theory 
of 
concurrent 
tems. 
If 
x, 
y 
E 
E*, 
we 
write 
x 
11 
y 
for 
the 
set 
of 
all 
strings 
that 
can 
be 
obtained 
by 
shufHing 
strings 
x 
and 
y 
together 
like 
a 
deck 
of 
cards; 
for 
example, 
ab 
11 
cd 
= 
{abcd, 
acbd, 
acdb, 
cabd, 
cadb, 
cdab}. 
The 
set 
x 
11 
y 
can 
be 
defined 
formally 
by 
induction: 
• 
11 
{y}, 
xa 
11 
yb 
(x 
11 
yb)· 
{a} 
U 
(xa 
U 
y). 
{b}. 
The 
shufHe 
of 
two 
languages 
A 
and 
B, 
denoted 
All 
B, 
is 
the 
set 
of 
all 
strings 
obtained 
by 
shufHing 
astring 
from 
A 
with 
astring 
from 
B: 
All 
B 
U 
xII 
y. 
For 
example, 
zEA 
y 
E 
B 
{ab} 
11 
{cd,e}'= 
{abe,aeb,eab,abcd,acbd,acdb,cabd,cadb,cdab}. 
(a) 
What 
is 
(01)* 
11 
(10)*? 
(b) 
Show 
that 
if 
A 
and 
B 
are 
regular 
sets, 
then, 
so 
is 
A 
11 
B. 
(Hint: 
Put 
a 
pebble 
on 
a 
machine 
for 
A 
and 
one 
on 
a 
machine 
for 
B. 
Guess 
nondeterministically 
which 
pebble 
to 
move. 
Accept 
if 
both 
pebbles 
occupy 
accept 
states.) 

_____________________________________________
3. 
Homework 
4 
305 
For 
each 
of 
the 
two 
automata 
a 
b 
a 
b 
--+ 
1 
1 
4 
--+ 
1F 
3 
5 
2 
3 
1 
2F 
8 
7 
3F 
4 
2 
3 
7 
2 
4F 
3 
5 
4 
6 
2 
5 
4 
6 
5 
1 
8 
6 
6 
3 
6 
2 
3 
7 
2 
4 
7 
1 
4 
8 
3 
1 
8 
5 
1 
(a) 
say 
which 
states 
are 
accessible 
and 
wh 
ich 
are 
not; 
(b) 
list 
the 
equivalence 
classes 
of 
the 
collapsing 
relation;::;:: 
defined 
in 
Lecture 
13: 
def 
* 
p;::;:: 
q 
{:::::} 
Vx 
E 
(6(p,x) 
E 
P 
{:::::} 
6(q,x) 
E 
F)i 
(c) 
give 
the 
automaton 
obtained 
by 
collapsing 
equivalent 
states 
and 
removing 
inaccessible 
states. 

_____________________________________________
306 
Exercises 
Homework 
5 
1. 
The 
following 
table 
defines 
four 
special 
types 
of 
CFGs 
obtained 
by 
restricting 
productions 
to 
the 
form 
shown, 
where 
A, 
B 
represent 
terminals, 
a 
a 
single 
terminal 
symbol, 
and 
x 
astring 
of 
terminals: 
Grammar 
type 
Form 
01 
productions 
right-linear 
A 
..... 
xB 
or 
A 
--+ 
x 
strongly 
right-linear 
A 
..... 
aB 
or 
A 
--+ 
f 
left-linear 
A 
..... 
Bx 
or 
A 
..... 
x 
strongly 
left-linear 
A 
--+ 
Ba 
or 
A 
..... 
f 
Prove 
that 
each 
of 
these 
four 
types 
of 
grammars 
generates 
exactly 
the 
regular 
sets. 
Conclude 
that 
every 
set 
is 
a 
CFL. 
2. 
Prove 
that 
the 
CFG 
S 
..... 
aSb 
I 
bSa 
I 
SS 
I 
f 
generates 
the 
set 
of 
all 
strings 
over 
{a, 
b} 
with 
equally 
many 
a's 
and 
b's. 
(Hint: 
Characterize 
elements 
of 
the 
set 
in 
terms 
of 
the 
graph 
of 
the 
function 
#b(y)-
#a(y) 
as 
y 
ranges 
over 
prefixes 
of 
x, 
as 
we 
did 
in 
Lecture 
20 
with 
balanced 
parentheses.) 
3. 
Give 
a 
CFG 
for 
t1.e 
set 
PAREN
2 
of 
balanced 
strings 
of 
parentheses 
of 
two 
types 
( ) 
and 
[]. 
For 
example, 
([ 
() 
[J] 
([J 
» 
is 
in 
PAREN
2
, 
but 
[ 
(] 
) 
is 
not. 
Prove 
that 
your 
grammar 
is 
correct. 
Use 
the 
following 
inductive 
definition: 
PAREN
2 
is 
the 
smallest 
set 
of 
strings 
such 
that 
(i) 
f 
E 
PAREN2; 
(ii) 
if 
x 
E 
PAREN
2
, 
then 
so 
are 
(x) 
and 
[x]; 
and 
(iii) 
if 
x 
and 
y 
are 
both 
in 
PAREN
2
, 
then 
so 
is 
xy. 
(Hint: 
Your 
grammar 
should 
closely 
model 
the 
inductive 
definition 
of 
the 
set. 
For 
one 
direction 
of 
the 
proof 
of 
correctness, 
use 
induCtion 
on 
the 
length 
of 
the 
derivation. 
For 
the 
other 
direction, 
use 
induction 
on 
stages 
of 
the 
inductive 
definition 
of 
PAREN
2
Ł 
The 
basis 
is 
(i), 
and 
there 
will 
be 
two 
cases 
of 
the 
induction 
step 
corresponding 
to 
(ii) 
and 
(iii).) 
4. 
Give 
a PDA 
for 
the 
set 
PAREN
2 
of 
Exercise 
3 
that 
accepts 
by 
empty 
stack. 
Specify 
all 
transitions. 

_____________________________________________
Homework 
6 
Homewor.k 
6 
307 
1. 
Prove 
that 
the 
following 
CFG 
G 
in 
Greibach 
normal 
form 
generates 
exactly 
the 
set 
of 
nonnull 
strings 
over 
{a, 
b} 
with 
equally 
many 
a's 
and 
b's: 
S 
-+ 
aB 
I 
bA, 
A 
-+ 
aS 
I 
bAA 
I 
a, 
B 
-+ 
bS 
I 
aBB 
I 
b. 
(Hint: 
Strengthen 
your 
induction 
hypothesis 
to 
describe 
the 
sets 
of 
strings 
generated 
by 
the 
nonterminals 
A 
and 
B: 
for 
x 
=I 
E, 
S 
x<==> 
#a(x) 
= 
#b(x), 
G 
A 
x<==> 
???, 
G 
B 
Ł 
??? 
) 
---+ 
x<==> 
.... 
G 
2. 
Construct 
a 
pushdown 
automaton 
that 
accepts 
the 
set 
of 
strings 
in 
{a, 
b} 
* 
with 
equally 
many 
a's 
and 
b's. 
Specify 
all 
transitions. 
3. 
Let 
b(n) 
denote 
the 
binary 
representation 
of 
n 
1, 
leading 
zeros 
omitted. 
For 
example, 
b(5) 
= 
101 
and 
b(12) 
= 
1100. 
Let 
$ 
be 
another 
symbol 
not 
in 
{O, 
I}. 
(a) 
Show 
that 
the 
set 
{b( 
n 
) 
$b( 
n 
+ 
1) 
I 
n 
I} 
is 
not 
a 
CFL. 
(b) 
Suppose 
we 
reverse 
the 
first 
numeral; 
that 
is, 
consider 
the 
set 
{revb(n)$b(n+ 
1) 
I 
n 
I}. 
Show 
that 
this 
set 
is 
a 
CFL. 
4. 
Recall 
from 
Exercise 
3 
of 
Homework 
5 
the 
set 
PAREN2 
of 
balanced 
strings 
of 
parentheses 
of 
two 
types, 
( ) 
and 
[]. 
Give 
CFGs 
in 
Chomsky 
and 
Greibach 
normal 
form 
generating 
the 
set 
PAREN2-{E}, 
and 
prove 
that 
they 
are 
correct. 

_____________________________________________
308 
Exercises 
Homework 
7 
1. 
Describe 
& 
parser 
to 
parse 
regular 
expressions 
according 
to 
the 
dence 
relation 
* 
>. 
>+ 
Here 
> 
means 
"has 
higher 
precedence 
than." 
The 
main 
differences 
you 
will 
have 
to 
account 
for 
between 
regular 
expressions 
and 
the 
metic 
expressions 
discussed 
in 
Lecture 
26 
are: 
(i) 
the 
unary 
operator 
* 
comes 
after 
its 
operand, 
not 
before 
it 
as 
with 
unary 
minus; 
and 
(ii) 
the 
concatenation 
operator 
. 
can 
be 
omitted. 
Illustrate 
the 
action 
of 
your 
parser 
on 
the 
expression 
(a 
+ 
b)* 
+ 
ab*a. 
2. 
Prove 
that 
if 
A 
is 
a 
CFL 
and 
R 
is 
a 
regular 
set, 
then 
An 
R 
is 
a 
CFL. 
(Hint: 
Use 
a 
product 
construction.) 
3. 
Recall 
the 
shufBe 
operator 
11 
from 
Homework 
4. 
Show 
that 
the 
shufBe 
of 
two 
CFLs 
is 
not 
necessarilya 
CFL. 
(Hint: 
Use 
the 
previous 
exercise 
to 
simplify 
the 
argument.) 
4. 
Let 
A 
be 
any 
regular 
set. 
Show 
that 
the 
set 
{x 
13y 
lyl 
= 
2
1
2:1 
and 
xy 
E 
A} 
is 
regular. 

_____________________________________________
Homework 
8 
Homework 
8 
309 
1. 
Describe 
a 
TM 
that 
accepts 
the 
set 
{an 
I 
n 
is 
apower 
of 
2}. 
Your' 
description 
should 
be 
at 
the 
level 
of 
the 
descriptions 
in 
Lecture 
29 
of 
·the 
TM 
that 
accepts 
{ww 
Iw 
E 
1::*} 
and 
the 
TM 
that 
implements 
the 
sieve 
of 
Eratosthenes. 
In 
particular, 
do 
not 
give 
a 
list 
of 
transitions. 
2. 
A 
linear 
bounded 
automaton 
(LBA) 
is 
exactly 
like 
a 
one-tape 
Turing 
machine, 
except 
that 
the 
input 
string 
x 
e 
1::* 
is 
enclosed 
in 
left 
and 
right 
endmarkers 
and 
-I 
which 
may 
not 
be 
overwritten, 
and 
the 
machine 
is 
constrained 
never 
to 
move 
left 
of 
the 
nor 
right 
of 
the 
-I. 
It 
may 
read 
and 
write 
all 
it 
wants 
between 
the 
endmarkers. 
two-way, 
read/write 
Q 
(a) 
Give 
a 
rigorous 
formal 
definition 
of 
deterministic 
linearly 
bounded 
automata, 
including 
adefinition 
of 
configurations 
and 
tance. 
Your 
definition 
should 
begin 
as 
follows: 
"A 
determiniltic 
linearly 
bounded 
automaton 
(LB..A) 
is 
a 
9-tuple 
. M 
= 
(Q, 
1::, 
r, 
-1,6, 
8, 
t, 
r), 
where 
Q 
is 
a 
finite 
set 
of 
s,tates, 
" 
(b) 
Let 
M 
be 
a 
linear 
bounded 
automaton 
with 
state 
set 
Q 
of 
size 
k 
and 
tape 
alphabet 
r 
of 
size 
m. 
How 
many 
possible 
configurations 
are 
there 
on 
input 
x, 
lxi 
= 
n? 
(c) 
Argue 
that 
the 
halting 
problem 
for 
deterministic 
linear 
bounded 
automatais 
decidable. 
(Hint: 
You 
need 
to 
be 
able 
to 
detect.a.fter 
a 
finite 
time 
if 
the 
machine 
is 
in 
an 
infinite 
loop. 
Presumably 
the 
result 
of 
part 
(b) 
would 
be 
useful 
here.) 
( 
d) 
Prove 
by 
diagonalization 
that 
there 
exists 
a 
recursive 
set 
that 
is 
not 
accepted 
by 
any 
LBA. 
. 
3. 
Let 
A 
be 
any 
regular 
set. 
Show 
that 
the 
set 
{x 
I 
3l' 
Il'l 
= 
Ixl
2 
and 
xl' 
e 
A} 
is 
regular. 

_____________________________________________
310 
Exercises 
Homework 
9 
1. 
Prove 
that 
the 
following 
question 
is 
undecidable. 
Given 
a 
Turing 
chine 
M 
and 
state 
q 
of 
M, 
does 
M 
ever 
enter 
state 
q 
on 
some 
input? 
(This 
problem 
is 
analogous 
to 
the 
problem 
of 
identifying' 
dead 
code: 
given 
a 
PASCAL 
pro 
gram 
containing 
a 
Clesignated 
block 
of 
code, 
will 
that 
block 
of 
code 
ever 
be 
executed?) 
2. 
Prove 
that 
it 
is 
undecidable 
whether 
two 
given 
Turing 
machines 
accept 
the 
same 
set. 
(This 
problem 
is 
analogous 
to 
determining 
whether 
two 
given 
PASCAL 
programs 
are 
equivalent.) 
3. 
Prove 
that 
the 
emptiness 
problem 
for 
deterministic 
linearly 
bounded 
automata 
(i.e., 
whether 
L(M) 
= 
0) 
is 
undecidable. 
(Hint: 
Think 
VALCOMPS.) 
4. 
Prove 
that 
an 
r.e. 
set 
is 
recursive 
iff 
there 
exists 
an 
enumeration 
machine 
that 
enumerates 
it 
in 
increasing 
order. 
5. 
For 
A, 
B 
define 
AI 
B 
'{ 
x 
I 
3y 
E 
B 
xy 
E 
A}, 
A..-
B 
{x 
I 
'v'y 
E 
B 
xy 
E 
A}. 
(a) 
Show 
that 
if 
Ais 
regular 
and 
B 
is 
any 
set 
whatsoever, 
then 
AlB 
and 
A..-
B 
are 
regular. 
(b) 
Show 
that 
even 
if 
we 
are 
given 
a 
finite 
automaton 
for 
A 
and 
a 
ing 
machine 
for 
B, 
we 
cannot 
necessarily 
construct 
an 
automaton 
for 
AlB 
or 
A 
..-
B 
effectively. 

_____________________________________________
Homework 
10 
Homework 
10 
311 
1. 
Show 
that 
neither 
the 
set 
TOTAL 
{M 
I 
M 
halts 
on 
all 
inputs} 
nor 
its 
complement 
is 
r.e. 
2. 
Consider 
one-tape 
Turing 
machines 
that 
are 
constrained 
not 
to 
write 
the 
input 
string. 
They 
may 
write 
all 
they 
want 
on 
the 
blank 
portion 
of 
the 
tape 
to 
the 
right 
of 
the 
input 
string. 
(a) 
Show 
that 
these 
accept 
only 
regular 
sets. 
(If 
you're 
thinking, 
"Hey, 
why 
not 
just 
copy 
the 
input 
string 
out 
to 
the 
blank 
portion 
of 
the 
tape," 
think 
again 
... 
) 
(b) 
Show 
that, 
despite 
(a), 
it 
is 
impossible 
to 
construct 
an 
equivalent 
finite 
automaton 
effectively 
from 
such 
a 
machine. 
3. 
Show 
that 
it 
is 
undecidable 
whether 
the 
intersection 
of 
two 
CFLs 
is 
nonempty. 
(Hint: 
Use 
a 
variant 
of 
VALCOMPS 
in 
which 
every 
other 
configuration 
is 
reversed: 
o:o#rev 
0:1 
#0:2 
#rev 
0:3#··· 
#O:n. 
Express 
this 
set 
as 
the 
intersection 
of 
two 
CFLs.) 

_____________________________________________
312 
Exercises 
Homework 
11 
1. 
Recursive 
enumerability 
is 
intimately 
linked 
with 
the 
idea 
of 
unbounded 
existential 
search. 
Often 
an 
algorithm 
for 
accepting 
an 
r.e. 
set 
can 
be 
characterized 
in 
terms 
of 
searching 
for 
a, 
witness 
or 
prool 
ihat 
a 
given 
input 
x 
is 
in 
the 
set. 
A 
binary 
relation 
R 
on 
strings 
over 
{O, 
1} 
is 
called 
recursive 
if 
the 
set 
{x#y 
I 
R(x, 
y)} 
is 
a 
recursive 
set. 
Here 
# 
is 
just 
another 
input 
symbol 
different 
from 
o 
or 
1. 
Show 
that 
a 
set 
A 
{O, 
1} 
* 
is 
r 
.e. 
if 
and 
only 
if 
there 
exists 
a 
recursive 
binary 
relation 
R 
such 
that 
A 
= 
{x 
E-{O, 
1}* 
13y 
R(x,y)}. 
2. 
Show 
that 
it 
is 
undecidable 
whether 
the 
intersection 
of 
two 
given 
CFLs 
is 
again 
a 
CFL. 
(Hint: 
Use 
Homework 
10, 
Exercise 
3.) 

_____________________________________________
Homework 
12 
Homework 
12 
313 
1. 
A 
context-sensitive 
gramm 
ar 
(CSG) 
or 
type 
1 
grammar 
is 
a 
type 
0 
grammar 
that 
obeys 
the 
following 
additional 
restriction: 
all 
tions 
a 
-
ß 
satisfy 
lai 
:5 
IßI. 
Give 
CSGs 
for 
the 
following 
sets. 
(a) 
{x 
E 
{a,b,c}+ 
I 
#a(x) 
= 
#b(x) 
= 
#c(x)} 
(b) 
{a
nl 
I 
n 
1} 
2. 
Show 
that 
context-sensitive 
gra.mmars 
and 
nondeterministic 
linearly 
bounded 
automata 
are 
equivalent 
in 
the 
following 
sense: 
(a) 
for 
every 
context-sensitive 
gra.mmar 
G, 
there 
is 
a 
istic 
LBA 
M 
such 
that 
L(M) 
= 
L(G)j 
and 
(b) 
for 
every 
nondeterministic 
LBA 
M, 
there 
is 
a 
context-sensitive 
grammar 
G 
such 
that 
L(G) 
= 
L(M) 
-
{tl 
3. 
Give 
constructions 
showing 
that 
the 
following 
number-theoretic 
tions 
are 
primitive 
recursive. 
(a) 
quotient(x,y) 
= 
quotient 
when 
dividing 
x 
by 
y 
using 
integer 
divisionj 
for 
example, 
quotient(7,2) 
= 
3. 
(b) 
remainder( 
x, 
y) 
= 
remainder 
when 
dividing 
x 
by 
y 
usinl; 
integer 
divisionj 
for 
example, 
remainder(7, 
2) 
= 
1. 
(c) 
priqle( 
x) 
= 
1 
if 
x 
is 
prime, 
0 
otherwise. 

_____________________________________________
Miscellaneous 
Exercises 
Finite 
Automata 
and 
Regular 
Sets 
1. 
Let 
B 
be 
a 
set 
of 
strings 
over 
a 
fixed 
finite 
alphabet. 
We 
say 
B 
is 
transitive 
if 
BB 
Band 
reflexive 
if 
• 
E 
B. 
Prove 
that 
for 
any 
set 
of 
strings 
A, 
A 
* 
is 
the 
smallest 
reflexive 
and 
transitive 
set 
containing 
A. 
That 
is, 
show 
that 
A 
* 
is 
a 
reflexive 
and 
transitive 
set 
containing 
A, 
and 
if 
B 
is 
any 
other 
reflexive 
and 
transitive 
set 
containing 
A, 
then 
A* 
B. 
2. 
Consider 
the 
following 
pairs 
of 
deterministic 
finite 
automata. 
Use 
the 
product 
construction 
to 
produce 
deterministic 
automata 
accepting 
(i) 
the 
intersection 
and 
(ii) 
the 
union 
of 
the 
sets 
accepted 
by 
these 
tomata. 
(a) 
a 
b 
a 
b 
-
IrrT 
-
11fT 
2F 
1 1 
2F 
2 1 
(b) 
a 
b 
a 
b 
-
IrrT 
-
IFrrT 
2F 
3 1 
2 1 3 
3F 
1 2 
3F 
2 1 

_____________________________________________
316 
Miscellaneous 
Exercises 
(c) 
-+ 
a 
b 
If22 
2F 
11 
1 
-+ 
a 
b 
2F 
11 
2 
3. 
Consider 
the 
following 
nondeterministic 
finite 
automaton. 
(a) 
Give 
astring 
beginning 
with 
a 
that 
is 
not 
accepted. 
(b) 
Construct 
an 
equivalent 
deterministic 
automaton 
using 
the 
set 
construction. 
Assuming 
the 
states 
are 
named 
8, 
t, 
u, 
v 
from 
left 
to 
right, 
show 
clearly 
which 
subset 
of 
{8, 
t, 
U, 
v} 
corresponds 
to 
each 
state 
of 
the 
deterministic 
automaton. 
Omit 
inaccessible 
states. 
4. 
Consider 
the 
following 
NFA. 
(a) 
Construct 
an 
equivalent 
DFA 
using 
the 
subset 
construction. 
Omit 
inaccessible 
states. 
(b) 
Give 
an 
equivalent 
regular 
expression. 
5. 
Convert 
the 
following 
nondeterministic 
finite 
automata 
to 
equivalent 
deterministic 
ones 
using 
the 
subset 
construction. 
Show 
clearly 
which 
sub 
set 
of 
{8, 
t, 
U, 
v} 
corresponds 
to 
each 
state 
of 
the 
determiriistic 
automaton. 
Omit 
inaccessible 
states. 
(a) 
Ö. 
a 
b· 
.® 
... 
I Ł 
8 
t 
U 
v 

_____________________________________________
Finite 
Automata 
and 
Regular 
Sets 
317 
(b) 
a,b 
a,b 
Oa 
a 
ŁŁ 
b 
0 
.. 
. 
s 
t 
u 
V 
H6. 
Prove 
that 
NFAs 
are 
exponentially 
more 
succinct 
than 
DFAs: 
for 
any 
m, 
there 
exists 
an 
NFA 
with 
m 
states 
such 
that 
any 
equivalent 
DFA 
has 
at 
least 
2
m
-
1 
states. 
7. 
A 
convenient 
way 
of 
specifying 
automata 
is 
in 
terms 
of 
transition 
tri 
ces. 
If 
the 
automaton 
has 
n 
states, 
the 
transition 
function 
6 
can 
be 
specified 
by 
an 
n 
x 
n 
matrix 
G, 
indexed 
by 
states, 
whose 
u, 
vth 
entry 
gives 
the 
set 
of 
input 
symbols 
taking 
state 
u 
to 
state 
V; 
in 
symbols, 
GU'IJ 
= 
{a 
E 
E 
16(u,a) 
= 
v}. 
For 
example, 
the 
transition 
function 
of 
the 
automaton 
of 
Example 
3.1 
of 
Lecture 
3 
could 
be 
represented 
by 
the 
4 
x 
4 
matrix 
{al 
{b} 
o 
o 
o 
{a} 
{b} 
o 
o 
1 
o 
{al 
{a,b} 
Consider 
the 
collection 
of 
square 
matrices 
indexed 
by 
Q 
whose 
entries 
are 
subsets 
of 
E*. 
We 
can 
define 
addition 
and 
multiplication 
on 
such 
matrices 
in 
a 
natural 
way 
as 
follows: 
( 
) 
def 
A 
+ 
B 
U'IJ 
= 
Au'IJ 
U 
Bu'IJ' 
(AB)u'IJ 
U 
AuwBw'IJ. 
wEQ 
Let 
us 
also 
define 
the 
identity 
matrix 
I: 
if 
u 
= 
v, 
otherwise. 
The 
powers 
of 
a 
matrix 
are 
defined 
inductively: 
A
O 
I, 
An+! 
An 
A . 
Ł 
S(a) 
Prove 
that 
(An)u'IJ 
= 
{x 
E 
E* 
Ilxl 
= 
n 
and 
8(u, 
x) 
= 
v}. 

_____________________________________________
318 
MiscelianeousExercises 
(b) 
Define 
the 
asterate 
A 
* 
of 
the 
matrix 
A 
to 
be 
the 
componentwise 
union 
of 
all 
the 
powers 
of 
A: 
(A*)uv 
U 
(An)uv. 
Let 
s 
be 
the 
start 
state 
of 
the 
automaton 
and 
F 
the 
set 
of 
accept 
states. 
Prove 
that 
L(M) 
= 
U 
(A*)st. 
tEF 
8. 
Generalize 
Homework 
2, 
Exercise 
3 
to 
arbitrary 
distance 
k. 
That 
is, 
prove 
that 
if 
A 
{O, 
I} 
* 
is 
regular, 
then 
so 
is 
N 
k 
(A), 
the 
set 
of 
strings 
of 
Hamming 
distance 
at 
most 
k 
from 
some 
string 
in 
A. 
9. 
(a) 
Show 
that 
if 
an 
NFA 
with 
k 
states 
accepts 
any 
string 
at 
all, 
then 
it 
accepts 
astring 
of 
length 
k 
-
1 
or 
less. 
H 
(b) 
Give 
an 
NFA 
over 
a 
single 
letter 
alphabet 
that 
rejects 
some 
string, 
but 
the 
length 
of 
the 
shortest 
rejected 
string 
is 
strictly 
more 
than 
the 
number 
of 
states. 
uH 
(c) 
Give 
a 
construction 
for 
arbitrarily 
large 
NFAs 
showing 
that 
the 
length 
of 
the 
shortest 
rejected 
string 
can 
be 
exponential 
in 
the 
number 
of 
states. 
10. 
Recall 
from 
Lecture 
10 
that 
an 
NFA 
with 
E-transitions 
is 
a 
structure 
M 
= 
(Q, 
•, 
.0., 
S, 
F) 
such 
that 
• 
is 
a 
special 
symbol 
not 
in 
and 
M• 
= 
(Q, 
.0., 
S, 
F) 
is 
an 
ordinary 
NFA 
over 
the 
alphabet 
U 
{•}. 
Define 
the 
E-closure 
CE(A) 
of 
a 
set 
A 
Q 
to 
be 
the 
set 
of 
all 
states 
reachable 
from 
some 
state 
in 
A 
under 
a 
sequence 
of 
zero 
or 
more 
f-transitions: 
CE(A) 
U 
"'E{•}* 
(a) 
Using 
E-closure, 
define 
formally 
acceptance 
for 
NFAs 
with 
transitions 
in 
a'way 
that 
captures 
the 
intuitive 
description 
given 
in 
Lecture 
6. 
(b) 
Prove 
that 
under 
your 
definition, 
NFAs 
with 
f-transitions 
accept 
only 
regular 
sets. 

_____________________________________________
Finite 
Automata 
and 
Regular 
Sets 
319 
(c) 
Prove 
that 
the 
two 
definitions 
of 
acceptance-the 
one 
given 
in 
part 
(a) 
involving 
E-closure 
and 
the 
one 
given 
in 
Lecture 
10 
involving 
homomorphisms-are 
equivalent. 
11. 
Give 
regular 
expression& 
for 
each 
of 
the 
following 
subsets 
of 
{a,b}*. 
Recall 
that 
regular 
expressions 
over 
{a, 
b} 
may 
use 
E, 
ß, 
a, 
b, 
and 
operators 
+, 
*, 
and 
. 
only. 
(a) 
{x 
I 
x 
does 
not 
contain 
the 
substring 
a} 
(b) 
{x 
I 
x 
does 
not 
contain 
the 
su 
bstring 
ab} 
.. 
(c) 
{x 
I 
x 
does 
not 
contain 
the 
su 
bstring 
aba} 
Try 
to 
simplify 
the 
expressions 
as 
much 
as 
possible 
using 
the 
algebraic 
laws 
of 
Lecture 
9. 
12. 
Match 
each 
NFA 
with 
an 
equivalent 
regular 
expression. 
(a) 
1 
(b) 
1 
(c) 
1 
1 
0 
0 
000 
(d) 
1 
(e) 
1 
1 
0 
o 
0 
(i) 
E 
+,0(01 
*1 
+ 
00)*01 
* 
(ii) 
f 
+ 
0(10*1 
+ 
10)*10* 
(iii) 
E + 
0(10*1 
+ 
00)*0 
(iv) 
E 
+ 
0(01 
*1 
+ 
00)*0 
(v) 
f 
+ 
0(10*1 
+ 
10)*1 

_____________________________________________
320 
Miscellaneous 
Exercises 
13. 
Match 
each 
NFA 
with 
an 
equivalent 
regular 
expression. 
b 
b 
(c) 
b 
b 
a 
0 
a 
a 
b 
a 
a 
b 
b 
a 
a 
a 
a 
a 
a 
(i) 
(aa*b 
+ 
ba*b)*ba* 
(ii) 
(aa*a 
+ 
aa*b)*aa* 
(iii) 
(ba*a 
+ 
ab*b)*ab* 
(iv) 
(ba*a 
+ 
aa*b)*aa* 
(v) 
(ba*a 
+ 
ba*b)*ba* 
14. 
Give 
an 
NFA 
with 
four 
states 
equivalent 
to 
the 
regular 
expression 
(01 
+ 
011 
+ 
0111)*. 
Convert 
this 
automaton 
to 
an 
equivalent 
deterministic 
one 
using 
the 
subset 
construction. 
Name 
the 
states 
of 
your 
NFA, 
and 
show 
clearly 
wh 
ich 
set 
of 
states 
corresponds 
to 
each 
state 
of 
the 
DFA. 
Omit 
cessible 
states. 
15. 
Give 
a 
regular 
expression 
equivalent 
to 
the 
following 
automaton. 
16. 
Give 
deterministic 
finite 
automata 
equivalent 
to 
the 
following 
regular 
expressions. 

_____________________________________________
·Finite 
Automata 
and 
Regular 
Sets 
321 
(a) 
(00 
+ 
11)* 
(01 
+ 
10) 
(00 
+ 
11)* 
(b) 
(000)*1 
+ 
(00)*1 
(e) 
(0(01)*(1 
+ 
00) 
+ 
1(10)*(0 
+ 
11))* 
17. 
Give 
a 
regular 
expression equivalent 
to 
the 
following 
DFA. 
b 
a 
a 
b 
18. 
Consider 
the 
regular 
sets 
denoted 
by 
the 
following 
pairs 
of 
regular 
expressions. 
For 
each 
pair, 
say 
whether 
the 
two 
sets 
are 
equal. 
If 
so, 
give 
a 
proof 
using 
the 
algebraie 
laws 
of 
Leeture 
9;if 
not, 
give 
an 
example 
of 
astring 
in 
one 
that 
is 
not 
in 
the 
other. 
(i) 
(0 
+ 
1)* 
0* 
+ 
1* 
(ii) 
0(120)*12 
01(201)*2 
(Hi) 
10* 
(iv) 
(0*1*)* 
(0* 
1)* 
(v) 
(01 
+ 
0)*0 
0(10 
+ 
0)* 
19. 
Let 
a: 
= 
(a 
+ 
b)*a6(a 
+ 
b)*). 
Give 
a 
regular 
expression 
equivalent 
to 
the 
pattern 
'" 
a 
when 
(a) 
= 
{a,b}, 
(b) 
= 
{a, 
b, 
c}. 
Simplify 
the 
expressions 
as 
mueh 
as 
possible. 
20. 
Prove 
the 
following 
theorems 
of 
Kleene 
algebra. 
Reason 
equationally 
using 
axioms 
(A.1) 
through 
(A.15) 
only. 
S(a) 
a*a* 
= 
a* 
(b) 
a*a=aa* 
(e) 
a** 
= 
a* 

_____________________________________________
322 
Miscellaneous 
Exercises 
(d) 
(a*b)*a* 
= 
(a 
+ 
b)* 
(e) 
a(ba)* 
= 
(ab)*a 
(f) 
a* 
= 
(aa)* 
+ 
a(aa)* 
21. 
Prove 
Lemma 
A.1. 
·22. 
Prove 
that 
in 
the 
presence 
of 
Kleene 
algebra 
axioms 
(A.l) 
through 
(A.ll), 
axioms 
(A.12) 
and 
(A.14) 
are 
equivalent 
(and 
by 
symmetry, 
so 
are 
(A.13) 
and 
(A.15)). 
·23. 
Here 
is 
a 
purely 
algebraic 
version 
of 
Miscellaneous 
Exercise 
1. 
An 
ement 
C 
of 
a 
Kleene 
algebra 
1( 
is 
said 
to 
be 
reflexive 
if 
1 
c 
and 
transitive 
if 
cc 
c. 
We 
say 
that 
c 
contains 
a 
if 
a 
c. 
Prove 
that 
for 
anya 
E 
1(, 
(a) 
a* 
is 
reflexive 
and 
transitive 
and 
contains 
aj 
and 
(b) 
a* 
is 
the 
least 
element 
of 
1( 
satisfying 
these 
properties. 
That 
is, 
if 
c 
is 
any 
element 
of 
I( 
that 
is 
reflexive 
and 
transitive 
and 
contains 
a, 
then 
a*' 
c. 
This 
justifies 
the 
terminology 
reflexive 
transitive 
closure. 
··24. 
Prove 
Lemma 
A.2: 
the 
family 
M(n, 
1() 
of 
n 
x 
n 
matrices 
over 
a 
Kleene 
algebra 
1( 
with 
the 
matrix 
operations 
defined 
as 
in 
Supplementary 
Lecture 
A 
again 
forms 
a 
Kleene 
algebra. 
s25. 
Prove 
Theorem 
A.3. 
HS26. 
For 
any 
set 
of 
strings 
A, 
define 
the 
set 
FirstHalves 
A 
= 
{x 
I 
3y 
lyl 
= 
lxi 
and 
xy 
E 
A}. 
For 
example, 
FirstHalves 
{a, 
ab, 
bab, 
bbab} 
= 
{a, 
bb}. 
Show 
that 
if 
Ais 
regular, 
then 
so 
is 
FirstHalves 
A. 
27. 
For 
any 
set 
of 
strings 
A, 
define 
the 
set 
FirstThirdsA 
= 
{x 
13y 
lyl 
= 
21xl 
and 
xy 
E 
A}. 
For 
example, 
FirstThirds 
{!ö, 
a, 
ab, 
bab, 
bbab} 
= 
{!ö, 
b}. 
Show 
that 
if 
A 
is 
regular, 
then 
so 
is 
FirstThirds 
A. 
28. 
Given 
a 
set 
A 
{O, 
l}*, 
let 
A' 
= 
{xy 
I 
xly 
E 
A}. 

_____________________________________________
.Finite 
Automata 
and 
Regular 
Sets 
323 
That 
is, 
A' 
consists 
of 
all 
strings 
obtaiJled 
from 
astring 
in 
A 
by 
deleting 
exactly 
one 
1. 
Show 
that 
if 
Ais 
regular, 
then 
so 
is 
A'. 
29. 
For 
Aaset 
of 
natural 
numbers, 
define 
binary 
A 
= 
{binary 
representations 
of 
numbers 
in 
A} 
(0 
+ 
1) 
* , 
unary 
A 
= 
{on 
In 
E 
A} 
0* 
For 
example, 
if 
A 
= 
{2,3, 
5}, 
then 
binary 
A 
= 
{1O, 
11, 
101}, 
unary 
A 
= 
{OO, 
000, 
OOOOO}. 
Consider 
the 
following 
two 
propositions: 
(i) 
For 
all 
A, 
if 
binary 
Ais 
regular, 
then 
so 
is 
unary 
A. 
(ii) 
For 
all 
A, 
if 
unary 
A 
is 
regular, 
then 
so 
is 
binary 
A. 
One 
of 
(i) 
and 
(ii) 
is 
true 
and 
the 
other 
is 
false. 
Which 
is 
which? 
Give 
a 
proof 
and 
a 
counterexample. 
*30. 
Let 
A 
be 
a 
regular 
set. 
Consider 
the 
two 
sets 
{x 
13n 
0 
3y 
E 
A y 
= 
x
n
}, 
{x 
13n 
0 
3y 
E 
A x 
= 
yn}. 
One 
is 
necessarily 
regular 
and 
one 
is 
not. 
Which 
is 
which? 
Give 
a 
proof 
and 
a 
counterexample. 
*H31. 
One 
of 
the 
following' 
subsets 
of 
{a, 
b, 
$}* 
is 
regular 
and 
the 
other 
is 
not. 
Which 
is 
which? 
Give 
proofs. 
{xy 
I 
x,y 
E 
{a,b}*, 
#a(x) 
= 
#b(y)} 
{x$y 
I 
x,y 
E 
{a,b}*, 
#a(x) 
= 
#b(y)} 
Two 
of 
the 
following 
three 
sets 
are 
always 
regular 
for 
any 
regular 
set 
A. 
Which 
are 
they? 
Give 
two 
proofs 
and 
a 
counterexample. 
(a) 
{x 
I 
xl"l 
E 
A} 
(b) 
{x 
13y 
lyl 
= 
2
21z1 
and 
xy 
E 
A} 
(c) 
{x 
13y 
lyl 
= 
log 
lxi 
and 
xy 
E 
A} 
**
Hs
3J. 
Let 
p 
be 
any 
polynomial 
of 
degree 
d 
with 
nonnegative 
integer 
cients. 
Show 
that 
if 
A 
is 
a 
regular 
set, 
then 
so 
is 
the 
set 
A' 
= 
{x 
13y 
lyl 
= 
p(lxl) 
and 
y 
E 
A}. 

_____________________________________________
324 
Miscellaneous 
Exercises 
···34. 
(Seiferas 
and 
McNaughton 
[113]) 
Let 
f 
: 
N 
-+ 
N 
be 
a 
function. 
CaU 
f 
regularity 
preserving 
if 
the 
set 
{x 
13y 
lyl 
= 
f(lxl) 
and 
xy 
E 
A} 
is 
regular 
whenever 
Ais. 
CaU 
f 
weakly 
regularity 
preserving 
if 
the 
set 
{x 
I 
3y 
lyl 
= 
f(lxl) 
and 
y 
E 
A} 
is 
regular 
whenever 
Ais. 
Prove 
that 
the 
following 
statements 
are 
equivalent: 
(a) 
f 
is 
regularity 
preservingj 
(b) 
f 
is 
weakly 
regularity 
preservingj 
(c) 
for 
any 
ultimately 
periodic 
set 
U 
N, 
the 
set 
f-
1
(U) 
= 
{m 
I 
f(m) 
E 
U} 
is 
also 
ultimately 
periodicj 
and 
(d) 
for 
any 
n 
E 
N, 
the 
set 
{m 
I 
f(m) 
= 
n} 
is 
ultimately 
periodic, 
and 
f 
is 
ultimately 
periodic 
mod 
p 
for 
any 
p 
2: 
1 
in 
the 
sense 
that 
00 
Vp 
2: 
1 
3q 
2: 
1 V 
n 
f(n) 
== 
f(n+ 
q) 
modp. 
00 
Here 
V 
means 
"for 
almost 
all" 
or 
"for 
all, 
but 
finitely 
many." 
Formally, 
00 
def 
V 
n 
cp(n) 
{::=} 
3m 
2: 
° 
Vn 
2: 
m 
cp(n). 
335. 
Show 
that 
the 
set 
{ww 
Iw 
E 
{O, 
1}*} 
is 
not 
regular. 
s36. 
Show 
that 
the 
set 
PRIMES 
{a
P 
I 
p 
is 
prime} 
is 
not 
regular. 
37. 
Which 
of 
the 
following 
sets 
are 
regular 
and 
which 
are 
not? 
Give 
fication. 
(a) 
{a
n
b
2m 
I 
n 
2: 
0 
and 
m 
2: 
O} 
(b) 
{anb
m 
In 
= 
2m} 
(c) 
{anb
m 
In 
# 
m} 
(d) 
{a
P
-
1 
I 
p 
is 
prime} 

_____________________________________________
( 
e) 
{xcx 
I 
X 
E 
{a, 
b} 
*} 
(f) 
{xcy 
I 
x,y 
E 
{a,b}*} 
(g) 
{a
n
b
n
+481 
I 
n 
O} 
(h) 
{anb
m 
In 
-m 
481} 
Finite 
Automata 
and 
Regular 
Sets 
325 
(i) 
{anb
m 
I 
n 
m 
and 
m 
481} 
(j) 
{anb
m 
I 
n 
m 
and 
m 
481} 
(k) 
L((a*b)*a*) 
(1) 
{anbnc
n 
I 
n 
O} 
(m) 
{syntactically 
correct 
PASCAL 
programs} 
38. 
For 
each 
of 
the 
following 
subsets 
of 
{O, 
I} 
*, 
tell 
whether 
or 
not 
it 
is 
regular. 
Give 
proof. 
(a) 
{x 
I 
#l(x) 
= 
2· 
#O(x)} 
(b) 
{x 
I 
#l(x) 
-
#O(x) 
< 
1O} 
(c) 
{x 
I 
#l(x). 
#O(x) 
is 
even} 
39. 
Prove 
that 
the 
set 
{anbmc
k 
I 
n, 
m, 
k 
0, 
n 
+ 
m 
= 
k} 
is 
not 
regular. 
40. 
Prove 
that 
the 
set 
{ailJi 
li 
is 
even 
or 
j 
< 
i} 
is 
not 
regular. 
41. 
Prove 
that 
no 
infinite 
subset 
of 
{anb
n 
I 
n 
O} 
is 
regular. 
··42. 
Give 
a 
set 
A 
<; 
{a, 
b} 
* 
such 
that 
neither 
A 
nor 
{a, 
b} 
* -
A 
contains 
an 
infinite 
regular 
subset. 
Prove 
that 
this 
is 
true 
of 
your 
set 
. 
.. 
H43. 
Give 
a 
nonregular 
set 
that 
satisfies 
condition 
(P) 
of 
the 
pumping 
lemma 
for 
regular 
sets 
(Lecture 
11); 
that 
is, 
such 
that 
the 
dem 
on 
has 
a 
winning 
strategy. 
Thus 
(P) 
is 
a 
necessary 
but 
not 
a 
sufficient 
condition 
for 
a 
set 
to 
be 
regular. 
44. 
Prove 
the 
following 
stronger 
versions 
of 
the 
pumping 
lemma 
for 
regular 
sets 
that 
give 
necessary 
and 
sufficient 
conditions 
for 
a 
set 
to 
be 
regular 
. 
Ł s 
(a) 
(Jaffe 
[62)) 
A 
set 
A 
<; 
is 
regular 
if 
and 
only 
if 
there 
exists 
k 
0 
such 
that 
for 
all 
y 
E 
with 
lyl 
= 
k, 
there 
exist 
u, 
v, 
w 
E 
such 
that 
y 
= 
UVW, 
v 
I-
•, 
and 
for 
all 
z 
E 
and 
i 
0, 
yz 
E 
A 
{::=:} 
uviwz 
E 
A. 

_____________________________________________
326 
Miscellaneous 
Exercises 
Ł 
(b) 
(Stanat 
and 
Weiss 
[117]) 
A 
set 
A 
is 
regular 
if 
and 
önly 
if 
there 
exists 
k 
0 
such 
that 
for 
all 
y 
E 
with 
lyl 
k, 
there 
exist 
u, 
v, 
w 
E 
such 
that 
y 
= 
uvw, 
v 
=I-
•, 
and 
for 
all 
x, 
z 
E 
and 
i 
0, 
xuz 
E 
A 
-<==? 
xuviz 
E 
A. 
45. 
Let 
A 
be 
any 
subset 
of 
{a} 
* 
whatsoever 
. 
Ł 
H(a) 
Show 
that 
A* 
is 
regular 
. 
.. 
8 
(b) 
Show 
that 
46. 
47. 
A 
* 
= 
{a 
np 
I 
n 
O} 
-
G, 
where 
G 
is 
some 
finite 
set 
and 
p 
is 
the 
greatest 
common 
divisor 
of 
all 
elements 
of 
the 
set 
{m 
I 
a
ffi 
E 
A}. 
This 
is 
a 
generalization 
of 
the 
so-called 
postage 
stamp 
problem: 
any 
amount 
of 
postage 
over 
7 
cents 
can 
be 
made 
with 
some 
combination 
of 
3 
and 
5 
cent 
stamps. 
Prove 
that 
the 
DFA 
with 
15 
states 
shown 
in 
Lecture 
5 
for 
the 
set 
(5.1) 
is 
minimal. 
Minimize 
the 
following 
DFAs. 
Indicate 
dearly 
which 
equivalence 
dass 
corresponds 
to 
each 
state 
of 
the 
new 
automaton. 
(a) 
(b) 
a 
b 
a 
b 
-+ 
1 
I 
6 
3 
-+ 
1 2 
3 
2 
5 
6 
2 
5 
6 
3F 
I 
4 
5 
3F 
1 
4 
4F 
3 
2 
4F 
6 
3 
5 2 1 
5 
2 
1 
6 
1 
4 
6 
5 
4 
(c) 
(d) 
a 
b 
a 
b 
-+ 
OF 
3 
2 
-+ 
0 
IF 
3 
5 
1 
2 4 
2 
2 
6 
2 
6 
3 
3 
2 1 
3 
6 6 
4 
5 
4 
4F 
0 
2 
5 
5 3 
5F 
1 
6 
6 
5 
0 
6 
2 
6 

_____________________________________________
Finite 
Automata 
and 
Regular 
Sets 
327 
(e) 
(f) 
a b 
a b 
--> 
0 3 
5 
--> 
0 
2 
5 
1 
6 
3 
1 
6 
2 
2 
6 
4 
2 
6 6 
3 
6 
6 
3 
6 
4 
4F 
0 
5 
4F 
5 
0 
5F 
2 
4 
5F 
4 
3 
6 
1 
6 
6 
1 
6 
(g) 
(h) 
a b 
a 
b 
--> 
1F 
I 
6 
4 
--> 
1 
6 
2 
2F 
7 
5 
2 
3 
6 
3 
2 
8 
3F 
2 4 
4 
1 
8 
4F 
5 
3 
5 
2 
6 
5 
4 1 
6 
3 
1 
6 
1 
5 
7 
5 
2 
7 
1 
8 
8 
4 2 
8 
8 
7 
48. 
Consider 
the 
DFA 
with 
states;Es 
= 
{O, 
1, 
2, 
3, 
4}, 
input 
alphabet 
{O, 
l}, 
start 
state 
0, 
final 
state 
0, 
and 
transition 
function 
8(q,i) 
= 
(q2 
-
i) 
mod 
5, 
q 
E 
;Es, 
i 
E 
{O, 
I}. 
Prove 
that 
this 
DFA 
accepts 
exactly 
the 
set 
of 
binary 
strings 
containing 
an 
even 
number 
of 
1 
's. 
HS 
49. 
Prove 
the 
correctness 
of 
the 
collapsing 
algorithm 
of 
Lecture 
14 
orem 
14.3). 
50. 
Let 
I; 
= 
{a, 
b}. 
For 
any 
x 
E 
I;*, 
define 
sufx 
= 
{ux 
I 
u 
E 
the 
set 
of 
strings 
ending 
with 
x. 
The 
set 
suf 
x 
is 
accepted 
by 
a 
deterministic 
finite 
automaton 
with 
lxi 
+ 
1 
states. 
For 
example, 
here 
is 
a 
nondeterministic 
finite 
automaton 
for 
suf 
abbaba: 
a,b 
Oa 
. 
b 
.. 
. 
b 
.. 
. 
a 
Ł 
Ł 
b 
.. 
. 
(a) 
Draw 
the 
minimal 
deterrninistic 
finite 
automaton 
for 
suf 
abbaba. 

_____________________________________________
328 
Miscellaneous 
Exercises 
uH 
(b) 
Argue 
that 
for 
any 
x, 
the 
minimal 
deterministic 
finite 
automaton 
for 
suf 
x 
has 
exactly 
lxi 
+ 
1 
states. 
uH51. 
(Greibach) 
Let 
M 
be 
an 
NFA, 
A 
= 
L(M). 
Starting 
with 
M, 
do 
the 
following: 
52. 
(a) 
reverse 
the 
transitions 
and 
interchange 
st.l\rt 
and 
final 
states 
to 
get 
an 
NFA 
for 
rev 
A; 
(b) 
determinize 
the 
resulting 
NFA 
by 
the 
subset 
construction, 
ting 
inaccessible 
states; 
( 
c) 
do 
the 
above 
two 
steps 
again. 
Prove 
that 
theresulting 
automaton 
is 
the 
minimal 
DFA 
for 
A. 
For 
each 
of 
the 
following 
finite 
automata: 
(i) 
Give 
an 
equivalent 
minimal 
deterministic 
finite 
automaton. 
Don't 
forget 
to 
remove 
inaccessible 
states. 
(ii) 
Give 
an 
equivalent 
regular 
expression. 
(a) 
(b) 
a 
b 
a 
b 
-+ 
IF 
2 
5 
-+ 
IF 
2 
6 
2F 
1 
4 
2F 
1 
7 
3 
7 
2 
3 
5 
2 
4 
5 7 
4 
2 
3 
5 
4 
3 
5 
3 
1 
6 3 6 
6 
7 
3 
7 
3 
1 
7 
6 
5 
(c) 
(d) 
a 
b 
a 
b 
-+ 
1 1 
3 
-+ 
IF 
I 
2 
5 
2F 
6 3 
2F 
1 
6 
3 
5 7 
3 
4 
3 
4F 
6 
1 
4 
7 
1 
5 
1 
7 
5 
6 
7 
6F 
2 
7 
6 
5 
4 
7 
5 
3 
7 
4 
2 

_____________________________________________
Finite 
Automata 
and 
Regular 
Sets 
329 
53. 
Consider 
the 
DFA 
with 
states 
Z5 
= 
{O, 
1,2,3, 
4}, 
input 
alphabet 
{O, 
1}, 
start 
state 
0, 
final 
state 
0, 
and 
transition 
function 
t5(q,i) 
= 
(q2 
+ 
i) 
mod 
5, 
q 
E 
Z5, 
i 
E 
{O, 
I}. 
Give 
an 
equivalent 
minimal 
DFA. 
54. 
Let 
== 
be 
any 
right 
congruence 
of 
finite 
index 
on 
Prove 
that 
any 
equivalence 
class 
of 
== 
is 
a 
regular 
subset 
of 
E* 
55. 
Consider 
the 
regular 
set 
R 
represented 
by 
the 
regular 
expression 
a*b* 
+ 
b*a*. 
(a) 
Draw 
the 
minimal 
DFA 
for 
R. 
H 
(b) 
Give 
a 
regular 
expression 
describing 
each 
of 
the 
equivalence 
classes 
of 
the 
Myhill-Nerode 
relation 
==R 
defined 
in 
Lecture 
16. 
56. 
Let 
PAREN 
be 
the 
set 
of 
balanced 
strings 
of 
parentheses 
[ 
]. 
Describe 
the 
equivalence 
classes 
of 
the 
relation 
==PAREN 
defined 
in 
Lecture 
16 
. 
.. 
857. 
For 
strings 
x 
and 
y 
over 
a 
finite 
alphabet 
define 
x!;;; 
y 
if 
x 
i8 
a 
(not 
necessarily 
contiguous) 
substring 
of 
y; 
that 
is, 
if 
x 
can 
be 
obtained 
from 
y 
by 
deleting 
zero 
or 
more 
letters. 
For 
example, 
abc 
!;;; 
ababac 
!;;; 
cabacbaac. 
A 
subset 
A 
is 
said 
to 
be 
closed 
downward 
under 
c: 
if 
x 
E 
A 
wheneve.r 
x 
y 
and 
y 
E 
A. 
Show 
that 
any 
subset 
of 
closed 
downward 
under 
is 
regular. 
You 
may 
use 
Higman's 
lemma: 
any 
subset 
of 
has 
a 
finite 
A 
!;;;-base 
of 
a 
set 
X 
is 
a 
subset 
X
o 
X 
such 
that 
for 
all 
y 
E 
X 
there 
exists 
an 
x 
E 
X
o 
such 
that 
x 
y. 
Higman's 
lemma 
is 
'equivalent 
to 
saying 
that 
the 
set 
of 
!;;;-minimal 
elements 
of 
any 
X 
is 
finite. 
You 
need 
not 
prove 
Higman's 
lemma. 
58. 
(Kapur) 
Allow 
a 
concise 
representation 
of 
strings 
by 
using 
exponents 
to 
denote 
repeated 
substrings. 
For 
example, 
Denote 
by 
/ 
x 
/ 
the 
length 
of 
the 
most 
concise 
representation 
of 
string 
x 
(exponents 
are 
given 
in 
binary). 
Let 
lxi 
denote 
the 
ordinary 
length 
of 
x. 
Let 
R 
be 
a 
regular set. 

_____________________________________________
330 
Miscellaneous 
Exercises 
*(a) 
Show 
that 
there 
exist 
constants 
c 
and 
d 
depending 
only 
on 
R 
such 
that 
for 
all 
x 
E 
R, 
there 
exists 
another 
string 
Y 
ERsuch 
that 
lyl 
-lxi 
d 
and 
/y/ 
clog 
lyl· 
** 
(b) 
Answer 
the 
same question 
with 
the 
condition 
lyl 
-
lxi 
d 
placed 
by 
the 
condition 
lyl 
= 
!x!-
.. 
*s 
59. 
Here 
is 
a 
generalization 
of 
nondeterminism. 
An 
alternating 
finite 
tomaton 
(AFA) 
is 
a 
5-tuple 
M 
= 
(Q, 
8, 
F,o:) 
where 
Q 
is 
a 
finite 
set 
of 
states, 
is 
a 
finite 
input 
alphabet, 
F 
: 
Q 
-> 
{O. 
I} 
is 
the 
characteristic 
function 
of 
a 
set 
of 
final 
that 
is, 
F(q) 
= { 
if 
q 
is 
a 
final 
state, 
otherwise, 
{) 
is 
the 
transition 
function 
8: 
(Q 
-+ 
((Q 
-+ 
{O,l}) 
-+ 
{O,l}), 
and 
0: 
is 
the 
acceptance 
condition 
0:: 
(Q 
-+ 
{O,l}) 
-+ 
{O,l}. 
Intuitively, 
.a 
computation 
of 
M 
generates 
a 
computation 
tree 
whose 
depth 
is 
the 
length 
of 
the 
input 
string. 
The 
function 
F 
gives 
a 
labeling 
of 
0 
or 
1 
to 
the 
leaves 
of 
this 
computation 
tree. 
For 
all 
q 
E 
Q 
and 
a 
E 
the 
Boolean 
function 
c5(q,a): 
(Q 
-+ 
{O,l}) 
-+ 
{O,l} 
takes 
a 
Boolean 
labeling 
on 
states 
at 
level 
i 
and 
computes 
a 
new 
beling 
at 
level 
i-I; 
this 
is 
used 
to 
pass 
Boolean 
labels 
0 
or 
1 
back 
up 
the 
computation 
tree. 
The 
machine 
accepts 
if 
the 
labeling 
at 
level 
0 
satisfies 
0:. 
An 
NFA 
is 
just 
an 
AFA 
in 
which 
the 
Boolean 
functions 
0: 
and 
6(q, 
a) 
compute 
thE: 
Boolean 
"or" 
of 
some 
subset 
of 
the 
inputs. 
Formally, 
the 
transition 
function 
6 
uniquely 
determines 
a 
map 
'8: 
(Q 
x 
E*) 
-+ 
((Q 
-+ 
{O, 
I}) 
-+ 
{O, 
I}), 
defined 
inductively 
as 
folIows: 
for 
q 
E 
Q, 
a 
E 
and 
x 
E 
E*, 
6(q,•)(u) 
= 
u(q), 
6(q,ax)(u) 
= 
6(q,a)(Ap.(6(p,x)(u))). 
(Here 
"Ap 
... 
" 
means 
"the 
function 
which 
on 
input 
p 
computes 
... 
;" 
see 
Lecture 
37.) 
The 
machine 
is 
said 
to 
accept 
xE 
E* 
if 
o:{ 
Ap. 
(6(p, 
x)( 
F))) 
= 
1. 

_____________________________________________
Finite 
Automata 
and 
Regular 
Sets 
331 
Prove 
that 
a 
set 
A 
E* 
is 
accepted 
by 
a 
k-state 
alternating 
finite 
automaton 
if 
and 
only 
if 
its 
reverse 
rev 
A 
is 
accepted 
by 
a 2
k
-state 
deterministic 
finite 
automaton. 
H60. 
'Show 
that 
minimal-state 
NFAs 
are 
not 
necessarily unique. 
61. 
(Vardi 
[122]) 
In 
this 
exercise 
we 
show 
that 
two-way 
nondeterministic 
finite 
automata 
accept 
only 
regular 
sets. 
Let 
M 
be 
a 
2NFA 
with 
states 
Q, 
start 
states 
S, 
accept 
state 
t, 
and 
transition 
function 
/l: 
Q 
x 
(E 
U 
{r, 
-1}) 
_ 
2
QX
({L,R}) 
Assurne 
without 
loss 
of 
generality 
that 
whenever 
M 
accepts, 
it 
moves 
its 
head 
aB 
the 
way 
to 
the 
right 
endmarker 
-1 
and 
enters 
its 
accept 
state 
t. 
(a) 
Let 
x 
= 
ala2···an 
E 
E*, 
ai 
E 
E, 
1 
i 
n. 
Let 
ao 
=r 
and 
an+l 
= 
-1. 
Argue 
that 
x 
is 
not 
accepted 
by 
M 
iff 
there 
exist 
sets 
Wi 
Q, 
0 
i 
n 
+ 
1, 
such 
that 
Ł S 
Wo; 
Ł 
if 
1
'
, 
E 
Wi, 
0 
i 
n, 
and 
(v,R) 
E 
/l(u,ai), 
then 
v 
E 
Wi+1; 
Ł 
ifu 
E 
Wi, 
1 
i 
n+l, 
and 
(v,L) 
E 
/l(u,ai), 
then 
v 
E 
Wi-l; 
and 
H(b) 
Using 
(a), 
show 
that 
rvL(M), 
hpnce 
L(M), 
is 
regular. 
62. 
Prove 
Lemma 
B.B. 
63. 
Prove 
that 
if 
abisimulation 
between 
two 
NFAs 
is 
a 
one-to-one 
spondence 
on 
the 
states, 
then 
it 
is 
an 
isomorphism. 
64. 
Prove 
that 
if 
NFAs 
M 
and 
N 
are 
bisimilar, 
then 
the 
relation 
(B.l) 
of 
Supplementary 
Lecture 
B 
gives 
abisimulation 
between 
the 
istic 
automata 
obtained 
from 
M 
and 
N 
by 
the 
subset 
construction. 
65. 
Prove 
that 
two 
DFAs 
are 
bisimilar 
if 
and 
only 
if 
they 
accept 
the 
same 
set. 
66. 
Prove 
Lemma 
C.l1. 
67. 
Prove 
Lemma 
D.2. 

_____________________________________________
332 
Miscellaneous 
Exercises 
68. 
Prove 
that 
the 
relation 
=A 
defined 
in 
the 
statement 
of 
the 
Nerode 
theorem 
for 
term 
automata 
(Theorem 
D.3) 
is 
a 
congruence 
on 
the 
term 
algebra 
TE(A). 

_____________________________________________
Miscellaneous 
Exercises 
Pushdown 
Automata 
and 
Context-Free 
Languages 
69. 
Consider 
the 
following 
context-free 
grammar 
G: 
S-ABSI 
AB, 
A-aAla, 
B-bA. 
Which 
of 
the 
following 
strings 
are 
in 
L( 
G) 
and 
which 
are 
not? 
Provide 
derivations 
for 
those 
that 
are 
in 
L( 
G) 
and 
reasons 
for 
those 
that 
are 
not. 
(a) 
aabaab 
(b) 
aaaaba 
(c) 
aabbaa 
(d) 
abaaba 
*H70. 
Consider 
the 
context-free 
grammar 
G 
with 
start 
symbol 
Sand 
ductions 
S 
-
aAB 
I 
aBA 
I 
bAAII 
E, 
A-
aS 
I 
bAAA, 
B 
-
aABB 
I 
aBAB 
I 
aBBA 
I 
bS. 

_____________________________________________
334 
Miscellaneous 
Exercises 
Prove 
that 
L( 
G) 
is 
the 
language 
consisting 
of 
a.ll 
words 
that 
have 
exa.ctly 
twice 
a.s 
many 
a's 
a.s 
b's. 
71. 
Give 
80 
grammar 
with 
no 
f-
or 
unit 
productions 
generating 
the 
set 
L(G) 
-
{•}, 
where 
G 
is 
the 
grammar 
S 
-+ 
aSbb 
I 
T, 
T 
-+ 
bTaa 
I 
S 
I 
•. 
72. 
Give 
grammars 
in 
Chomsky 
and 
Greibach 
normal 
form 
for 
the 
ing 
context-free 
languages. 
(80) 
{a
n
b
2n
c
/c 
I 
k,n 
1} 
(b) 
{anb/can 
I 
k,n 
1} 
(c) 
(d) 
{a, 
b} 
* -
{palindromes} 
73. 
Let 
E 
= 
{O, 
1}. 
Let 
x 
denote 
the 
Boolean 
complement 
of 
Xj 
that 
is, 
the 
string 
obtained 
from 
x 
by 
changing 
all 
O's 
to 
l's 
and 
l's 
to 
O's. 
Let 
rev 
x 
denote 
the 
reverse 
of 
Xj 
that 
is, 
the 
string 
x 
written 
backwards. 
Consider 
the 
set 
A 
= 
{x 
I 
rev 
x 
= 
x}. 
For 
instance, 
the 
strings 
011001 
and 
010101 
are 
in 
A 
but 
101101 
is 
not. 
(80) 
Give 
80 
CFG 
for 
this 
set. 
(b) 
Give 
grammars 
in 
Chomsky 
and 
Griebach 
normal 
form 
for 
A -
{f}. 
Ł 
74. 
Consider 
the 
set 
of 
a11 
strings 
over 
{a, 
b} 
with 
no 
more 
than 
twice 
80S 
many 
a's 
as 
b's: 
{x 
E 
{a,b}* 
I 
#a(x) 
2#b(x)}. 
(80) 
Give 
80 
CFG 
for 
this 
set, 
and 
prove 
that 
it 
is 
correct. 
(b) 
Give 
a 
pushdown 
automaton 
for 
this 
set. 
Specify 
cornpletely 
alJ 
data 
(states, 
transitions, 
etc.) 
and 
wh 
ether 
your 
rnachine 
accepts 
by 
final 
state 
or 
empty 
stack. 
Show 
sam 
pie 
runs 
on 
the 
input 
strings 
aabbaa, 
aaabbb, 
and 
aaabaa. 

_____________________________________________
Pushdown 
Automata 
and 
Context-Free 
Languages 
335 
75. 
Our 
definition 
of 
patterns 
and 
regular 
expressions 
in 
Lecture 
7 
was 
a 
little 
imprecise 
since 
it 
did 
not 
mention 
parentheses. 
Make 
this 
tion 
precise 
by 
using 
a 
CFG 
to 
specify 
the 
set 
of 
regular 
expressions 
over 
an 
alphabet 
The 
grammar 
you 
co 
me 
up 
with 
should 
have 
terminal 
symbols 
U 
{E, 
+, 
" 
(, 
), 
*}. 
76. 
Consider 
the 
set 
a*b*c* 
-
{anbnc
n 
In 
2: 
O}, 
the 
set 
of 
all 
strings 
of 
a's 
followed 
by 
b's 
followed 
by 
c's 
such 
that 
the 
number 
of 
a's, 
b's, 
and 
c's 
are 
not 
all 
equal. 
(a) 
Give 
a 
CFG 
for 
the 
set, 
and 
prove 
that 
your 
grammar 
is 
correct. 
(b) 
Give 
an 
equivalent 
PDA. 
08
77. 
What 
set 
is 
generated 
by 
the 
following 
grammar? 
S 
-+ 
bS 
I 
Sa 
I 
aSb 
I 
E 
Give 
proof. 
878. 
For 
A, 
B 
define 
AI 
B 
{x 
E 
13y 
E 
B 
xy 
E 
A}. 
Prove 
that 
if 
L 
is 
a 
CFL 
and 
R 
is 
a 
regular 
set, 
then 
LI 
R 
is 
CFL. 
H79. 
Show 
that 
the 
context-free 
languages 
are 
closed 
under 
homomorphic 
images 
and 
preimages. 
H80. 
(Ginsburg 
and 
Rice 
[45]) 
Show 
that 
any 
context-free 
subset 
of 
{a}* 
is 
regular. 
81. 
The 
specification 
of 
while 
programs 
at 
the 
beginning 
of 
tary 
Lecture 
I 
is 
rat 
her 
imprecise. 
Give 
a 
rigorous 
context-free 
cation. 
82. 
Prove 
that 
the 
set 
PRIMES 
{a
P 
I 
p 
is 
prime} 
is 
not 
context-free 
. 
.. 
H83. 
Show 
thai 
{a, 
b} 
* -
{a
n
b
n2 
In 
2: 
O} 
is 
not 
context-free. 

_____________________________________________
336 
Miscellaneous 
Exercises 
84. 
Which 
of 
the 
following 
sets 
are 
context-free 
and 
wh 
ich 
are 
not? 
Give 
grammars 
for 
those 
that 
are 
context-free 
and 
proof 
for 
those 
that 
are 
not. 
(a) 
{anbmcAtI 
n,m,k 
1 
and 
(2n 
= 
3/0 
or 
5k 
= 
7m)} 
(b) 
{anbmc
k 
I 
n,m,k 
1 
and 
(2n 
= 
3k 
and 
5k 
= 
7m)} 
(c) 
{anbmc
k 
I 
n,m,k 
1 
and 
(n", 
3m 
or 
n 
'" 
5k)} 
(d) 
{anbmc
k 
I 
n,m,k 
1 
and 
(n", 
3m 
and 
n", 
5k)} 
(e) 
{anbmc
k 
I 
n,m,k 
1 
and 
n+ 
k 
= 
m} 
(f) 
{ailJi 
ckd
l 
I 
i, 
j, 
k, 
l 
1, 
i 
= 
j, 
k 
= 
l} 
(g) 
{ailJickd
l 
I 
i,j,k,l 
1, 
i 
= 
k, 
j 
= 
l} 
(h) 
{ailJickdll 
i,j,k,l 
1, 
i 
= 
l, 
j 
= 
k} 
85. 
Say 
whether 
the 
following 
sets 
are 
(i) 
regular, 
(ii) 
context-free 
but 
not 
regular, 
or 
(iii) 
not 
context-free. 
Give 
justification. 
(a) 
{x 
E 
{a,b,c}* 
I 
#a(x) 
= 
#b(x) 
= 
#c(x)} 
(b) 
{ai 
I 
j 
is 
apower 
of 
2} 
(c) 
{x 
E 
{0,1}* 
I 
x 
represents 
apower 
of 
2 
in 
binary} 
(d) 
L(a*b*c*) 
(e) 
the 
set 
of 
all 
balanced 
strings 
of 
parentheses 
of 
three 
types, 
()[J{} 
(f) 
{anb
m 
In", 
m} 
(g) 
{anbmckd
l 
I 
2n 
= 
3k 
or 
5m 
= 
7l} 
(h) 
{anbmckd
l 
12n 
= 
3k 
and 
5m 
= 
7l} 
(i) 
{anbmckd
l 
12n 
= 
3m 
and 
5k 
= 
7l} 
(j) 
{anbmckdll2n 
= 
31 
and 
5k 
= 
7m} 
(k) 
{ailJic
k 
I 
i,j,k 
0 
and 
i 
> 
j 
and 
j 
> 
k} 
(1) 
{ailJick.1 
i,j,k 
0 
and 
(i 
> 
j 
or 
j 
> 
k)} 
(m) 
{x 
E 
{a,b}* 
I 
#a(x) 
> 
#b(x)} 
(n) 
5m+3n=24} 
(0) 
{amb
n 
I 
m,n 
0, 
5m 
3n 
= 
24} 

_____________________________________________
Pushdown 
Automata 
and 
Context-Free 
Languages 
337 
.. 
H86_ 
Give 
a 
non-context-free 
set 
that 
satisfies 
the 
condition 
of 
the 
pumping 
lemma 
for 
CFLs 
given 
in 
Lecture 
22; 
that 
is, 
such 
that 
the 
demon 
has 
a 
winning 
strategy_ 
87.-
Let 
be 
a 
finite 
alphabet. 
For 
a 
set 
A 
define 
cycle 
A 
= 
{yx 
1 
xy 
E 
A}, 
permute 
A 
= 
{y 
13x 
E 
A 
rta 
E 
#a(x) 
= 
#a(y)}. 
For 
example, 
if 
= 
{a, 
b, 
c} 
and 
A 
= 
{aaabc}, 
then 
cycle 
A 
= 
{aaabc, 
aabca, 
abcaa, 
bcaaa, 
caaab}
, 
permute 
A 
= 
{aaabc, 
aabca, abcaa, 
bcaaa, 
caaab, 
aabac,abaca,bacaa,acaab,caaba, 
abaac,baaca,aacab,acaba,cabaa, 
baaac, 
aaacb, 
aacba, 
acbaa, 
cbaaa}. 
Which of 
the 
following 
propositions 
are 
true 
and 
which 
are 
false? 
Give 
proof. 
(a) 
For 
aB 
A 
if 
Ais 
regular, 
then 
sö 
is 
cycle 
A. 
(b) 
For 
aB 
A 
if 
A 
is 
regular, 
then 
so 
is 
permute 
A . 
.. 
H 
(c) 
For 
aB 
A 
if 
A 
is 
context-free, 
then 
so 
is 
cycle 
A. 
(d) 
For 
aB 
A 
if 
A 
is 
context-free, 
then 
so 
is 
permute 
A. 
88. 
Recall 
t.he 
shufHe 
operator 
11 
from 
Homework 
4. 
(a) 
Show 
that 
if 
L 
is 
context·free 
and 
R 
is 
regular, 
then 
L 
11 
R 
is 
context-free. 
*(b) 
If 
L 
is 
a 
DCFL, 
is 
L 
11 
R 
necessarily 
a 
DCFL? 
Give 
proof. 
* 
89. 
For 
A, 
B 
define 
AI 
B 
{x 
13y 
E 
B 
xy 
E 
A} 
A 
-
B 
{x 
1 
rty 
E 
B 
xy 
E 
A}. 
;Exactly 
one 
of 
the 
following 
two 
statements 
is 
true. 
(a) 
If 
L 
is 
context-free, 
then 
so 
is 
(b) 
If 
L 
is 
context-free, 
then 
so 
is 
L 
_ 
Which 
is 
true? 
Give 
a 
proof 
and 
a 
counterexample. 
90. 
Let 
E 
= 
{a, 
b, 
c}. 
Exact1y 
one 
of 
the 
following 
four 
statements 
is 
true. 

_____________________________________________
338 
Miscellaneous 
Exercises 
(a) 
For 
any 
A 
if 
Ais 
regular, 
then 
so 
is 
{xx 
I 
x 
E 
A}. 
(b) 
For 
any 
A 
1;*, 
if 
A 
is 
regular, 
then 
so 
is 
{x 
I 
xx 
E 
A}. 
(c) 
For 
any 
A 
1;*, 
if 
Ais 
context-free, 
then 
so 
is 
{xx 
I 
x 
E 
A} 
. 
Ł 
H(d) 
For 
any 
A 
1;*, 
if 
Ais 
context-free, 
then 
so 
is 
{x 
I 
xx 
E.A}. 
Wh 
ich 
is 
true? 
Give 
a 
proof 
and 
three 
counten>xamples. 
91. 
Using 
the 
grammar 
S 
-+ 
AB, 
A 
-+ 
a, 
B 
-+ 
AB 
I 
b, 
run 
the 
CKY 
algorithm 
on 
the 
string 
aab. 
Draw 
a 
table 
like 
the 
following 
one 
and 
fill 
it 
in 
completely. 
o 
92. 
(a) 
Modify 
the 
CKY 
algorithm 
to 
count 
the 
riumber 
of 
parse 
trees 
of 
a 
given 
string 
and 
to 
construct 
one 
if 
the 
number 
is 
nonzero. 
(b) 
Test 
your 
algorithm 
of 
part 
(a) 
on 
the 
grammar 
S 
-+ 
ST 
I 
a, 
T 
-+ 
BS, 
B-++ 
and 
string 
a+a+a+ 
a. 
(Sanity 
check: 
the 
string 
has 
five 
parse 
trees.) 
... 
H93. 
Let 
D 
1;* 
be 
a 
DCFL. 
One 
of 
the 
following 
sets 
is 
always 
a 
DCFL, 
the 
other 
is 
not 
necessarily. 
Which 
is 
which? 
Give 
proof 
for 
both. 
(a) 
{x 
13a 
E 
1; 
xa 
E 
D} 
(b) 
{x 
13a 
E 
1; 
ax 
E 
D} 
Conclude 
that 
the 
family 
of 
DCFLs 
is 
not 
closed 
under 
reversal. 

_____________________________________________
Pushdown 
Automata 
and 
Context-Free 
Languages 
339 
94. 
Let 
be 
a 
fixed 
finite 
signature 
as 
described 
in 
Supplementary 
Lecture 
C. 
Give 
an 
unambiguous 
context-free 
grammar 
for 
the 
set 
of 
ground 
terms 
Unambiguous 
means 
that 
there 
is 
exactly 
one 
parse 
tree 
for 
each 
ground 
term. 
Prove 
that 
your 
grammar 
is 
correct 
and 
that 
it 
i-s 
unambiguous. 
··95. 
The 
context-free 
language 
{nnb
n 
I 
n 
2: 
O} 
is 
the 
unique 
-minimal 
solution 
of 
the 
equation 
x 
= 
aXb+ 
E. 
In 
general, 
let 
be 
a 
finite 
alphabet. 
Consider 
finite 
systems 
of 
tions 
of 
the 
form 
where 
the 
Xi 
are 
variables 
ranging 
over 
subsets 
of 
and 
the 
t:; 
are 
regular 
expressions 
over 
Xl, 
... 
,X
n 
ana 
(a) 
Argue 
that 
any 
such 
system 
has 
a 
unique 
minimal solution 
You 
may 
want 
to 
use 
the 
Knaster-Tarski 
theorem: 
any 
monotone 
map 
on 
a 
complete 
partial 
order 
has 
a 
unique 
least 
fixpoint. 
A 
map 
f 
on 
a 
partially 
ordered 
set 
is 
monotone 
if 
x 
:::; 
y 
-> 
f 
(x) 
:::; 
f(y)· 
A 
partially 
ordered 
set 
is 
complete 
if 
every 
subset 
of 
that 
set 
has 
aleast 
upper 
bound. 
(b) 
For 
the 
Xl, 
.
.. 
,X
n 
of 
part 
(a), 
show 
that 
Xl 
is 
a 
context-free 
language. 
(c) 
Show 
that 
all 
context-free 
languages 
arise 
in 
this 
way. 

_____________________________________________
Miscellaneous 
Exercises 
Turing 
Machines 
and 
Effective 
Computability 
96. 
Give 
a 
·ruring 
machine 
with 
input 
alphabet 
{a} 
that 
on 
input 
a
m 
halts 
with 
a
m2 
written 
on 
its 
tape. 
Describe 
the 
operation 
of 
the 
machine 
both 
informally 
and 
formally. 
Be 
sure 
to 
specify 
all 
data. 
97. 
The 
Eudidean 
algorithm 
computes 
the 
greatest 
common 
divisor 
(GCD) 
of 
two 
nonnegative 
integers: 
procedure 
gcd(m,n): 
if 
n 
= 
0 
then 
return(m) 
else 
return(gcd(n,m 
mod 
n)) 
Give 
a 
Turing 
machine 
with 
input 
alphabet 
{a, 
#} 
that 
on 
input 
am#a
n 
halts 
with 
agcd(m,n) 
written 
on 
its 
tape. 
Describe 
the 
ation 
of 
the 
machin"e 
both 
informally 
and 
formally. 
Be 
sure 
to 
specify 
all 
data. 
98. 
Prove 
that 
the 
dass 
of 
r.e. 
sets 
is 
dosed 
under 
union 
and 
intersection. 
99. 
A 
queue 
machine 
is 
like 
a 
Turing 
machine, 
except 
that 
it 
has 
a 
queue 
instead 
of 
a 
tape. 
It 
has 
a 
finite 
queue 
alphabet 
rand 
a 
finite 
input 
alphabet 
E 
f. 
If 
x 
E 
E* 
is 
the 
input, 
the 
machine 
starts 
in 
its 
start 
state 
s 
with 
x$ 
in 
the 
queue, 
where 
$ 
is 
a 
special 
symbol 
in 
r -
L 
In 
each 
step, 
it 
removes 
a 
symbol 
from 
the 
front 
of 
the 
queue. 
Based 
on 

_____________________________________________
T 
uring 
Machines 
and 
Effective 
Computability 
341 
that 
symbol 
and 
the 
current 
state, 
it 
pushes 
astring 
Z 
E 
r* 
onto 
the 
back 
of 
the 
queue 
and 
enters 
a 
new 
state 
according 
to 
the 
transition 
function 
6. 
It 
accepts 
by 
emptying 
its 
queue. 
S(a) 
Give 
a 
rigorous 
form,al 
definition 
of 
these 
machines, 
including 
adefinition 
of 
configurations 
and 
acceptance. 
Your 
definition 
should 
begin 
as 
follows: 
"A 
queue 
machine 
is 
a 
sextuple 
M 
= 
(Q, 
r, 
$,6, 
s), 
where 
... 
" 
... 
HS 
(b) 
Prove 
that 
queue 
machines 
and 
Turing 
machines 
are 
equivalent 
in 
power. 
100. 
A 
one-counter 
automaton 
is 
an 
automaton 
with 
a 
finite 
set 
of 
states 
Q, 
a 
two-way 
read-only 
input 
head, 
and 
a 
separate 
counter 
that 
can 
hold 
any 
nonnegative 
integer. 
The 
input 
x 
E 
is 
enclosed 
in 
endmarkers 
f-, 
--f 
rf. 
and 
the 
input 
head 
may 
not 
go 
outside 
the 
endmarkers. 
The 
machine 
starts 
in 
its 
start 
state 
s 
with 
its 
counter 
empty 
and 
with 
its 
input 
head 
pointing 
to 
the 
left 
endmarker 
f-. 
In 
each 
step, 
it 
can 
test 
its 
counter 
for 
zero. 
Based 
on 
this 
information, 
its 
current 
state, 
and 
the 
symbol 
its 
input 
head 
is 
currently 
reading, 
it 
can 
either 
add 
one 
to 
its 
counter 
or 
subtract 
one, 
move 
its 
input 
head 
either 
left 
or 
right, 
and 
enter 
a 
new 
state. 
It 
accepts 
by 
entering 
a 
distinguished 
final 
state 
t. 
(a) 
Give 
a 
rigorous 
fürmal 
definition 
of 
these 
machines, 
iricluding 
a 
definition 
of 
acceptance. 
Your 
definition 
should 
begin 
as 
follows: 
"A 
one-counter 
automaton 
is 
a 
septuple 
M 
= 
(Q, 
f-, 
-1, 
s, 
t, 
8), 
where 
... 
" 
Ł 
(b) 
Prove 
that 
the 
membership 
problem 
for 
deterministic 
one-counter 
automata 
is 
decidable: 
given 
M 
and 
x, 
does 
M 
accept 
x? 
H 
(c) 
Prove 
that 
the 
emptiness 
problem 
is 
undecidable: 
given 
a 
counter 
automaton 
M, 
is 
L(M) 
= 
0? 
101. 
A 
ray 
automaton 
consists 
of 
an 
infinite 
number 
of 
deterministic 
finite 
automata 
A
o
, 
Al, 
A
2
, 
Ł.. 
arranged 
in 
a 
line. 
The 
automata 
all 
have 
the 
same 
set 
of 
states 
Q, 
the 
same 
start 
state 
s, 
and 
the 
same 
transition 
function 
8 
except 
A
o
, 
w 
hich 
has 
a 
different 
transition 
function 
80 
since 
it 
has 
no 
left 
neighbor. 
They 
all 
start 
simultaneously 
in 
their 
initial 
state 
sand 
execute 
synchronously. 
In 
each 
step, 
each 
Ai 
moves 
to 
a 
new 
state, 
depending 
on 
its 
own 
current 
state 
and 
the 
current 
states 
of 
its 

_____________________________________________
342 
Miscellaneous 
Exercises 
immediate 
left 
and 
right 
neighbors, 
according 
to 
its 
transition 
function. 
The 
ray 
automaton 
is 
said 
to 
halt 
if 
A
o 
ever 
enters 
a 
distinguished 
final 
state 
t. 
There 
is 
no 
input 
alphabet. 
(a) 
Give 
a 
rigorous 
formal 
definition 
oI' 
ray 
automata, 
including 
a 
definition 
of 
execution 
and 
halting. 
Your 
definition 
should 
begin 
as 
follows: 
"A 
ray 
automaton 
is 
a 
quintuple 
A 
= 
(Q, 
s, 
t, 
00, 
0), 
where 
Q 
is 
a 
finite 
set 
of 
states, 
... 
" 
(b) 
Prove 
that 
the 
halting 
problem 
for 
ray 
automata 
is 
undecidable. 
(c) 
Is 
the 
halting 
problem 
for 
ray 
automata 
semidecidable? 
Why 
or 
why 
not? 
102. 
A 
deterministic 
two-dimensional 
Turing 
machine 
is 
like 
a 
Turing 
chine 
except 
that 
instead 
of 
a 
one-dimensional 
tape 
it 
has 
a 
dimensional 
tape 
that 
is 
like 
a 
chessboard, 
infinite 
in 
all 
directions. 
It 
has 
a 
finite 
input 
alphabet 
1; 
and 
a 
finite 
tape 
alphabet 
r 
ing 
1; 
as 
a 
subset. 
If 
x 
E 
1;* 
is 
the 
input, 
lxi 
= 
n, 
the 
machine 
starts 
in 
its 
start 
state 
s 
with 
x 
written 
in 
tape 
cells 
(0,1), (0,2), 
... 
, 
(0, 
n), 
the 
origin 
(0,0) 
containing 
a 
special 
symbol 
0 
E 
r -
1;, 
and 
all 
other 
cells 
(i,j) 
containing 
a 
special 
blank 
symbol 
u 
E 
r -
1;. 
It 
has a 
read/write 
head 
initially 
pointing 
to 
the 
origin. 
In 
each 
step, 
it 
reads 
thesymbol 
of 
r 
currently 
occupying 
the 
cell 
it 
is 
scanning. 
Depending 
on 
that 
symbol 
and 
the 
current 
state 
of 
the 
finite 
control, 
it 
writes 
a 
symbol 
of 
r 
on 
that 
ceU, 
moves 
one 
ceU 
either 
north, 
south, 
east, 
or 
west, 
and 
enters 
a 
new 
state, 
according 
to 
its 
transition 
function 
O. 
It 
accepts 
its 
input 
by 
erasing 
the 
entire 
board; 
that 
is, 
filling 
all cells 
with 
u. 
H(a) 
Give 
a 
rigorous 
formal 
definition 
of 
these 
machines, 
including 
adefinition 
of 
configurations, 
the 
next 
configuration 
relation, 
and 
acceptance. 
Try 
to 
be 
as 
precise 
as 
possible. 
Your 
definition 
should 
begin 
as 
follows: 
"A 
two-dimensional 
Turing 
machine 
is 
a 
7-tuple 
M 
= 
(Q, 
1;, 
r, 
u, 
0, 
s, 
0), 
where 
Q 
is 
a 
finite 
set 
of 
states, 
... 
" 
(b) 
Argue 
that 
two-dimensional 
Turing 
machines 
and 
ordinary 
ing 
machines 
are 
equivalent 
in 
the 
sense 
that 
each 
can 
simulate 
the 
other. 
Describe 
thc 
simulations 
informally 
(Le.,· 
no 
tions) 
but 
in 
sufficient 
detail 
that 
transitions 
implementing 
your 
description 
could 
readily 
be 
written 
down. 

_____________________________________________
T 
uring 
Machines 
and 
Effective 
Computability 
343 
103. 
A 
nondeterministic 
Turing 
machine 
is 
one 
with 
a 
multiple-valued 
sition 
relation. 
Give 
a 
formal 
definition 
of 
these 
machines. 
Argue 
that 
every 
nondeterministic 
TM 
can 
be 
simulated 
by 
a 
deterministic 
TM. 
104. 
Show 
that 
the 
type 
0 
grammars 
(see 
Lecture 
36) 
generate 
exactly 
the 
r.e. 
sets. 
105. 
For 
A, 
B 
define 
AI 
B 
{x 
E 
I 
3y 
E 
B 
xy 
E 
A}. 
(a) 
Show 
that 
if 
A 
and 
Bare 
r.e., 
then 
so 
is 
AlB 
. 
Ł 
H(b) 
Show 
that 
every 
r.e. 
set 
can 
be 
represented 
as 
AlB 
with 
A 
and 
B 
CFLs. 
106. 
Is 
it 
decidable, 
given 
M 
#y, 
whether 
the 
Turing 
machine 
M 
ever 
writes 
a 
nonblank 
symbol 
on 
its 
tape 
on 
input 
y? 
Why 
or 
why 
not? 
107. 
Is 
it 
decidable 
for 
TMs 
M 
whether 
L(M) 
= 
rev 
LUv!)? 
Give 
proof. 
108. 
Tell 
whether 
the 
following 
problems 
are 
decidable 
or 
undecidable. 
Give 
proof. 
(a) 
Given 
a 
TM 
.\1 
and 
astring 
y, 
does 
M 
ever 
write 
the 
symbol 
# 
on 
its 
tape 
on 
input 
y? 
(b) 
Given 
a 
CFG 
C, 
does 
C 
generate 
all 
strings 
except 
f? 
(c) 
Given 
an 
LBA 
1'v[, 
does 
M 
accept 
astring 
of 
even 
(d) 
Given 
a 
TM 
M, 
are 
there 
infinitely 
many 
TMs 
equivalent 
to 
M? 
109. 
Tell 
whether 
or 
not 
the 
following 
sets 
are 
r.e. 
Give 
proof. 
(.'1) 
{(Al, 
N) 
I 
M 
takes 
[ewer 
steps 
than 
N 
on 
input 
t} 
(0) 
{M 
I 
AI 
takes 
[ewer 
than 
181
481 
steps 
on 
some 
input} 
(c) 
{M 
I 
Al 
takes 
fewer 
than181
481 
steps 
on 
at 
least 
481
481 
different 
inpllts} 
(d) 
{M 
I 
M 
takes 
fewer 
than 
181
181 
steps 
on 
all 
inputs} 
110. 
Show 
that 
the 
set 
{NI 
I 
AI 
accepts 
at 
least 
481 
strings} 
is 
r.e. 
but 
not 
co-r.e. 

_____________________________________________
344 
Miscellaneous 
Exercises 
111. 
One 
of 
the 
following 
sets 
is 
r.e. 
and 
the 
other 
is 
not. 
Which 
is 
which? 
Give 
proof 
for 
both. 
(a) 
{M 
I 
L(M) 
contains 
at 
least 
481 
elements} 
(b) 
{M 
I 
L(M) 
contains 
at 
most 
481 
elements} 
112. 
Show 
that 
the 
set 
{M 
I 
M 
halts 
on 
all 
inputs 
of 
length 
less 
than 
481} 
is 
r.e., 
but 
its 
complement 
is 
not. 
s113. 
Let 
M 
range 
over 
Turing 
machine 
descriptions. 
Show 
that 
neither 
the 
set 
REG 
{M 
I 
L(M) 
is 
a 
regular 
set} 
nor 
its 
complement 
is 
recursively 
enumerable. 
114. 
Let 
IMI 
denote 
the 
length 
of 
the 
description 
of 
the 
Turing 
machine 
M. 
Are 
the 
following 
problems 
decidable? 
Give 
proof. 
(a) 
Does 
a 
given 
Turing 
machine 
M 
take 
at 
least 
I 
steps 
on 
some 
input? 
(b) 
... 
on 
all 
inputs? 
115. 
Tell 
whether 
the 
following 
problems 
are 
decidable 
or 
undecictable, 
and 
give 
proof: 
(a) 
whether 
a 
given 
TM 
runs 
for 
at 
least 
481
481 
steps 
on 
input 
a
481
: 
(b) 
whether 
a 
given 
TM 
ever 
reenters 
its 
start 
state 
on 
any 
input; 
"( 
c) 
whether 
a 
given 
Turing 
machine 
will 
ever 
move 
its 
head 
left 
more 
than 
ten 
times 
on 
input 
a
481
; 
"(d) 
whether 
a 
given 
Turing 
machine 
will 
ever 
print 
more 
than 
481 
nonblank 
symbols 
on 
input 
a
481 
116. 
Think 
for 
twq 
minutes 
about 
why 
the 
following 
problems 
are 
able, 
but 
don't 
write 
anything 
down: 
(a) 
whether 
two 
given 
C++ 
programs compute 
the 
same 
function; 
(b) 
whether 
a 
given 
C++ 
program 
will 
ever 
get 
into 
an 
infinite 
loop 
on 
some 
input; 

_____________________________________________
Turing 
Machines 
and 
Effective 
Computability 
345 
(c) 
whether 
a 
given 
JL-recursive 
function 
is 
total; 
(d) 
whether 
a 
given 
'x-term 
reduces 
to 
normal 
form. 
117. 
Show 
that 
the 
following 
problems 
of 
pairs 
of 
Turing 
machines 
are 
undecidable: 
(a) 
whether 
L(M) 
= 
L(N); 
(b) 
whether 
L(M) 
L(N); 
(c) 
whether 
L(M) 
n 
L(N) 
= 
0; 
(d) 
whether 
L(.M) 
n 
L(N) 
is 
a 
recursive 
set; 
(e) 
whether 
L(M) 
n 
L(N) 
is 
finite. 
··118. 
,Forr.1.alize 
and 
prove 
the 
following 
extension 
ofRice's 
theorem 
that 
has 
the 
results 
of 
Exercise 
117 
as 
special 
cases: 
every 
nontrivial 
property 
of 
pairs 
of 
r.e. 
sets 
is 
undecidable. 
119. 
Let 
G 
and 
G' 
denote 
context-free 
grammars 
over 
{a, 
b}. 
Prove 
that 
the 
following 
problems 
are 
undecidable: 
H(a) 
whether 
L(G) 
= 
L(G'); 
(b) 
whether 
L(G) 
L(G'); 
'(cl 
whether 
L(G) 
= 
L(G)L(G). 
120. 
One 
of 
the 
following 
problems 
is 
decidable 
and 
the 
other 
is 
not. 
Which 
is 
which? 
Give 
proof 
for 
both. 
(a) 
Given 
a 
CFL 
Land 
a 
regular 
set 
R, 
is 
L 
R? 
(b) 
Given 
a 
CFL 
Land 
a 
regular 
set 
R, 
is 
R 
L? 
H 
121. 
Prove 
that 
the 
following 
problems 
are 
undecidable: 
(a) 
whether 
a 
given 
CFL 
is 
a 
DCFL; 
(b) 
whether 
the 
intersection 
of 
two 
given 
CFLs 
is 
a 
CFL; 
(c) 
whether 
the 
complement 
of 
a 
given 
CFL 
is 
a 
CFL; 
··(d) 
whether 
the 
union 
of 
two 
given 
DCFLs 
is 
a 
DCFL. 
122. 
Prove 
that 
it 
is 
undecidable 
whether 
a 
given 
LBA 
halts 
on 
all 
inputs. 

_____________________________________________
346 
Miscellaneous 
Exercises 
H123. 
Show 
that 
the 
finiteness 
problem 
for 
Turing 
machines 
reduces 
to 
the 
finiteness 
problem 
for 
LBAs. 
124. 
Prove 
that 
it 
is 
undecidable 
whether 
a 
given 
LBA 
accepts 
a 
regular 
set. 
125. 
Consider 
the 
followhg 
context-sensitive 
production::;. 
S 
----
bSb, 
S 
----
AcA, 
Ab 
----
A, 
Ab 
----
b, 
bA 
----
b, 
bA 
----
A. 
Let 
G 
be 
the 
grammar 
given 
by 
all 
the 
rules 
except 
for 
the 
last, 
and 
let 
G' 
the 
grammar 
given 
by 
all 
the 
rules 
including 
the 
last. 
One 
of 
L( 
G) 
and 
L( 
G') 
is 
regular, 
and 
the 
other 
is 
context-free 
but 
not 
regular. 
Wh 
ich 
is 
which, 
and 
why? 
126. 
Give 
a 
set 
over 
a 
single 
letter 
alphabet 
in 
each 
of 
the 
following 
classes, 
or 
explain 
why 
such 
a 
set 
does 
not 
exist: 
(a) 
regular; 
(b) 
DCFL 
but 
not 
regular; 
(c) 
CFL 
but 
not 
DCFL; 
(d) 
recursive 
but 
not 
CFL; 
(e) 
r.e. 
but 
not 
recursive; 
(f) 
not 
r.e. 
127. 
Prove 
that 
every 
infinite 
regular 
set 
contains 
a 
non-r.e. 
subset. 
oH 
128. 
Prove 
that 
every 
infinite 
r.e. 
set 
contains 
an 
infinite 
recursive 
subset. 
0129. 
In 
this 
exercise 
we 
will 
prove 
a 
kind 
of 
fixpoint 
theorem 
for 
Turing 
machines 
known 
as 
the 
recursion 
theorem. 
If 
M 
is 
a 
TM, 
let 
M 
(x) 
denote 
the 
contents 
of 
M's 
tape 
at 
the 
point 
that 
M 
halts 
on 
input 
x, 
provided 
M 
does 
indeed 
halt 
on 
input 
x. 
If 
M 
does 
not 
halt 
on 
input 
x, 
then 
M(x) 
is 
undefined. 

_____________________________________________
T 
uring 
Machines 
and 
Effective 
(omputability 
347 
A 
partial 
function 
u : 
I;* 
--+ 
I;* 
is 
said 
to 
be 
a 
computable 
function 
if 
u(x) 
= 
M(x) 
for 
some 
Turing 
machine 
M. 
In 
addition, 
u 
is 
a 
total 
computable 
function 
if 
M 
is 
total. 
Let 
M", 
be 
the 
TM 
whose 
encoding 
over 
.. 
is 
x. 
Theorem 
(Recursion 
theorem) 
Let 
u : 
I;* 
--+ 
I;* 
-be 
any 
total 
computable 
function. 
Then 
there 
exists 
astring 
u 
such 
that 
L(M
u
) 
= 
L(M<T(u))' 
(a) 
Let 
u : 
I;* 
--+ 
I;* 
be 
a 
given 
total 
computable 
function, 
say 
computable 
by 
a 
total 
TM 
K. 
Let 
N 
be 
a 
TM 
that 
on 
input 
X 
computes 
a 
description 
of 
a 
machine 
that 
does 
the 
following 
on 
input 
y: 
Ł 
constructs 
M",; 
Ł 
runs 
M", 
on 
input 
X; 
Ł 
if 
it 
halts, 
runs 
K 
on 
M",(x); 
Ł 
interprets 
the 
result 
of 
that 
computation, 
K(M",(x)), 
as 
the 
description 
of 
a 
TM, 
and 
simulates 
that 
TM 
on 
the 
original 
input 
y, 
accepting 
or 
rejecting 
as 
that 
machine 
accepts 
or 
rejects, 
respectively. 
Argue 
that 
N 
is 
total 
and 
that 
L(M
N
(",)) 
= 
L(M<T(M.(",)))' 
(b) 
Let 
v 
be 
a 
description 
ofthe 
machine 
N; 
that 
is, 
N 
= 
Mv. 
Argue 
that 
N( 
v) 
is 
the 
desired 
fixpoint 
of 
u. 
H130. 
Give 
a 
short 
proof 
of 
Rice's 
theorem 
using 
the 
recursion 
theorem 
(see 
Miscellaneous 
Exercise 
129) 
. 
.. 
H 
131. 
A 
TM 
is 
minimal 
if 
it 
has 
the 
fewest 
states 
among 
all 
TMs 
that 
accept 
the 
same 
set. 
Prove 
that 
there 
does 
not 
exist 
an 
infinite 
r.e. 
set 
of 
minimal 
TMs. 
132. 
H(a) 
Show 
that 
there 
does 
not 
exist 
an 
r.e. 
list 
of 
Turing 
machines 
such 
that 
every 
machine 
on 
the 
list 
is 
total 
(i.e., 
halts 
on 
all 
inputs) 
and 
every 
recursive 
set 
is 
represented 
by 
some 
machine 
on 
the 
list. 
UH(b) 
Show 
that 
there 
exists 
an 
r.e. 
list 
of 
Turing 
machines 
such 
that 
every 
machine 
on 
the 
list 
accepts 
a 
recursive 
set 
and 
every 
cursive 
set 
is 
represented 
by 
some 
machine 
on 
the 
list. 

_____________________________________________
348 
Miscellaneous 
Exercises 
"133. 
In 
addition 
to 
the 
usual 
constructs 
of 
while 
programs 
(simple 
ment, 
conditional, 
while 
loop, 
sequential 
composition), 
add 
a 
print 
statement 
print 
x 
and 
halt 
that 
prints 
the 
current 
value 
of 
a 
variable 
and 
halts. 
Call 
two 
programs 
equivalent 
if 
for 
all 
initial 
values 
of 
the 
variables, 
one 
program 
halts 
iff 
the 
other 
does, 
and 
whenever 
they 
both 
halt, 
they 
print 
the 
same 
value. 
One 
of 
the 
following 
problems 
js 
decidable 
and 
the 
other 
is 
undecidable. 
Which 
is 
which? 
Justify 
your 
answers. 
(a) 
Given 
a 
program, 
does 
therEl 
exist 
an 
equivalent 
program 
with 
at 
most 
one 
while 
lpop? 
(b) 
Given 
a 
program, 
does 
there 
exist 
an 
equivalent 
program 
with 
no 
while 
loops? 
134. 
This 
question 
is 
for 
those 
who 
know 
something 
about 
propositional 
logic. 
A 
propositional 
Horn 
clause 
is 
a 
disjnnction 
of 
literals 
with 
at 
most 
one 
positive 
literal. 
The 
dause 
-'P
I 
V 
"'P
2 
V 
... 
V 
"'P
n 
V 
Q 
is 
often 
writtenas 
PI 
/\ 
P
2 
/\ 
ŁŁŁ 
/\ 
P
n 
-+ 
Q, 
and 
the 
dause 
can 
be 
written 
as 
PI 
/\ 
P
2 
/\ 
.ŁŁ 
/\ 
P
n 
-+ 
.L, 
where 
.l 
denotes 
falsity. 
Any 
single 
positive 
literal 
Q 
is 
also 
aHorn 
dause. 
(a) 
Show 
that 
the 
emptiness 
problem 
for 
context-free 
languages 
(i.e., 
given 
a 
context-free 
grammar 
G, 
deciding 
whether 
L(G) 
= 
0) 
reduces 
to 
the 
satisfiability 
problem 
for 
finite 
conjunctions 
of 
Horn 
clauses, 
and 
vice 
versa. 
(b) 
Since 
the 
satisfiability 
of 
propositional 
formulas 
is 
decidable, 
what 
can 
we 
conclude 
about 
the 
decidability 
of 
the 
emptiness 
problem 
for 
CFLs? 

_____________________________________________
Turing 
Machines 
and 
Effective 
Computability 
349 
H 
135. 
Show 
that 
the 
finiteness 
problem 
for 
regular 
sets 
and 
context-free 
guages 
(i.e., 
whether 
a 
given 
machinejgrammar 
acceptsjgenerates 
a 
finite 
set) 
is 
decidable. 
136. 
Show 
that 
FIN 
ST 
REG. 
In 
other 
words, 
su·ppose 
you 
are 
given 
an 
orade 
that 
will 
always 
answer 
questions 
of 
the 
form 
"Is 
L(M) 
a 
regular 
set?" 
truthfully. 
Show 
how 
to 
use 
such 
an 
orade 
to 
decide 
questions 
of 
the 
form 
"Is 
L(M) 
finite?" 
**137. 
Prove 
Theorem 
J.1. 
.H138. 
Let 
HP
1 
HP, 
and 
let 
HP
n
+
1 
be 
the 
halting 
problem 
for 
oracle 
Turing 
machines 
with 
oracle 
HP 
n, 
n 
:::: 
1; 
that 
is, 
HP
n
+
1 
{M#x 
I 
M 
is 
an 
orade 
TM 
with 
oracle 
HP
n
, 
M 
halts 
on 
input 
x}. 
The 
orade 
need 
not 
be 
represented 
in 
the 
description 
of 
the 
oracle 
machine 
M. 
Show 
that 
HP
n 
E 
-
139. 
Show 
that 
t.he 
integer 
square 
root 
function 
is 
primitive 
recursive. 
On 
input 
n, 
the 
function 
should 
return 
the 
greatest 
integer 
less 
than 
or 
equal 
to 
the 
square 
root 
of 
n. 
H 
140. 
A 
language 
B 
is 
said 
to 
be 
computable 
in 
linear 
time 
if 
there 
exists 
a 
deterministic 
Turing 
machine 
M 
and 
a 
constant 
c 
> 
0 
such 
that 
L(M) 
= 
B 
anel 
M 
always 
halts 
within 
cn 
steps 
on 
inputs 
of 
length 
n. 
Show 
that 
there 
exists 
a 
recursive 
set 
that 
is 
not 
computable 
in 
linear 
time. 
141. 
Show 
that 
the 
Turing 
reducibility 
relation 
ST 
is 
reflexive 
and 
transitive 
and 
that 
Sm 
refines 
ST. 
142. 
Prove 
that 
the 
following 
sets are 
Sm-complete 
for 
the 
given 
dasses: 
(a) 
EMPTY 
is 
Sm-complete 
for 
m; 
"(b) 
TOTAL 
is 
Sm-complete 
for 
rrg; 
""(c) 
COF 
is 
Sm-complei.e 
for 
""(d) 
the 
set 
REG 
{M 
I 
L(M) 
is 
a 
regular 
set} 
is 
for 

_____________________________________________
350 
Miscellaneous 
Exercises 
.H143. 
Prove 
that 
there 
exists 
a 
total 
computable 
function 
f 
: 
N 
-> 
N 
that 
is 
not 
provably 
total 
in 
Peano 
arithmetic. 

_____________________________________________
Hints 
for 
Selected 
Miscellaneous 
Exercises 
6. 
Look 
for 
a 
clue 
in 
Lecture 
5. 
9. 
(b) 
Build 
an 
NFA 
with 
seven 
states.arranged 
in 
loops 
of 
length 
two 
and 
five. 
Assign 
start 
and 
final 
states 
so 
that 
the 
shortest 
rejected 
string 
is 
of 
length 
9 
= 
2 . 5 -
1. 
(c) 
The 
product 
of 
a 
set 
of 
distinct 
prim 
es 
is 
exponential 
in 
their 
sumo 
26. 
Given 
a 
DFA 
M 
for 
A, 
build 
an 
NFA 
M' 
for 
FirstHalvesA 
that 
ments 
the 
following 
idea: 
put 
a 
white 
pebble 
on 
the 
start 
state 
of 
M 
and 
greeIt 
and 
blue 
pebbles 
on 
an 
arbitrary 
guessed 
state 
of 
M. 
Never 
move 
the 
green 
pebble. 
Move 
the 
white 
pebble 
forward 
in 
response 
to 
input 
symbols 
and 
move 
the 
blue 
pebble 
forward 
according 
to 
some 
nondeterministically 
guessed 
input 
symbol. 
Accept 
if 
the 
white 
and 
green 
pebbles 
occupY 
thesame 
state 
and 
the 
blue 
pebble 
occupies 
an 
accept 
state 
when 
the 
input 
string 
is 
exhausted. 
Describe 
M' 
formally 
and 
prove 
that 
it 
accepts 
the 
set 
FirstHalves 
A. 
Presumably 
the 
set 
of 
states 
of 
M' 
will 
be 
the 
set 
Q 
x 
Q 
x 
Q 
encoding 
the 
positions 
of 
the 
three 
pebbles. 
In 
general, 
you 
will 
see 
several 
problems 
of 
the 
form 
show 
that 
if 
Ais 
a 
regular 
set, 
then 
so 
is 
A', 

_____________________________________________
352 
Hints 
for 
Selected 
Miscellaneous 
Exercises 
where 
A' 
is 
some 
set 
formed 
by 
massaging 
A 
in 
some 
way. 
Exercise 
2 
of 
Homework 
2 
about 
rev 
A 
and 
Exercise 3 
of 
Homework 
3 
ab 
out 
MiddleThirds 
Aare 
of 
this 
type. 
Most 
of 
these 
problems 
can 
be 
solved 
by 
applying 
the 
foHowing 
five-step 
protocol: 
Step 
1 
Assume 
we 
are 
given 
a 
deterministic 
finJte 
automaton 
M 
= 
(Q, 
E, 
6, 
s, 
F) 
accepting 
A. 
We 
want 
to 
build 
a 
nondeterministic 
automaton 
M' 
= 
(Q', 
E, 
/j.', 
S', 
F') 
accepting 
A'. 
Come 
up 
with 
an 
intuitive 
design 
of 
M' 
in 
terms 
of 
moving 
pebbles 
around 
on 
the 
states 
of 
M. 
Think 
ab 
out 
the 
initial 
configuration 
of 
pebbles, 
how 
the 
pebbles 
should 
move 
in 
response 
to 
each 
input 
symbol, 
and 
what 
the 
accepting 
configurations 
should 
be. 
Step 
2 
Write 
doW'Il 
a 
formal 
description 
of 
Q', 
E, 
/j.', 
S', 
and 
F' 
that 
formaHy 
captures 
your 
intuition 
about 
moving 
the 
pebbles 
developed 
in 
step 
1. 
The 
first 
thing 
to 
think 
about 
is 
what 
the 
states 
Q' 
should 
be. 
You 
need 
to 
figure 
out 
how 
to 
encode 
formally 
the 
information 
that 
the 
new 
machine 
needs 
to 
remember 
at 
each 
step. 
Make 
sure 
the 
types 
are 
right; 
for 
example, 
whatever 
you 
decide 
the 
set 
of 
states 
Q' 
should 
be, 
the 
start 
state 
should 
be 
an 
element 
of 
Q' 
and 
the 
set 
of 
accept 
states 
F' 
should 
be 
a 
subset 
of 
Q'. 
If 
you 
are 
des.igning 
a 
deterministic 
machine 
M', 
then 
6' 
should 
be 
a 
function 
Q' 
x E 
-t 
Q'. 
If 
M' 
is 
to 
be 
nondeterministic, 
then 
you 
should 
have 
/j.' 
: 
Q' 
X 
E 
-t 
2
Q
'. 
Step 
3 
In 
step 
2, 
you 
defincd 
a 
transition 
function 
fl' 
of 
M'. 
Most 
likely, 
fl' 
was 
defined 
in 
terms 
of 
the 
transition 
function 
6 
of 
M. 
State 
a 
lemma 
extending 
this 
relationship 
to 
a 
relationship 
between 
'8 
and 
Li'. 
Step 
4 
Prove 
the 
lemma 
stated 
in 
step 
3 
by 
induction 
on 
lxi. 
The 
proof 
will 
most 
likely 
use 
the 
standard 
inductive 
definitions 
of 
'8 
and 
Li', 
as 
weH 
as 
the 
definition 
you 
gave 
in 
step 
2 
of 
fl' 
in 
terms 
of 
6. 
Step 
5 
Prove 
that 
L( 
M') 
= 
A'. 
The 
proof 
will 
generally 
use 
the 
lemma 
proved 
in 
step 
4 
and 
the 
definitions 
of 
S' 
and 
F'. 
Step 
1 
is 
usually 
not 
much 
of 
a 
problem, 
since 
it 
is 
usually 
easy 
to 
see 
how 
to 
move 
the 
pebbles. 
Steps 
2 
and 
3 
typically 
give 
the 
most 
trouble. 
If 
the 
lemma 
in 
step 
3 
is 
formulated 
correctly, 
the 
proofs 
in 
4 
and 
5 
should 
be 
fairly 
routine. 
An 
example 
of 
an 
application 
of 
this 
protocol 
is 
given 
in 
the 
solution 
to 
this 
exercise 
on 
p. 
358. 
31. 
One 
of 
the 
sets 
is 
{a, 
b} 
* 

_____________________________________________
Hints 
for 
Selected 
Miscellaneous 
Exercises 
353 
32. 
Use 
the 
matrix 
representation 
of 
Miscellaneous 
Exercise 
7. 
33. 
U 
se 
the 
fact 
that 
where 
p(i) 
denotes 
the 
ith 
derivative 
of 
p. 
43. 
Try 
U 
(a+c)n(b+c)n 
+ 
(a 
+ 
b 
+ 
c)*cc(a 
+ 
b 
+ 
c)* 
with 
k 
= 
3. 
45. 
(a) 
By 
Theorem 
12.3, 
it 
suffices 
to 
show 
that 
A* 
is 
ultimately 
odic. 
You 
can 
take 
as 
period 
the 
length 
of 
the 
smallest 
nonnull 
element 
of 
A. 
This 
is 
not 
necessarily 
the 
smallest 
period, 
but 
it 
will 
do. 
49. 
For 
the 
direction 
(::}), 
use 
induction 
on 
the 
stages 
of 
the 
algorithm. 
For 
the 
direction 
use 
ind 
uction 
on 
Ix!-
50. 
(b) 
Consider 
the 
relation 
y 
== 
z 
overlap(y,x) 
= 
overlap(z, 
x), 
where 
overlap(y, 
x) 
is 
the 
longest 
string 
that 
is 
both 
a 
suffix 
of 
y 
and 
aprefix 
of 
x. 
Use 
the 
Myhill-Nerode 
theorem. 
51. 
Suppose 
that 
you 
start 
with 
a 
DFA 
for 
B 
with 
no 
inacce'ssible 
states, 
reverse 
the 
transitions 
and 
exchange 
the 
start 
and 
final 
states 
to 
get 
an 
NFA 
for 
rev 
B, 
then 
construct 
an 
equivalent 
DFA 
uslng 
the 
set 
construction, 
omitting 
inaccessible 
states. 
Prove 
that 
the 
resulting 
automaton 
is 
the 
minimal 
DFA 
for 
rev 
B. 
55, 
(b) 
Two 
of 
them 
are 
fand 
aa* 
60. 
aa* 
== 
a*a. 
61. 
(b) 
Build 
an 
NFA 
whose 
states 
are 
subsets 
of 
Q. 
Let 
the 
machine 
guess 
the 
sets 
Wi. 

_____________________________________________
354 
Hints 
for 
Selected 
Miscellaneous 
Exercises 
70. 
Prove 
induct.ively 
that 
S 
x{:::=} 
#a(x) 
= 
2#b(x), 
G 
A 
x{:::=} 
#a(x) 
= 
2#b(x) 
+ 
1, 
G 
B 
x{:::=} 
#a(x) 
::: 
2#b(x) 
-
2. 
G 
Think 
about 
the 
graph 
of 
the 
function 
#a(y) 
-
2#b(y) 
for 
prefixes 
y 
of 
x. 
79. 
Everything 
you 
need 
can 
be 
found 
in 
Lecture 
10. 
80. 
Use 
Parikh's 
theorem 
(Theorem 
H.1) 
and 
the 
theorem 
on 
ultimate 
periodicity 
(Theorem 
12.3). 
83. 
Use 
Parikh's 
theorem 
(Theorem 
H.1) 
and 
the 
fact 
that 
the 
complement 
of 
a 
semilinear 
subset 
of 
1'1" 
is 
semilinear. 
86. 
Show 
that 
the 
condition 
(P) 
of 
the 
pumping 
lemma 
for 
regular 
sets 
given 
in 
Lecture 
11 
implies 
the 
condition 
of 
the 
pumping 
lemma 
for 
context-free 
languages 
given 
in 
Lecture 
22. 
Give 
a 
non-context-free 
set 
satisfying 
(P). 
A 
slightly 
modified 
version 
of 
the 
hint 
for 
Miscellaneous 
Exercise 
43 
should 
do. 
87. 
(c) 
Build 
a 
PDA 
that 
pushes 
and 
pops 
antirnatter. 
90. 
(d) 
Consider 
the 
set 
A 
= 
{anbncmamb"c" 
I 
n,m,k;::: 
I}. 
93. 
(a) 
Let 
M 
= 
(Q, 
r, 
8, 
..l, 
-1, 
S, 
0) 
be 
a 
DPDA 
for 
D 
that 
accepts 
by 
empty 
stack. 
Prove 
that 
for 
p,q 
E 
Q 
and 
a 
hE 
r* 
I 
(q,•,•)} 
M 
is 
a 
regular 
set'. 
(b) 
See 
the 
'end 
of 
Lecture 
27. 
99. 
(b) 
Simulating 
a 
queue 
machine 
with 
a 
Turing 
machine 
is 
easy. 
The 
other 
direction 
is 
tricky. 
Make 
the 
queue 
of 
the 
queue 
machine 
contain 
a 
representation 
of 
the 
configuration 
of 
the 
Turing 
ma-

_____________________________________________
Hints 
for 
Selected 
Miscellaneous 
Exercises 
355 
chine. 
The 
hard 
part 
is 
simulating 
a 
left 
move 
of 
the 
Turing 
machine. 
You 
need 
to 
go 
all 
the 
way 
around 
the 
queue. 
You 
might 
try 
breaking 
your 
solution 
into 
two 
steps: 
(i) 
First 
invent 
a 
new 
kind 
of 
Turing 
machine, 
a 
one-way 
ing 
machine. 
machines 
can 
only 
move 
right 
on 
the 
tape. 
When 
they 
see 
the 
right 
endmarker, 
they 
magically 
jump 
back 
to 
the 
left 
endmarker. 
Show 
that 
one-way 
machines 
can 
si 
ulate 
ordinary 
TMs. 
Simulate 
a 
left 
inove 
of 
the 
ordinary 
TM 
by 
pushing 
a 
marker 
all 
the 
way 
around 
to 
the 
right. 
(ii) 
Simulate 
one-way 
machines 
with 
queue 
machines. 
100. 
(c) 
Think 
VALCOMPS. 
102. 
(a) 
The 
infinite 
checkerboard 
should 
be 
Z 
x 
Z, 
where 
Z 
is 
the 
set 
of 
integers 
{ 
... 
, -
2, 
-1,0, 
1, 
2,3, 
... 
}. 
Tape 
contents 
should 
br 
modeled 
by 
functions 
f 
: 
Z 
x 
Z 
--+ 
r, 
which 
assign 
a 
tape 
symbol 
in 
r 
to 
each 
cell 
(i,j) 
E 
Z 
x 
Z. 
105. 
(b) 
Think 
VALCOMPS. 
119. 
(a) 
Take 
G 
to 
be 
S 
--+ 
aS 
I 
bS 
I 
E. 
121. 
Think 
VALCOMPS. 
123. 
Think 
VALCOMPS. 
128. 
Use 
Exercise 
4 
of 
Homework 
9. 
130. 
Let 
P 
be 
a 
nontrivial 
property 
of 
the 
r.e. 
sets. 
Then 
there 
exist 
TMs 
MT 
and 
M.l 
such 
that 
P(L(MT)) 
= 
T 
and 
P(L(M.l)) 
= 
.L 
Show 
that 
if 
it 
were 
decidable 
for 
TMs 
M 
whether 
P(L(M)) 
= 
T, 
then 
we 
could 
construct 
a 
total 
computable 
map 
(J 
with 
no 
fixpoint, 
contradicting 
the 
recursion 
theorem 
(see 
Miscellaneous 
Exercise 
129). 
131. 
Use 
the 
re('ursion 
theorem 
(see 
Miscellaneous 
Exercise 
129). 
132. 
(a) 
Diagonalize. 
(b) 
Let::; 
be 
an 
arbitrary 
cornputable 
linear 
order 
on 
the 
set 
of 
input 
strings. 
Given 
!vi, 
let 
AI' 
be 
a 
machine 
that 
on 
input 
x 
simulat.es 
At 
on 
all 
y 
::; 
x. 

_____________________________________________
356 
Hints 
for 
Selected 
Miscellaneous 
Exercises 
135. 
Use 
the 
pumping 
lemma. 
138. 
Diagonalize. 
140. 
Construct 
a 
list 
of 
total 
Turing 
machines 
that 
run 
in 
linear 
time 
such 
that 
every 
set 
computable 
in 
linear 
time 
is 
accepted 
by 
some 
machine 
on 
the 
list. 
Build 
a 
machine 
that 
diagonalizes 
over 
this 
list. 
143. 
Diagonalize. 

_____________________________________________
Solutions 
to 
Selected 
Miscellaneous 
Exercises 
7. 
(a) 
By 
induction 
on 
n. 
Basis 
if 
11. 
= 
v, 
otherwise 
= 
{x 
E 
E* 
JJxJ 
= 
0 
and 
6(11., 
x) 
= 
v}. 
Ind11.ction 
step 
(A
n
+
1
)uv 
= 
(AnA)uv 
= 
U 
(An)uwAwv 
wEQ 
= 
U 
{x 
E 
E* 
JJxJ 
= 
n 
and 
6(11., 
x) 
= 
w}· 
{a 
E 
E 
J6(w,a) 
= 
v} 
wEQ 
= 
{xa 
E 
E* 
JJxJ 
= 
n 
and 
3w 
6(11., 
x) 
= 
wand 
6(w,a) 
= 
v} 
= 
{xa 
E 
JJxJ 
= 
n 
and 
6(11.,xa) 
= 
v} 
= 
{y 
E 
E* 
JJyJ 
= 
n 
+ 
1 
and 
6(11.,y) 
= 
v}. 

_____________________________________________
358 
Solutions 
to 
Selected 
Miscellaneous 
Exercises 
20. 
(a) 
We'll 
show 
the 
inequality 
in 
both 
directions 
using 
the 
axioms 
(A.l) 
through 
(A.15). 
Since 
a* 
= 
l+aa.* 
by 
(A.IO), 
we 
have 
aa* 
a* 
by 
the 
definition 
Then 
a*a* 
a* 
follows 
from 
(A.14). 
Conversely, 
a*a* 
= 
a*(l 
+ 
aa*) 
by 
(A.IO) 
= 
a* 
+ 
a*aa* 
by 
(A.8); 
therefore, 
a* 
a*a* 
by 
the 
definition 
25. 
We 
show 
first 
that 
A*b 
is 
a 
solution 
of 
x 
= 
Ax+b. 
By 
Lemma 
A.2, 
we 
have 
that 
A* 
= 
AA* 
+ 
Ij 
this 
isjust 
axiom 
(A.lO) 
of 
Kleene 
algebra. 
Multiplying 
both 
by 
band 
distributing, 
we 
get 
A*b 
= 
AA*b+ 
b, 
which 
is 
just 
x 
= 
Ax 
+ 
b 
with 
A*b 
substituted 
for 
x. 
Now 
we 
wish 
to 
show 
that 
A*b 
is 
the 
least 
solution. 
Let 
c 
be 
any 
other 
solution; 
then 
c 
= 
Ac 
+ 
b. 
The 
array 
c 
is 
a 
vector 
of 
length 
n 
over 
the 
Kleene 
algebra 
K. 
Form 
a 
square 
matrixC 
by 
juxtaposing 
n 
copies 
of 
c. 
Form 
the 
matrix 
B 
from 
b 
similarly. 
Then 
C 
= 
AC 
+ 
B. 
By 
Lemma 
A.2 
and 
axiom 
(A.12) 
ofKleene 
algebra, 
A* 
B 
C, 
therefore 
A*b 
c. 
26. 
We 
show 
that 
if 
Ais 
regular, 
then 
so 
is 
FirstHalvesA 
using 
the 
five-step 
protocol 
given 
in 
tne 
hint 
for 
this 
exercise 
on 
p. 
351. 
Step 
1 
Let 
M 
= 
(Q, 
8, 
s, 
F) 
be 
a 
DFA 
for 
A. 
Here 
is 
an 
informal 
description 
of 
an 
NFA 
M' 
for 
FirstHalves 
A 
in 
terms 
of 
pebbles. 
There 
will 
be 
a 
white 
pebble, 
a 
green 
pebble, 
and 
a 
blue 
pebble 
on 
the 
automaton 
at 
any 
point 
in 
time. 
We 
start 
with 
the 
white 
pebble 
on 
the 
start 
state 
of 
M 
and 
the 
blue 
and 
green 
pebbles 
together 
on 
a 
nondeterministically 
chosen 
state 
of 
M. 
The 
initial 
position 
of 
the 
blue 
and 
green 
pebbles 
is 
a 
guess 
as 
to 
where 
M 
will 
be 
after 
scanning 
x. 
In 
each 
step, 
we 
move 
the 
white 
pebble 
forward 
according 
to 
the 
input 
symbol 
and 
move 
the 
blue 
pebble 
forward 
according 
to 
some 
nondeterministically 
chosen 
symbol. 
The 
green 
pebble 
never 
moves. 
When 
the 
end 
of 
the 
input 
x 
is 
reached, 
we 
accept 
iff 
the 
white 
pebble 
and 
green 
pebble 
occupy 
the 
same 
state 
and 
the 
blue 
pebble 
occupies 
an 
accept 
state. 
The 
white 
pebble 
will 
occupy 
the 
state 
8( 
s, 
x), 
since 
we 
moved 
it 
according 
to 
the 
input 
x. 
The 
blue 
pebble 
will 
occupy 
some 
state 
q 
reachable 
from 
the 
position 
of 
the 
green 
pebble 
under 
some 
string 
y 
such 
that 
lyl 
= 
lxi. 
If 
the 
white 
and 
green 
pebbles 
occupy 
the 
same 
state 
and 
the 
blue 

_____________________________________________
Solutions 
to 
Selected 
Miscellaneous 
Exercises 
359 
pebble 
occupies 
an 
accept 
state, 
then 
we 
can 
concatenate 
x 
and 
y 
to 
get 
astring 
twice 
as 
long 
as 
x 
accepted 
by 
M. 
Step 
2 
Now 
let's 
do 
this 
formally. 
Define 
the 
NFA 
Step 
3 
Lemma 
M
' 
-
(Q' 
ß' 
S' 
F') 
-
" 
, 
, 
as 
follows. 
We 
take 
the 
states 
of 
M' 
to 
be 
Q' 
Q3, 
the 
set 
of 
ordered 
tripies 
of 
elements 
of 
Q. 
For 
(p, 
q, 
r) 
E 
Q', 
the 
first 
component 
models 
the 
position 
of 
the 
white 
pebble, 
the 
second 
models 
the 
position 
of 
the 
green 
pebble, 
and 
the 
third 
models 
the 
position 
of 
the 
blue 
pebble. 
The 
transition 
function 
ß' 
must 
be 
a 
function 
ß' 
: 
Q' 
x 
-+ 
2
Q
' Ł 
For 
any 
p, 
q, 
r E 
Q 
and 
a 
E 
we 
define 
ß'((p,q,r),a) 
{(ö(p,a),q,ö(r,b)) 
I 
b 
E 
These 
are 
the 
possible 
next 
pebble 
positions 
after 
(p, 
q, 
r) 
on 
input 
a 
,E 
The 
first 
component 
ö(p, 
a) 
says 
that 
we 
move 
the 
white 
pebble 
according 
to 
the 
input 
symbol 
a; 
the 
second 
component 
q 
says 
that 
we 
leave 
the 
green 
pebble 
where 
it 
is; 
and 
the 
third 
component 
ö(r, 
b) 
says 
that 
we 
move 
the 
blue 
pebble 
according 
to 
b 
E 
All 
possible 
bare 
included, 
which 
reflec.ts 
the 
idea 
that 
M' 
is 
guessing 
the 
next 
symbol 
of 
the 
string 
y. 
We 
define 
the 
start 
states 
of 
M' 
to 
be-
S' 
{(s,t,t) 
I 
tE 
Q}, 
modeling 
all 
possible 
initial 
configurations 
of 
pebbles. 
The 
'ilhite 
ble 
initially 
occupies 
the 
start 
state 
of 
M 
and 
the 
green 
and 
blue 
pebbles 
occupy 
anarbitrary 
nondeterministically 
chosen 
state 
of 
M. 
Finally, 
we 
take 
the 
accept 
states 
of 
lv{' 
tO 
'oe 
F' 
{(u,u,v) 
lu 
E 
Q, 
v 
E 
F}, 
indicating 
that 
we 
accept 
provided 
the 
white 
and 
green 
pebbles 
occupy 
the 
same 
state 
and 
the 
blue 
pebble 
occupies 
an 
accept 
state. 
Our 
formal 
definition 
specifies 
a 
relations 
hip 
between 
6 
and 
ß'. 
Now 
let's 
try 
to 
extend 
it 
to 
a 
relations 
hip 
between 
"6 
and 
Li'. 
Intuitively, 
after 
scanning 
astring 
:t: 
of 
length 
n 
starting 
in 
some 
start 
state 
(s, 
q, 
q), 
the 
machine 
M' 
can 
be 
in 
any 
state 
ofthe 
form 
("6(s,x),q,"6(q,y)) 
for 
some 
y 
E 
For 
any 
x 
E 
Li'(s',x) 
= 
{("6(s,x),q,6(q,y)) 
I 
q 
E 
Q, 
Y 
E 

_____________________________________________
360 
Solutions 
to 
Selected 
Miscellaneous 
Exercises 
Step 
4 
We 
prove 
the 
lemma 
of 
step 
3 
by 
induction 
on 
lxi. 
For 
the 
basis 
x 
= 
f, 
we 
use 
the 
base 
clauses 
(3.1) 
and 
(6.1) 
in 
the 
inductive 
definitions 
of 
6 
and 
and 
the 
definition 
of 
S': 
= 
S' 
= 
{(s,q,q) 
I 
q 
E 
Q} 
= 
{(6(s, 
•),q,6(q, 
•)) 
I 
q 
E 
Q} 
= 
{(6(s,•),q,6(q,y)) 
I 
q 
E 
Q, 
y 
E 
EO}. 
For 
the 
induction 
step, 
assume 
that 
the 
lemma 
is 
true 
for 
Xj 
that 
is, 
= 
{(6(s,x),q,6(q,y)) 
I 
q 
E 
Q, 
y 
E 
EI"'I}. 
We 
want 
to 
show 
that 
it 
is 
true 
for 
xaj 
that 
is, 
= 
{(6(s, 
xa),q, 
6(q, 
yb)) 
I 
q 
E 
Q, 
y 
E 
EI"'I, 
bE 
E}, 
where 
a 
E 
E. 
The 
argument 
uses 
the 
inductive 
definitions 
of 
and 
6, 
the 
induction 
hypothesis, 
and 
the 
definition 
of 
ll' 
in 
terms 
of 
° 
given 
in 
step 
2: 
= 
U 
ll'((p,q,r),a) 
by 
(6.2) 
,,,,) 
= 
U 
1l'((6(s,x),q,6(q,y)),a) 
induction 
hypothesis 
qEQ, 
yEElzl 
U 
{(o(6(s,x),a),q,o(6(q,y),b)) 
I 
b 
E 
E} 
qEQ, 
yEElzl 
definition 
of 
ll' 
U 
{(6(s,xa),q,6(q,yb)) 
I 
bE 
E} 
by 
(3.2) 
qEQ, 
yEEI-1 
1"'1 
= 
{(o(s,xa),q,o(q,yb)) 
I 
q 
E 
Q, 
y 
E 
E 
,b 
E 
E}. 
Step 
5 
Finally, 
we 
prove 
L(M') 
= 
FirstHalves 
L(M). 
For 
any 
x 
E 
E*, 
xE 
L(M') 
<==> 
n 
F'::I 
0 
<==> 
{(6( 
s, 
x), 
q, 
6(q, 
y)) 
I 
q 
E 
Q, 
y 
E 
EI"'I} 
the 
lemma 
of 
step 
3 
n 
((u,u,v) 
lu 
E 
Q, 
v 
E 
F}::I 
0 
definition 
of 
F' 
<==> 
3y 
E 
EI"'I 
3q 
E 
Q 
6(s,z) 
= 
q 
and 
6(q,y) 
E 
F 
<==> 
3y 
E 
EI"'I 
6(6(s,z),y) 
E 
F 
o 
<==> 
3y 
E 
EI"'I 
6(s,zy) 
E 
F 
Homework 
1, 
Exercise 
3 

_____________________________________________
Solutions 
to 
Selected 
Miscellaneous 
Exercises 
361 
{::::? 
3y 
e 
EI"'I 
xye 
L(M) 
{::::? 
x 
e 
FirstHalves 
L(M). 
33. 
Using 
the 
fact 
that 
d Ł 
p(i)( 
n) 
p(n 
+ 
1) 
= 
L 
-.,-, 
i=O 
where 
p(i) 
denotes 
the 
ith 
derivative 
of 
p, 
we 
have 
(j) 
_ 
p(i+j)(n) 
_ 
p(k)(n) 
p 
(n+1)-L.." 
., 
-L..,,(k-·)" 
Ic' 
J . 
Ł = 
=3 
therefore 
+ 
1) 
= 
t 
( 
) 
J. 
Ic' 
J 
k. 
=3 
Also, 
pW 
(0) 
-,-,-
=aj, 
J. 
o 
(1) 
(2) 
where 
aj 
is 
the 
coefficient 
of 
n
j 
in 
p(n). 
Let 
M 
= 
(Q, 
E, 
ß, 
s, 
F) 
be 
an 
NFA 
for 
A. 
We 
will 
build 
a 
DFA 
M' 
= 
(Q', 
E, 
0', 
s', 
F') 
for 
A'. 
Let 
B 
be 
the 
square 
Boolean 
matrix 
indexed 
by 
states 
of 
M 
such 
that 
B 
{1 
if 
3a 
e 
E 
v 
e 
ß(u,a), 
uv 
-
0 
otherwise. 
Let 
Bn 
be 
the 
nth 
Boolean 
power 
of 
B, 
BO 
the 
identity 
matrix. 
One 
can 
show 
by 
induction 
on 
n 
that 
(Bn)uv= 
{1 
if3ye.En
v
eLi({u},y), 
(3) 
o 
otherWlse. 
In 
other 
words, 
(Bn)uv 
= 
1 
iff 
there 
exists 
a 
path 
of 
length 
n 
from 
u 
to 
v 
in 
the 
graph 
representation 
of 
the 
automaton. 
Now 
consider 
the 
set 
of 
all 
square 
Boolean 
matrices 
indexed 
by 
the 
states 
of 
M. 
The 
states 
Q' 
of 
M' 
will 
be 
the 
set 
of 
all 
sequences 
(Co, 
Cl, 
... 
,Cd) 
of 
d 
+ 
1 
such 
matrices, 
which 
we 
denote 
by 
(Ci 
I 
o 
:5 
i 
:5 
d). 
Define 
d Ł 
o'((C·1 
0 
:5 
i 
:5 
d),a) 
= 
(11 
d
j
) 
10:5 
j 
:5 
d), 
Ic=j 
s' 
= 
(BQ; 
10 
$ 
i 
$ 
d), 
F' 
= 
{(Ci 
10 
$ 
i 
$ 
d) 
13q 
e 
F 
(CO).q 
= 
1}. 

_____________________________________________
362 
Solutions 
to 
Selected 
Miscellaneous 
Exercises 
Lemma 
Let 
x 
E 
1::*. 
Then 
6'(s',x) 
= 
(BP(i)(\"'I)/i! 
10:$ 
i 
:$ 
d). 
Proof. 
By 
induction 
on 
lxi. 
Basis 
8' 
(s', 
•) 
= 
s' 
definition 
of 
8' 
= 
(BQi 
I 
0 
:$ 
i 
:$ 
d) 
definition 
of 
s' 
= 
(BP(i)(O)/i! 
10 
:$ 
i 
:$ 
d) 
by 
(2) 
= 
(BP(i)(\<I)/i! 
I 
0 
:$ 
i 
:$ 
d). 
Induction 
step 
Assume 
true 
for 
x. 
Then 
8' 
(s', 
xa) 
= 
15' 
(8' 
(s', 
x), 
a) 
definition 
of 
6' 
= 
ö'((BP(i)(\"'I)/i! 
10:$ 
i 
:$ 
d),a) 
induction 
hypothesis 
d 
= 
(II 
10:$ 
j 
:$ 
d) 
definition 
of 
15' 
k=j 
= 
I 
0 
:$ 
j 
:$ 
d) 
= 
(BP(i)(\"'\+l)/i! 
10:$ 
i 
:$ 
d) 
by 
(1) 
= 
(BP(i)(\:caj)/i! 
I 
0 
i 
d). 
o 
Theorem 
L(M') 
= 
A'. 
Proof. 
35. 
Let 
xE 
L(M') 
{::::::::} 
8'(s',x) 
E 
F' 
{::::::::} 
(BP(i)(\"'I)/i! 
I 
0 
:$ 
i 
:$ 
d) 
E 
p' 
{::::::::} 
3q 
E 
P 
BP(\"'\)(s,q) 
= 
1 
{::::::::} 
3y 
E 
EP(\"'j) 
0 
{::::::::} 
3y 
E 
EP(\"'j) 
y 
E 
L(M) 
{::::::::} 
x 
E 
A' 
A 
= 
{ww 
Iw 
E 
{O,l}*}. 
definition 
of 
acceptance 
by 
the 
lemma 
definition 
of 
P' 
by 
(3) 
definition 
of 
acceptance 
definition 
of 
A' 
0 
Note 
that 
{ww 
I 
w 
E 
{O} 
*} 
is 
is 
just 
the 
lIet 
of 
strings 
of 
O's 
of 
even 
length. 

_____________________________________________
Solutions 
to 
Selected 
Miscellaneous 
Exercises 
363 
To 
show 
that 
A 
is 
nonregular, 
we 
will 
show 
that 
we 
have 
a 
winning 
strategy 
in 
the 
demon 
game. 
The 
demon 
picks 
some 
k. 
Now 
we 
take 
x 
= 
0, 
Y 
= 
lk, 
and 
z 
= 
Olk. 
Then 
xyz 
= 
OlkOlk, 
which 
is 
in 
A, 
and 
lyl 
= 
k. 
The 
dem 
on 
must 
now 
choose 
u, 
v, 
w 
such 
that 
y 
= 
uvw 
and 
v 
f. 
E. 
Say 
the 
demon 
picks 
u, 
v, 
w 
of 
lengths 
j, 
m, 
n, 
respectively. 
Then 
k 
= 
j 
+ 
m 
+ 
n 
and 
m 
> 
O. 
But 
whatever 
the 
d'emon 
picks, 
we 
can 
win 
by 
taking 
i 
= 
0: 
xuvowz 
= 
xuwz 
= 
01 
k-mQ1 
k, 
which 
is 
not 
in 
A 
because 
it 
is 
not 
of 
the 
form 
ww 
for 
any 
w. 
36. 
Recall 
that 
a 
number 
is 
prime 
if 
it 
is 
greater 
than 
1 
and 
has 
no 
sors 
other 
than 
land 
itself. 
PRIMES 
is 
an 
example 
of 
a 
single 
letter 
alphabet 
set 
that 
is 
not 
regular 
because 
the 
elements 
of 
the 
set 
do 
not 
appear 
with 
any 
predictable 
pattern. 
Suppose 
the 
demon 
chooses 
k. 
You 
choose 
x 
= 
z 
= 
E 
and 
y 
= 
a
P
, 
where 
p 
is 
the 
smallest 
prime 
greater 
than 
k 
(Euclid 
proved 
that 
there 
exist 
infinitely 
many 
primes, 
so 
p 
exists). 
Then 
xyz 
= 
a
P 
E 
PRIMES 
and 
lyl 
= 
p 
> 
k. 
The 
demon 
must 
now 
choose 
u, 
v, 
w 
such 
that 
y 
= 
uvw 
and 
v 
f. 
E. 
Say 
the 
lengths 
of 
u, 
v, 
ware 
j, 
m, 
n, 
respectively. 
Then 
k 
= 
j 
+ 
m 
+ 
n 
and 
m 
> 
o. 
You 
now 
need 
to 
find 
i 
such 
that 
xuviwz 
f/. 
PRIMES 
(Le., 
Ixuviwzl 
is 
not 
prime). 
But 
Ixuviwzl 
= 
j 
+ 
im 
+ 
n 
= 
p 
+ 
(i 
-
l)m, 
so 
we 
need 
to 
find 
i 
such 
that 
p 
+ 
(i 
-
l)m 
is 
not 
prime. 
Take 
i 
= 
p 
+ 
l. 
Then 
p 
+ 
(i 
-
l)m 
= 
p 
+ 
pm 
= 
p(1 
+ 
m), 
which 
is 
not 
prime 
since 
it 
has 
factors 
p 
and 
1 
+ 
m. 
You 
win. 
44. 
(a) 
First 
we 
show 
that 
the 
given 
condition 
is 
necessary 
for 
regularity. 
Let 
A 
be 
a 
regular 
set, 
and 
let 
k 
be 
the 
number 
of 
states 
of 
a 
DFA 
for 
A. 
Then 
for 
all 
y 
E 
with 
lyl 
= 
k, 
the 
automaton 
repeats 
astate 
while 
scanning 
y. 
Let 
v 
be 
a 
nonnull 
substring 
of 
y 
such 
that 
the 
automaton 
is 
in 
the 
same 
state 
just 
before 
and 
just 
after 
scanning 
v, 
and 
let 
y 
= 
uvw. 
Then 
for 
all 
z 
E 
and 
i 
2: 
0, 
the 
automaton 
is 
in 
the 
same 
state 
after 
scanning 
yz 
= 
uvwz 
as 
after 
scanning 
uviwz; 
therefore, 
yz 
E 
A 
<==:> 
uviwz 
E 
A. 
Now 
we 
show 
that 
the 
given 
condition 
is 
sufficient 
for 
regularity. 
Let 
A 
such 
that 
A 
satisfies 
the 
given 
condition. 
For 
any 
x 
E 
with 
lxi 
2: 
k, 
by 
applying 
the 
condition 
with 
i 
= 
0 

_____________________________________________
364 
Solutions 
to 
Selected 
Miscellaneous 
Exercises 
as 
many 
times 
as 
necessary, 
we 
can 
repeatedly 
delete 
nonnull 
substrings 
of 
x 
until 
we 
obtain 
astring 
x' 
of 
length 
k 
-
1 
or 
less 
such 
that 
for 
all 
z 
E 
xz 
E 
A 
{::::::} 
x' 
z 
E 
A. 
This 
says 
that 
x 
=A 
x', 
where 
=A 
is 
the 
relation 
(16.1). 
Since 
every 
=A-class 
contains 
astring 
oflength 
k-1 
or 
less, 
the 
relation 
=A 
is 
offinite 
index. 
By 
the 
Myhill-Nerode 
theorem, 
Ais 
regular. 
45. 
(b) 
Equivalently, 
if 
A 
N 
and 
A 
is 
the 
smallest 
subset 
of 
N 
taining 
A 
and 
0 
and 
closed 
under 
addition, 
then 
A 
* 
consists 
of 
all 
but 
finitely 
many 
multiples 
of 
gcd 
A. 
First 
we 
conside{ 
the 
case 
lAI 
= 
2. 
Let 
mN 
denote 
the 
set 
of 
all 
nonnegative 
multiples 
of 
m, 
and 
let 
mN 
+ 
nN 
denote 
the 
set 
of 
all 
sums 
am 
+ 
bn 
for 
a, 
b 
O. 
Write 
A 
'" 
B 
if 
A 
and 
B 
differ 
by 
a 
finite 
set; 
that 
is, 
if 
the 
set 
(A 
-
B) 
U 
(B 
-
A) 
is 
finite. 
Lemma 
Let 
m 
and 
n 
be 
positive 
integers, 
9 
= 
gcd( 
m, 
n). 
Then 
mN 
+nN 
rv 
gN. 
Moreover, 
lcm( 
m, 
n) 
-
m 
-
n 
is 
the 
largest 
multiple 
01 
9 
not 
expressible 
in 
the 
lorm 
am 
+ 
bn 
with 
a, 
b 
O. 
Proof. 
We 
first 
show 
a 
special 
case: 
if 
m 
and 
n 
are 
relatively 
prime, 
then 
mN 
+ 
nN 
'" 
N 
Moreover, 
the 
largest 
number 
not 
expressible 
in 
the 
form 
am 
+ 
bn 
with 
a, 
b 
0 
is 
mn 
-
m 
-
n. 
Suppose 
mn-m-n 
were 
so 
expressible, 
say 
am+bn 
= 
n. 
Then 
(a 
+ 
l)m 
+ 
(b 
+ 
l)n 
= 
mn. 
Since 
m 
and 
n 
are 
relatively 
prime, 
m 
must 
divide 
b+ 
1 
and 
n 
must 
divide 
a+ 
1. 
The 
smallest 
values 
of 
a 
and 
b 
for 
which 
this 
is 
true 
would 
be 
a 
= 
n 
-
1 
and 
b 
= 
m 
-
1, 
which 
are 
already 
too 
big: 
(n 
-
l)m 
+ 
(m 
-
l)n 
= 
2mn 
-
m 
-
n 
> 
mn 
-
m 
-
n. 
Now 
let's 
show 
that 
mn 
-
m 
-
n 
+ 
1 
is 
expressible. 
Let 
u 
< 
n 
and 
v 
< 
m 
such 
that 
vn 
-
um 
= 
1. 
(The 
numbers 
u 
and 
v 
can 
be 
produced 
by 
an 
extended 
version 
of 
the 
Euclidean 
GCD 
algorithm.) 
Take 
a 
= 
n 
-
u 
-
1 
and 
b 
= 
v 
-
1. 
Then 
am 
+ 
bn 
= 
(n 
-
u 
-
1)m 
+ 
(v 
-
1)n 
= 
mn 
-
um 
-
m 
+ 
vn 
-
n 
= 
mn-m-
n+ 
1. 
Now 
we 
proceed 
by 
induction. 
Suppose 
we 
have 
so 
me 
am 
+ 
bn 
mn 
-
m 
-
n 
+ 
1. 
Since 
(u 
-
l)m 
+ 
(m 
-
v 
-l)n 
= 
um 
-
m 
+ 
mn 
-
vn 
-
n 

_____________________________________________
Solutions 
to 
Selected 
Miscellaneous 
J:xercises 
365 
= 
mn-m-n-1, 
we 
must 
have 
either 
a 
2:: 
u 
or 
b 
2:: 
m -
v. 
If 
the 
former, 
take 
a' 
= 
a 
-
u 
and 
b' 
= 
b 
+ 
v 
to 
get 
a'm 
+ 
b n 
= 
(a 
-
u)m 
+ 
(b 
+ 
v)n 
= 
am 
-
um 
+ 
bn 
+ 
vn 
='am 
+ 
bn+ 
1. 
If 
the 
latter, 
take 
a' 
= 
a 
+ 
n 
-
u 
and 
b' 
= 
b 
-
m 
+ 
v, 
and 
again 
a'm 
+ 
b'n 
= 
(a 
+ 
n 
-
u)m 
+ 
(b 
-
m 
+ 
v)n 
= 
am 
+ 
mn 
-
um 
+ 
bn 
-
mn 
+ 
vn 
= 
am+bn+ 
1. 
If 
m 
and 
n 
are 
not 
relatively 
prime, 
say 
9 
= 
gcd(m, 
n), 
then 
everything 
is 
scaled 
by 
g. 
Any 
am 
+ 
bn 
is 
a 
multiple 
of 
9 
since 
9 
divides 
m 
and 
n, 
and 
the 
largest 
multiple 
öf 
9 
not 
so 
expressible 
is 
((mjg)(njg) 
-
mjg 
-
njg)g 
= 
lcm(m,n) 
-
m 
-
n. 
N 
ow 
we 
use 
this 
to 
show 
that 
for 
any 
A 
N, 
A 
consists 
of 
all 
but 
finitely 
many 
multiples 
of 
9 
= 
gcd 
A. 
This 
follows 
from 
the 
observation 
that 
gcd 
A 
= 
gcd 
X 
for 
some 
finite 
subset 
X 
A 
and 
from 
applying 
the 
lemma 
iteratively 
to 
obtain 
L 
LmN'VgN. 
o 
mEX 
49. 
Suppose 
first 
that 
{p, 
q} 
is 
marked. 
We 
proceed 
by 
induction 
on 
the 
stages 
of 
the 
algorithm. 
If 
{p, 
q} 
is 
marked 
in 
step 
2, 
then 
either 
pE 
F 
and 
q 
F 
or 
vice 
versa, 
therefore 
p 
';ft 
q 
(take 
x 
= 
E 
in 
the 
definition 
of 
If 
it 
is 
marked 
in 
step 
3, 
then 
for 
some 
a 
E 
{6(p, 
a), 
6(q, 
an 
was 
marked 
at 
some 
earlier 
stage. 
By 
the 
induction 
hypothesis, 
t5(p, 
a) 
';ft 
t5(q, 
a), 
therefore 
p 
';ft 
q 
by 
Lemma 
13.5. 
Conversely, 
p 
';ft 
q. 
definition, 
there 
exists 
an 
x 
E 
such 
that 
either 
6(p, 
x) 
E 
Fand 
t5( 
q, 
x) 
F 
or 
vi 
ce 
versa. 
We 
proceed 
by 
induction 
on 
the 
length 
of 
x. 
If 
x 
= 
E, 
then 
either 
p 
E 
Fand 
q 
F 
or 
vice 
versa, 
so 
{p, 
q} 
is 
marked 
in 
step 
2. 
If 
x 
= 
ay, 
then 
either 
8(6(p,a),y) 
E 
Fand 
8(6(q,a),y) 
F 
or 
vice 
versa. 
By 
the 
induction 
hypothesis, 
{6(p,a),6(q,an 
is 
eventually 
marked 
by 
the 
algorithm, 
and 
{p, 
q} 
will 
be 
marked 
in 
the 
following 
step. 
57. 
A 
subset 
of 
is 
closed 
downward 
under!;;;; 
iff 
its 
complement 
is 
closed 
upward 
under 
!;;;;. 
Since 
the 
complement 
of 
any 
regular 
set 
is 
regular, 
it 
suffices 
to 
show 
that 
all 
upward-closed 
sets 
are 
regular. 

_____________________________________________
366 
Solutions 
to 
Selected 
Miscellaneous 
Exercises 
Let 
min 
X 
denote 
the 
set 
of 
!;;;-minimal 
elements 
of 
X, 
and 
let 
y 
j 
= 
{x 
E 
I 
y 
!;;; 
x}. 
Then 
X 
is 
upward-closed 
Hf 
X 
= 
{x 
E 
1.3y 
Emin 
X 
y 
!;;; 
x} 
U 
yj. 
yEminX 
By 
Higman's 
lemma, 
this 
is 
a 
finite 
union 
of 
sets 
of 
the 
form 
y 
j. 
Since 
a 
finite 
union 
of 
regular 
sets 
is 
regular, 
it 
suffices 
to 
show 
that 
any 
y 
j 
is 
regular. 
But 
ala2'" 
an 
j 
= 
59. 
To 
construct 
a 
DFA 
from 
an 
AFA, 
let 
A 
= 
(QA, 
8A, 
FA, 
Q:A) 
be 
the 
given 
AFA, 
IQ 
AI 
= 
k. 
Let 
Q 
D 
be 
the 
set 
of 
all 
functions 
Q 
A 
--+ 
{O, 
I}. 
Define 
the 
DFA 
D 
= 
(QD, 
8D, 
F
D
, 
SD), 
where 
DD(u,a)(q) 
= 
8A(q,a)(u), 
F
D 
= 
Q:A, 
SD 
= 
FA. 
To 
construct 
an 
AFA 
from 
a 
DFA, 
let 
D 
= 
(QD, 
DD, 
F
D
, 
SD) 
(4) 
(5) 
(6) 
be 
the 
given 
DFA, 
IQDI 
= 
k. 
Let 
QA 
be 
any 
set 
of 
size 
pogkl 
and 
identify 
each 
element 
of 
Q 
D 
with 
a 
distinct 
function 
Q 
A 
--+ 
{O, 
I}. 
Define 
the 
AFA 
A 
= 
(QA, 
DA, 
FA, 
Q:A), 
where 
DA, 
FA, 
and 
Q:A 
are 
defined 
such 
that 
(4), (5), 
and 
(6) 
hold. 
(For 
u 
(j. 
QD, 
define 
8
A
(q,a)(u) 
arbitrarily.) 
In 
both 
reductions, 
one 
can 
show 
by 
induction 
on 
lxi 
that 
for 
any 
q 
E 
QA, 
U 
E 
QD, 
and 
x 
E 
8D( 
u, 
x 
)(q) 
= 
8
A
(q, 
rev 
x)( 
u). 
In 
particular, 
xE 
L(D) 
<==> 
F
D
(8
D
(sD, 
x)) 
= 
1 

_____________________________________________
Solutions 
to 
Selected 
Miscellaneous 
Exercises 
367 
-<===> 
(XA(8D(F
A
,x)) 
= 
1 
-<===> 
(XA(,Xq.(8
D
(F
A
,x)(q))) 
= 
1 
-<===> 
x)(F
A
))) 
= 
1 
-<===> 
rev 
x 
E 
L(A). 
77. 
The 
set 
generated 
is 
{a, 
b} 
*. 
We 
can 
prove 
this 
by 
induction 
on 
string 
length. 
The 
null 
string 
is 
generated 
in 
one 
step. 
For 
nonnull 
strings, 
ther 
the 
string 
begins 
with 
b, 
in 
which 
case 
we 
use 
the 
first 
production; 
or 
ends 
with 
a, 
in 
which 
case 
we 
use 
the 
second 
production; 
or 
neither, 
in 
which 
case 
we 
use 
the 
third 
production. 
In 
any 
case 
the 
induction 
hypothesis 
implies 
that 
the 
rest 
of 
the 
string 
can 
be 
generated. 
78. 
Let 
P 
be 
a 
PDA 
for 
Land 
M 
a 
DFA 
for 
R. 
Build 
a 
PDA 
for 
LI 
R 
that 
on 
input 
x 
scans 
x 
and 
simulates 
P, 
then 
when 
it 
comes 
to 
the 
end 
of 
the 
input 
x, 
guesses 
the 
string 
y 
and 
continues 
to 
simulate 
P 
(from 
the 
same 
configuration 
where 
it 
left 
off) 
but 
also 
runs 
M 
simultaneously 
on 
the 
guessed 
y 
starting 
from 
the 
start 
state. 
It 
accepts 
if 
both 
Land 
M 
accept. 
Thus 
it 
accepts 
its 
original 
input 
x 
if 
it 
was 
successfully 
able 
to 
guess 
astring 
y 
such 
that 
xy 
E 
L(P) 
and 
y 
E 
L(M); 
that 
is, 
if 
there 
exists 
y 
such 
that 
xy 
E 
Land 
y 
E 
R. 
Here 
is 
an 
alternative 
proof 
using 
homomorphisms. 
Suppose 
the 
phabet 
is 
{a, 
b}. 
Let 
{a', 
b'} 
be 
another 
copy 
of 
the 
alphabet 
disjoint 
from 
{a, 
b}. 
Let 
h 
be 
the 
homomorphism 
that 
erases 
marks; 
that 
is, 
h(a) 
= 
h(a') 
= 
a 
and 
h(b) 
= 
h(b') 
= 
b. 
Let 
9 
be 
the 
homomorphism 
that 
erases 
the 
unmarked 
symbols 
and 
erases 
the 
marks 
on 
the 
marked 
symbols; 
that 
is, 
g(a) 
= 
g(b) 
= 
t, 
g(a') 
= 
a, 
g(b') 
= 
b. 
Then 
LIR 
= 
g(h-1(L) 
n 
{a',b'}* 
R). 
(7) 
This 
is 
a 
CFL, 
since 
CFLs 
are 
closed 
under 
homomorphic 
preimage, 
intersection 
with 
regular 
set, 
and 
homomorphic 
image. 
To 
see 
(7), 
first 
consider 
the 
set 
h-1(L). 
This 
is 
thc 
set 
of 
all 
strings 
that 
look 
like 
strings 
in 
L, 
except 
that 
some 
of 
the 
symbols 
are 
marked. 
Now 
intersect 
with 
the 
regular 
set 
{a',b'}*R. 
This 
gives 
the 
set 
of 
strings 
of 
the 
form 
x'y 
su.ch 
that 
the 
symbols 
of 
x' 
are 
marked, 
those 
of 
y 
are 
unmarked, 
xy 
E 
L, 
and 
y 
E 
R. 
Now 
apply 
9 
to 
this 
set 
of 
strings. 
Applied 
to 
astring 
x'y 
as 
described 
above, 
we 
would 
get 
x. 
Therefore, 
the 
resulting 
set 
is 
the 
set 
of 
all 
x 
such 
that 
there 
exists 
y 
such 
that 
x'y 
E 
h-1(L) 
n 
{a',b'}* 
R; 
in 
other 
words, 
such 
that 
xy 
E 
L 
and 
y 
E 
R. 
This 
is 
LI 
R. 

_____________________________________________
368 
Solutions 
to 
Selected 
Miscellaneous 
Exercises 
99. 
(a) 
A 
queue 
machine 
is 
a 
sextuple 
M 
= 
(Q, 
r, 
$, 
s, 
6), 
where 
Ł Q 
is 
a 
finitE: 
set 
of 
states, 
Ł 
is 
a 
finite 
input 
alphabet, 
Ł r 
is 
a 
finite 
queue 
alphabet, 
Ł $ 
E 
r -
is 
the 
initial 
queue 
symbol, 
Ł s 
E 
Q 
is 
the 
start 
state, 
and 
Ł 
8: 
Q 
x 
r 
Q 
x 
r* 
is 
the 
transition 
function. 
A 
configuration 
is 
a 
pair 
(q, 
1') 
E 
Q 
x 
r* 
giving 
the 
current 
state 
and 
current 
contents 
of 
the 
queue. 
The 
start 
configuration 
on 
input 
x 
is 
the 
pair 
(s,x$). 
The 
next 
configuration 
relation 
M 
is 
defined 
as 
follows: 
if 
8(p,A) 
= 
(q,1'), 
then 
(p,Aa) 
(q,a1'). 
M 
The 
relation 
.....:.. 
is 
the 
reflexive 
transitive 
closure 
of 
An 
M 
M 
accept 
configuration 
is 
any 
configuration 
of 
the 
form 
(q, 
E), 
where 
q 
E 
Q 
and 
E 
is 
the 
null 
string. 
The 
queue 
machine 
M 
is 
said 
to 
accept 
x 
E 
if 
(s,x$)""':" 
(q,E) 
M 
for 
some 
q 
E 
Q. 
(b) 
To 
simulate 
a 
queue 
machine 
U 
on 
a 
Turing 
machine 
T, 
let 
T 
maintain 
the 
contents 
of 
U's 
queue 
on 
its 
tape 
and 
simulate 
the 
action 
of 
U, 
shuttling 
back 
and 
forth 
from 
the 
front 
to 
the 
back 
of 
the 
simulated 
queue. 
Each 
simulated 
step 
of 
U 
consists 
of 
T 
moving 
to 
the 
front 
of 
the 
simulated 
queue, 
erasing 
the 
first 
symbol 
and 
remembering 
it 
in 
the 
finite 
control, 
then 
moving 
to 
the 
back 
of 
the 
simulated 
queue 
to 
write 
symbols. 
The 
simulated 
queue 
migrates 
to 
the 
right 
on 
T's 
tape, 
but 
that's 
okay, 
because 
there's 
plenty 
of 
room. 
The 
machine 
T 
accepts 
if 
üs 
tape 
ever 
becomes 
completely 
blank, 
which 
indicates 
that 
the 
simulated 
queue 
of 
U 
is 
empty. 
The 
simulation 
in 
the 
other 
direction 
is 
much 
harder. 
Given 
a 
Turing 
machine 
T, 
we 
build 
a 
queue 
machine 
U 
that 
simulates 

_____________________________________________
Solutions 
to 
Selected 
Miscellaneous 
Exercises 
369 
moves 
of 
T, 
using 
the 
queue 
to 
maintain 
a 
description 
of 
T's 
current 
configuration. 
We 
will 
represent 
configurations 
by 
strings 
such 
as 
a b a a b a q b b a $ 
for 
example; 
exact1y 
one 
of 
the 
symbols 
is 
astate 
of 
T 
(q 
in 
this 
example) 
, 
and 
its 
position 
in 
the 
string 
indicates 
th'e 
Rosition 
of 
the 
tape 
head 
of 
T, 
which 
is 
immediately 
to 
the 
right 
of 
the 
state. 
If 
the 
state 
is 
just 
before 
the 
$, 
this 
models 
T 
scanning 
a 
blank 
cell 
to 
the 
right 
of 
the 
portion 
of 
its 
tape 
represented 
in 
the 
configuration. 
The 
queue 
machine 
U 
will 
also 
have 
an 
"internal 
queue" 
that 
will 
hold 
two 
symbols 
of 
this 
configuration; 
since 
this 
is 
only 
a 
finite 
amount 
of 
information-, 
the 
internal 
queue 
can 
be 
encoded 
in 
the 
finite 
control 
of 
U. 
The 
remaining 
portion 
of 
the 
configuration 
will 
be 
held 
in 
the 
external 
queue. 
Since 
configurations 
have 
at 
least 
three 
symbols 
(state, 
left 
endmarker, 
$), 
even 
if 
the 
tape 
is 
blank, 
the 
external 
queue 
will 
never 
be 
prematurely 
emptied. 
Let 
x 
be 
the 
input 
string. 
The 
queue 
machine 
U 
starts 
with 
x$ 
in 
its 
external 
queue. 
It 
enters 
astate 
representing 
an 
internat 
queue 
of 
s 
The 
internal 
and 
external 
queues 
concatenated 
together 
sent 
the 
start 
configuration 
of 
T 
on 
input 
x: 
s 
x 
$ 
A 
rotate 
operation 
consists 
of 
rotating 
the 
symbols 
on 
the 
ternal 
and 
externat 
queues 
as 
follows: 
internat 
queue 
externat 
queue 
C 
___ 
b_a_ 
..... 
Formatly, 
this 
is 
done 
by 
popping 
the 
first 
element 
off 
the 
front 
of 
the 
externat 
queue 
and 
pushingthe 
front 
element 
of 
the 
internat 
queue 
onto 
the 
back 
of 
the 
external 
queue, 
then 
changing 
state 
to 
refiect 
the 
new 
contents 
of 
the 
internat 
queue. 
In 
this 
example, 
after 
one 
rotation 
we 
would 
have 
a a 
b 
a 
q 
b b 
$ 
a a 
b b 
on 
the 
internat 
and 
externat 
queues, 
respectively. 

_____________________________________________
370 
Solutions 
to 
Selected 
Miscellaneous 
Exercises 
113. 
Let 
We 
simulate 
a 
step 
of 
T 
as 
follows. 
If 
the 
rightmost 
symbol 
of 
the 
internal 
queue 
is 
not 
astate 
of 
T, 
we 
rotate 
until 
this 
becomes 
. 
true. 
In 
this 
example, 
after 
three 
more 
steps 
we 
would 
have 
a 
q 
b b 
$ 
a a b b a a b 
Now 
we 
have 
the 
symbol 
that 
T 
is 
scanning 
(say 
b) 
at 
the 
head 
of 
the 
external 
queue 
and 
the 
current 
state 
of 
T 
(say 
q) 
rightmost 
on 
the 
internal 
queue. 
We 
read 
b 
at 
the 
head 
of 
the 
queue. 
If 
b 
is 
not 
$, 
we 
simulate 
a 
move 
of 
T 
as 
follows 
. 
Ł 
If 
b
T 
(q, 
b) 
= 
(p, 
c, 
R) 
and 
the 
leftmost 
symbol 
of 
the 
internal 
queue 
is 
d, 
we 
push 
d 
onto 
the 
back 
of 
the 
external 
queue 
and 
make 
(c 
p) 
the 
new 
internal 
queue 
. 
Ł 
If 
8
T
( 
q, 
b) 
= 
(p, 
c, 
L) 
and 
the 
leftmost 
symbol 
of 
the 
internal 
queue 
is 
d, 
we 
push 
p 
onto 
the 
back 
of 
the 
external 
queue 
and 
make 
(d 
c) 
the 
new 
internal 
queue. 
In 
the 
example 
above, 
this 
would 
give 
a 
p 
b 
$ 
a a b b a a b a 
if 
bT(q,b) 
= 
(p,a,R), 
and 
aa 
if 
b
T 
(q, 
b) 
= 
(p, 
a, 
L). 
If 
the 
symbol 
at 
the 
head 
of 
the 
external 
queue 
is 
$, 
then 
this 
indicates 
that 
the 
tape 
head 
of 
T 
is 
scanning 
a 
blank 
symbol 
to 
the 
right 
of 
the 
portion 
of 
the 
tape 
represented 
in 
the 
configuration. 
For 
example, 
b q 
$ 
a a b b b a a b a b 
In 
this 
case, 
when 
we 
pop 
$ 
we 
don't 
simulate 
a 
move 
of 
T 
mediatelYi 
first 
we 
insert 
an 
extra 
blank 
symbol 
u 
between 
the 
state 
and 
the 
$. 
We 
do 
this 
by 
pushing 
both 
symbols 
in 
the 
ternal 
queue 
onto 
the 
back 
of 
the 
external 
queue 
and 
making 
the 
new 
internal 
queue 
(u 
$). 
In 
this 
example, 
the 
resulting 
queue 
contents 
would 
be 
u 
$ 
a a b b b a a b a b b q 
We 
continue 
to.simulate 
moves 
of 
T. 
If 
T 
ever 
enters 
its 
accept 
state, 
then 
U 
goes 
into 
a 
little 
subroutine 
that 
just 
empties 
its 
extern 
al 
queue, 
thereby 
accepting. 
REG 
= 
{M 
I 
L(M) 
is 
regular}. 

_____________________________________________
Solutions 
to 
Selected 
Miscellaneous 
Exercises 
371 
A 
corollary 
of 
Rice's 
theorem 
for 
r.e. 
sets 
(Theorem 
34.2) 
is 
that 
any 
semidecidable 
property 
of 
the 
r.e. 
sets 
is 
monotone. 
That 
is, 
if 
P 
is 
a 
property 
of 
r.e. 
sets 
such 
that 
{M 
I 
P(L(M))} 
is 
r.e., 
A 
and 
Bare 
r.e. 
sets, 
A 
B, 
and 
P(A), 
then 
P(B). 
Applying 
this 
corollary 
with 
P( 
C) 
= 
"C 
is 
regular," 
A 
= 
0, 
and 
B 
= 
{anb
n 
I 
n 
O} 
gives 
immediately 
that 
REG 
is 
not 
r.e. 
Simil1!-rly, 
applying 
the 
corollary 
with 
P( 
C) 
= 
"C 
is 
not 
regular," 
A 
= 
{anb
n 
I 
n 
O}, 
and 
B 
= 
E* 
gives 
immediately 
that 
REG 
is 
not 
co-r.e. 

_____________________________________________
References 
[1) 
W. 
ACKERMANN, 
Zum 
Hilbertschen 
Aufbau 
der 
reellen 
Zahlen, 
Math. 
Annalen, 
99 
(1928), 
PP: 
118-133. 
[2) 
A.V. 
AHO 
AND 
J.D. 
ULLMAN, 
The 
Theory 
of 
Parsing, 
Translation, 
and 
Compiling, 
Vol. 
I: 
Parsing, 
Prentice 
Hall, 
Englewood 
Cliffs, 
NJ, 
1972. 
[3) 
--, 
The 
Theory 
of 
Parsing, 
Translation, 
and 
Compiling, 
Vol. 
II: 
Compiling, 
Prentice 
Hall, 
Englewood 
Cliffs, 
NJ, 
1973. 
[4) 
--, 
Principles 
of 
Compiler 
Design, 
Addison-Wesley, 
Reading, 
MA, 
1977. 
[5) 
M.A. 
ARBIB 
AND 
Y. 
GIVE'ON, 
Algebra 
automata 
I: 
Parallel 
programming 
as 
a 
prolegomenon 
to 
the 
categorical 
approach, 
Information 
and 
Control, 
12 
(1968), 
pp. 
331-345. 
[6) 
R.C. 
BACKHOUSE, 
Cloaure 
Algorithms 
and 
the 
Star-Height 
Problem 
of 
Regular 
Languages, 
Ph.D. 
thesis, 
Imperial 
College, 
London, 
1975. 
[7) 
J. 
W. 
BACKUS, 
The 
syntax 
and 
semantics 
of 
the 
proposed 
international 
gebraic 
language 
of 
the 
Zürich 
ACM-GAMM 
conference, 
in 
Proc. 
Intl. 
Conf. 
Information 
Processing, 
UNESCO, 
1959, 
pp. 
125-132. 
[8) 
Y. 
BAR-HILLEL, 
M. 
PERLES, 
AND 
E. 
SHAMIR, 
On 
formal 
properties 
of 
ple 
phrase 
structure 
grammars, 
Z. 
Phonetik. 
Sprachwiss. 
K 
forsch., 
14 
(1961), 
pp. 
143-172. 
[9) 
H.P. 
BARENDREGT, 
The 
Lambda 
Calculus, 
North-Holland, 
Amsterdam, 
1984. 
(10) 
S.L. 
BLOOM 
AND 
Z. 
ESIK, 
Equational 
axioms 
for 
regular 
sets, 
Mathematical 
Structures 
in 
Computer 
Science, 
3 
(1993), 
pp. 
1-24. 
[11) 
M. 
BOFFA, 
Une 
remarque 
sur 
les 
systemes 
complets 
d'identites 
tionnelles, 
Informatiquf 
Theoretique 
et 
ApplicationsjTheoretical 
Informatics 
and 
Application.s, 
24 
(1990), 
pp. 
419-423. 

_____________________________________________
374 
References 
[12J 
--, 
Une 
condition 
impliquant 
toutes 
les 
identites 
rationnelles, 
tique 
Theoretique 
et 
ApplicationsjTheoretical 
Informatic& 
and 
Application&, 
29 
(1995), 
pp. 
515-518. 
[13J 
W.S. 
Tree 
Generating 
Systems 
and 
Tree 
Automata, 
Ph.D. 
thesis. 
Purdue 
University, 
Indiana, 
1967. 
[14J 
--, 
The 
minimalization 
of 
tree 
automata, 
Information 
and 
Control, 
13 
(1968), 
pp. 
484-491. 
[15] 
W.S. 
BRAINERD 
AND 
L.H. 
LANDWEBER, 
Theory 
0/ 
Computation, 
John 
Wiley, 
New 
York, 
1974. 
[16] 
G. 
CANTOR, 
Über 
eine 
Eigenschaft 
des 
Inbegriffes 
aller 
reellen 
algebraischen 
Zahlen, 
J. 
für 
die 
reine 
und 
angewandte 
Mathematik, 
77 
(1874), 
pp. 
258-
262. 
Reprinted 
in 
Georg 
Cantor 
Gesammelte 
Abhandlungen, 
Berlin, 
Verlag, 
1932, 
pp. 
115-118. 
[17J 
N. 
CHOMSKY, 
Three 
models 
for 
the 
description 
of 
languages, 
IRE 
Tran&. 
Information 
Theory, 
2 
(1956), 
pp. 
113-124. 
[18] 
--, 
On 
certain 
formal 
properties 
of 
grammars, 
Information 
and 
Control, 
2 
(1959), 
pp. 
137-167. 
[19] 
--, 
Context-free 
grammars 
and 
pushdown 
storage, 
Tech. 
Rep., 
MIT 
Research 
Lab. 
in 
Electronics, 
Cambridge, 
MA, 
1962. 
[20) 
--, 
Formal 
properties 
of 
grammars, 
Handbook 
of 
Math. 
Psych., 
2 
(\963), 
pp. 
323-418. 
[21J 
N. 
CHOMSKY 
AND 
G.A. 
MILLER, 
Finite 
state 
languages, 
Information 
and 
Control,1 
(1958), 
pp. 
91-112. 
[22] 
N. 
CIIOMSKY 
AND 
M.P. 
SCHÜTZENBERGER, 
The 
algebraic 
theory 
of 
context 
free 
languages, 
in 
Computer 
Programming 
and 
Formal 
Sydem&, 
P. 
Braffort 
and 
D. 
eds., 
North-Holland, 
Amsterdam, 
1963, 
pp. 
118-161. 
(23) 
A. 
CHURCH, 
A 
set 
of 
postulates 
for 
the 
foundation 
of 
logic, 
Ann. 
Math., 
33-34 
(1933), 
pp. 
346-366,839-864. 
[24J 
--, 
A 
note 
on 
the 
Entscheidungsproblem, 
J. 
Symbolic 
Logic, 
58 
(1936), 
pp. 
345-363. 
[25) 
--, 
An 
unsolvable 
problem 
of 
elementary 
number 
theory, 
Amer. 
J. 
Math., 
58 
(1936), 
pp. 
345-363. 
[26) 
--, 
The 
calculi 
of 
lambda-conversion, 
Ann. 
Math. 
Studies,
6 
(1941). 
[27) 
J.H. 
CONWAY, 
Regular 
Algebra 
and 
Finite 
Machine", 
Chapman 
and 
Hall, 
London, 
1971. 
[28] 
S.A. 
COOK, 
The 
complexity 
of 
theorem 
proving 
procedures, 
in 
Proc. 
Third 
Symp. 
Theory 
of 
Computing, 
Assoe. 
Comput. 
Mach., 
New 
York, 
1971, 
pp. 
151-158. 
[29) 
H.B. 
CURRY, 
An 
analysis 
of 
logical 
substitution, 
Amer. 
J. 
Math., 
51 
(1929), 
pp. 
363-384. 
[30) 
N.J. 
Cl'TLAND, 
Computability, 
Cambridge 
University, 
Cambridge, 
1980. 

_____________________________________________
References 
375 
[31] 
M. 
DAVIS, 
Computability 
and 
Unsolvability, 
McGraw-Hill, 
New 
York, 
1958. 
[32] 
--, 
The 
Undecidable, 
Raven 
Press, 
Hewlitt, 
NY, 
1965. 
[33] 
A. 
EHRENFEUCHT, 
R. 
PARIKH, 
AND 
G. 
ROZENBERG, 
Pumping 
lemmas 
and 
regular 
sets, 
SIAM 
J. 
Computing, 
10 
(1981), 
pp. 
536-54l. 
[34] 
S. 
EILEN 
BERG 
AND 
J 
.B. 
WRIGHT, 
Automata 
in 
general 
algebra, 
Information 
and 
Control, 
11 
(1967), 
pp. 
452-470. 
[35] 
J. 
ENGI::LFRIET, 
Tree 
automata 
and 
tree 
grammars, 
Tech. 
Rep. 
DAIMI 
FN'-
10, 
Aarhus 
University, 
Aarhus, 
Denmark, 
1975. 
[36] 
J. 
EVEY, 
Application 
of 
pushdown 
store 
machines, 
in 
Pror;. 
Fall 
Joint 
Computer 
Conf., 
AFIPS 
Press, 
Montvale, 
NJ, 
1963, 
pp. 
215-2r17. 
[37] 
p.e. 
FISCHER, 
On 
computability 
by 
certain 
classes 
of 
restrictoo 
Turing 
chines, 
in 
Proc. 
Fourth 
Symp. 
Swztching 
Circuit 
Theory 
and 
Logical 
Design, 
1963, 
pp. 
23-32. 
[38] 
--, 
Turing 
machines 
with 
restricted 
memory 
access, 
Information 
and 
Control, 
9 
(1966), 
pp. 
364-379. 
[39] 
p.e. 
FISCHER, 
A.R. 
MEYER. 
AND 
A.L. 
ROSENBERG, 
Counter 
machines 
and 
counter 
languages, 
Math. 
Systems 
Theory, 
2 
(1968), 
pp. 
265-283. 
[40] 
M.R. 
GAREY 
AND 
D.S. 
JOl/\SON, 
Computers 
and 
Intractibility: 
A 
Guide 
to 
the 
Theory 
of 
NP-Completeness, 
W.H. 
Freeman, 
New 
York, 
1979. 
[41] 
F. 
GECSEG 
AND 
I. 
PEAK, 
Algebraic 
Theory 
of 
Automata, 
Akademiai 
Kiad6, 
Budapest, 
1972. 
[42] 
F. 
Gf:CSEG 
'\\0 
STEINUY, 
Tree 
Automata, 
Akademiai 
Kiad6, 
Budapest, 
1984. 
[43] 
S. 
GINSBURG, 
The 
Mathematical 
Theory 
of 
Context·Free 
Languages, 
Hill, 
New 
York, 
1966. 
[44] 
S. 
GINSBCRG 
ANO 
S.A. 
GREIBACH, 
Deterministic 
context-free 
languages, 
Information 
and 
Control, 
9 
(1966), 
pp. 
563-582. 
[45] 
S. 
GINSBURG 
A:\O 
H.G. 
RICE, 
Two 
familiesoflanguages 
related 
to 
ALGOL, 
J. 
Assoc. 
Comput. 
Mach., 
9 
(1962), 
pp. 
350-37l. 
[46] 
S. 
GINSBURG 
ANO 
G.F. 
ROSE, 
Operations 
which 
preserve 
definability 
in 
languages, 
J. 
Assoc. 
Comput. 
Mach., 
10 
(1963), 
pp. 
175-195. 
[47] 
--, 
Some 
recursively 
unsolvable 
problems 
in 
J. 
Assoc. 
Comput. 
Mach., 
10 
(1963), 
pp. 
29-47. 
[48] 
--, 
Preservation 
of 
languages 
by 
transducers, 
Information 
and 
Control, 
9 
(1966), 
pp. 
153-176. 
[49] 
S. 
GINSBURG 
AND 
E.H. 
SPANIER, 
Quotients 
of 
context-free 
languages, 
J. 
Assoc. 
Comput. 
Mac·h., 
10 
(1963), 
pp. 
487-492. 
[50] 
K. 
GÖDEL, 
Über 
formal 
unentscheidbare 
Sätze 
der 
Principia 
ica 
und 
verwandter 
Systeme 
I, 
Monatshefte 
für 
Mathematik 
und 
Physik, 
38 
(1931), 
pp. 
173-198. 

_____________________________________________
376 
References 
[51) 
--, 
On 
undecidable 
propositions 
of 
formal 
mathematical 
systems, 
in 
The 
Undecidable, 
M. 
Davis, 
ed., 
Raven 
Press, 
Hewlitt, 
NY, 
1965, 
pp. 
5-38. 
[52) 
J. 
GOLDSTINE, 
A 
simplified 
proof 
of 
Parikh's 
theorem, 
DiJcrete 
Math., 
19 
(1977), 
pp. 
235-240. 
[53) 
S.A. 
GREIBACH, 
A 
new 
normal 
form 
for 
context-free 
phrase 
structure 
grammars, 
J. 
AJJOc. 
Compv.t. 
Mach., 
12 
(1965), 
pp. 
42-52. 
[54) 
L. 
HAINES, 
Generation 
and 
recognition 
of 
formal 
langv.ageJ, 
Ph.D. 
thesis, 
MIT, 
Cambridge, 
MA, 
1965. 
[55) 
M.A. 
HARRISON, 
Introdv.ction 
to 
Formal 
Langv.age 
Theory, 
Addison-Wesley, 
Reading, 
MA, 
1978. 
[56) 
J. 
HARTMANIS 
AND 
J 
.E. 
HOPCROFT, 
Structure 
of 
undecidable 
problems 
in 
automata 
theory, 
in 
Proc. 
Ninth 
Symp. 
Switching 
and 
Automata 
Theory, 
IEEE, 
1968, 
pp. 
327-333. 
[57] 
J. 
HARTMANIS 
AND 
R.E. 
STEARNS, 
On 
the 
complexity 
of 
algorithms, 
1TanJ. 
Amer. 
Soc., 
117 
(1965), 
pp. 
285-306. 
[58) 
F.C. 
HENNIE, 
Introdv.ction 
to 
Computability, 
Addison-Wesley, 
Reading, 
MA, 
1977. 
[59) 
J.E. 
HOPCROFT, 
An 
nlogn 
algorithm 
for 
minimizing 
the 
states 
in 
a 
finite 
automaton, 
in 
Tlfe 
Theory 
of 
MachineJ 
and 
Compv.tation, 
Z. 
Kohavi, 
ed., 
Academic 
Press, 
New 
York, 
1971, 
pp. 
189-196. 
[60) 
J.E. 
HOPCROFT 
AND 
J.D. 
ULLMAN, 
Introdv.ction 
to 
Av.tomata 
Theory, 
Langv.ageJ, 
and 
Compv.tation, 
Addison-Wesley, 
Reaepng, 
MA, 
1979. 
[61] 
D.A. 
HUFFMAN, 
The 
synthesis 
of 
sequential 
switching 
circuits, 
J. 
Franklin 
In"Utv.te, 
257 
(1954), 
pp. 
161-190,275-303. 
[62) 
J. 
JAFFE, 
A 
necessary 
and 
sufficient 
pumping 
lemma 
for 
regular 
languages, 
SIGACT 
NewJ, 
10 
(1978), 
pp. 
48-49. 
[63) 
N.p. 
JONES, 
Compv.tability 
Theory: 
An 
Introdv.ction, 
Academic 
Press, 
New 
York, 
1973. 
[64) 
R.M. 
KARP, 
Reducibility 
among 
combinatorial 
problems, 
in 
Complexity 
of 
Compv.ter 
ComputationJ, 
R.E. 
Miller 
and 
J 
.W. 
Thatcher, 
eds., 
Plenum 
Press, 
New 
York, 
1972, 
pp. 
85-103. 
[65] 
T. 
KApAMI, 
An 
efficient 
recognition 
and 
syntax 
algorithm 
for 
context-free 
languages, 
Tech. 
Rep. 
AFCRL-65-758, 
Air 
Force 
Cambridge 
Research 
Lab, 
Bedford, 
MA, 
1965. 
[66J 
S.C. 
KLEENE, 
A 
theory 
of 
positive 
integers 
in 
formallogic, 
Amer. 
J. 
Math., 
57 
(1935), 
pp. 
153-173; 
219-244. 
[67] 
--, 
General 
recursive 
functions 
of 
natural 
numbers, 
Math. 
Annalen, 
112 
(1936), 
pp. 
727-742. 
[68J 
--, 
Recursive 
predicates 
and 
quantifiers, 
1TanJ. 
Amer. 
Math. 
Soc., 
53 
(1943), 
pp. 
41-74. 

_____________________________________________
References 
377 
[69) 
--, 
Introduction 
to 
Metamathematie", 
D. 
van 
Nostrand, 
Princeton, 
NJ, 
1952. 
[70) 
--, 
Representation 
of 
events 
in 
nerve 
nets 
and 
finite 
automata, 
in 
A 
tomata 
Studie", 
C.E. 
Shannon 
and 
J. 
McCarthy, 
eds., 
Princeton 
University 
Press, 
Princeton, 
NJ, 
1956, 
pp. 
3-41. 
[71) 
D.E. 
KNUTH, 
On 
the 
translation 
of 
languages 
from 
left 
to 
right, 
Information 
and 
Control, 
8 
(1965), 
pp. 
607-639. 
[72) 
D.C. 
KOZEN, 
The 
De"ign 
and 
Analy"i" 
0/ 
Algorithm", 
Springer-Verlag, 
New 
York,1991. 
[73) 
--, 
A 
completeness 
theorem 
for 
Kleene 
algebras 
and 
the 
algebra 
of 
regular 
events, 
Infor. 
and 
Comput., 
110 
(1994), 
pp. 
366-390. 
[74) 
D. 
KROß, 
A 
complete 
system 
of 
B-rational 
identities, 
Theoretieal 
Computer 
Seience, 
89 
(1991), 
pp. 
207-343. 
[75) 
W. 
KUICH, 
The 
Kleene 
and 
Parikh 
theorem 
in 
complete 
semirings, 
in 
J'roc. 
14th 
Colloq. 
Automata, 
Languages, 
and 
Programming, 
T. 
Ottmann, 
ed., 
vol. 
267 
of 
Leet. 
Note" 
in 
Comput. 
Sei., 
EATCS, 
Springer-Verlag, 
New 
York, 
1987, 
pp. 
212-225. 
[76) 
W. 
KUICH 
AND 
A. 
SALOMAA, 
Semiring", 
Automata, 
and 
Languages, 
Springer-Verlag, 
Berlin, 
1986. 
[77) 
S.Y. 
KURODA, 
Classes 
oflanguages 
and 
linear 
bounded 
automata, 
tion 
and 
Control, 
7 
(1964), 
pp. 
207-223. 
(78) 
P 
.S. 
LANDWEßER, 
Three 
theorems 
on 
phrase 
structure 
grammars 
of 
type 
1, 
Information 
and 
Control, 
6 
(1963), 
pp. 
131-136. 
(79) 
H.R. 
LEWIS 
AND 
C.H. 
PAPADIMITRIOU, 
Element.5 
of 
the 
Theory 
of 
Computation, 
Prentice 
Hall, 
Englewood 
ClifIs, 
NJ, 
1981. 
[80) 
P.M. 
LEWIS, 
D.J. 
ROSENKRANTZ, 
AND 
R.E. 
STEARNS, 
Compiler 
De"ign 
Theory, 
Addison-
Wesley, 
Reading, 
MA, 
1976. 
[81) 
M. 
MACHTEY 
AND 
P. 
YOUNG, 
An 
Introduction 
to 
the 
General 
Theory 
0/ 
Algorithm", 
North-Holland, 
Amsterdam, 
1978. 
[82) 
Z. 
MANNA, 
Mathematieal 
Theory 
of 
Computation, 
McGraw-Hill, 
New 
York, 
1974. 
[83) 
A.A. 
MARKOV, 
The 
Theory 
of 
Algorithm", 
vol. 
42, 
Trudy 
Math. 
Steklov 
Inst., 
1954. 
English 
translation, 
National 
Science 
Foundation, 
Wöllhington, 
DC,1961. 
[84) 
W.S. 
MCCULLOCH 
AND 
W. 
PITTS, 
A 
logicalcalculus 
ofthe 
ideas 
immanent 
in 
nervous 
activity, 
Bull. 
Mäth. 
Biophy"ics,5 
(1943), 
pp. 
115-143. 
[85) 
R. 
McNAUGHTON 
AND 
H. 
YAMADA, 
Regular 
expressions 
and 
state 
graphs 
for 
automata, 
IEEE 
Tran". 
Electronic 
Computer", 
9 
(1960), 
pp. 
39-47. 
[86) 
A.R. 
MEYER 
AND 
D.M. 
RITCHlE, 
The 
complexity 
ofloop 
programs, 
in 
Proe. 
ACM 
Natl. 
Meeting, 
1967, 
pp. 
465-469. 

_____________________________________________
378 
References 
[87J 
R. 
MILNER, 
Operational 
and 
algebraic 
semantics 
of 
concurrent 
processes, 
in 
Handbook 
0/ 
Theoretical 
Computer 
Science, 
J. 
van 
Leeuwen, 
ed., 
vol. 
B, 
North-Holland, 
Amsterdam, 
1990, 
pp. 
1201-1242. 
[881 
M.L. 
14INSKY, 
Recursive 
unsolvability 
of 
Post's 
problem 
of 
'tag' 
and 
other 
topics 
in 
the 
theory 
of 
Turing 
machines, 
Ann. 
Math., 
74 
(1961), 
pp. 
437-455, 
[89] 
M.L. 
MINSKY 
AND 
S. 
PAPERT, 
Unrecognizable 
sets 
of 
numbers, 
J. 
A""oc. 
Comput. 
Mach., 
13 
(1966), 
pp. 
281-286. 
[90] 
E.F. 
MooRE, 
Gedanken 
experimentl 
on 
sequential 
machines, 
Automata 
Studies, 
(1956), 
pp. 
129-153. 
[91] 
J. 
MYHILL, 
Finite 
automata 
anq 
the 
representation 
of 
events, 
Technieal 
Note 
WADD 
57-624, 
Wright 
Patterson 
AFB, 
Dayton, 
Ohio, 
1957. 
[92] 
--, 
Linear 
bounded 
automata, 
Technical 
Note 
WADD 
60-165, 
Wright 
Patterson 
AFB, 
Dayton, 
Ohio, 
1960. 
[93] 
P. 
NAUR, 
Revised 
report 
on 
the 
algorithmic 
language 
ALGOL 
60, 
Comm. 
Assoc. 
Comput. 
Mach., 
6 
(1963), 
pp. 
1-17. 
[94] 
A. 
NERODE, 
Linear 
automat 
on 
transformations, 
Proc. 
Amer. 
Math. 
Soc., 
9 
(1958), 
pp. 
541-544. 
[95] 
A.G. 
OETTINGER, 
Automatie 
syntactic 
analysis 
and 
the 
pushdown 
store, 
Proc. 
Symposia 
on 
Applied 
Math., 
12 
(1961). 
[96J 
W. 
OGDEN, 
A 
helpful 
result 
for 
proving 
inherent 
ambiguity, 
Math. 
Sy"tem& 
Theory, 
2 
(1968), 
pp. 
191-194. 
[97] 
C.H. 
PAPADIMITRIOU, 
Computational 
Complexity, 
Addison-Wesley, 
Reading, 
MA,1994. 
[98J 
R. 
PARIKH, 
On 
context-free 
languages, 
J. 
A.",oc. 
Comput. 
Mach., 
13 
(1966), 
pp. 
570-581. 
[99] 
E. 
POST, 
Finite 
combinatory 
processes-formulation, 
I, 
J. 
Symbolic 
Logic, 
1 
(1936), 
pp. 
103-105. 
[100] 
--, 
Formal 
reductions 
of 
the 
general 
combinatorial 
decision 
problem, 
Amer. 
J. 
Math., 
65 
(1943), 
pp. 
197-215. 
[101] 
--, 
Recursively 
enumerable 
sets 
of 
positive 
natural 
numbers 
and 
their 
decision 
problems. 
Bull. 
Amer. 
Math. 
Soc., 
50 
(1944), 
pp. 
284-316. 
[102] 
M.a. 
RABIN 
AND 
D.S. 
SCOTT, 
Finite 
automata 
and 
their 
decision 
problems, 
IBM 
J. 
Res. 
Develop., 
3 
(1959), 
pp. 
115-125. 
[103] 
V. 
N. 
REDKO, 
On 
defining 
relations 
for 
the 
algebra 
ofregular 
events, 
Ukrain. 
Mat. 
Z., 
16 
(1964), 
pp. 
120-126. 
In 
Russian. 
[104] 
H.G. 
fuCE, 
Classes 
of 
recursively 
enumerable 
sets 
and 
their 
decision 
problems, 
Trans. 
Amer. 
Math. 
Soc., 
89 
(1953), 
pp. 
25-59. 
[105] 
--, 
On 
completely 
recursively 
enumerable 
classes 
and 
their 
key 
arrays, 
J. 
Symbolic 
Logic, 
21 
(1956), 
pp. 
304-341. 
[106] 
H. 
ROGERS, 
JR., 
Theory 
0/ 
Recursive 
Functions 
and 
Effective 
ity, 
McGraw-Hill, 
New 
York, 
1967. 

_____________________________________________
References 
379 
[107] 
J 
.B. 
ROSSER, 
A 
mathematical 
logic 
without 
variables, 
Ann. 
Math., 
36 
(1935), 
pp. 
127-150. 
[108] 
A. 
SALOMAA, 
Two 
complete 
axiom 
systems 
for 
the 
algebraofregular 
events, 
J. 
A.uoc. 
Comput. 
M.ach., 
13 
(1966), 
pp. 
158-169. 
[109] 
A. 
SALOMAA 
AND 
M. 
Automata 
Theoretic 
A.5pects 
of 
Formal 
Power 
Series, 
Springer· 
Verlag, 
New 
York, 
1978. 
[110] 
S. 
SCHEINBERG, 
Note 
on 
the 
Boolean 
properties 
of 
context-free 
languages, 
Information 
and 
Control, 
3 
(1960), 
pp. 
372-375. 
[111] 
M. 
SCHÖNFINKEL, 
Über 
die 
Bausteine 
der 
mathematiscnen 
Logik, 
Math. 
Annalen, 
92 
(1924), 
pp. 
305-316. 
[112] 
M.P. 
SCHÜTZENBERGER, 
On 
context-free 
languages 
and 
pushdown 
tomata, 
Information 
and 
Control, 
6 
(1963), 
pp. 
246-264. 
[113) 
J.I. 
SEIFERAS 
AND 
R. 
McNAUGHTON, 
Regularity-preserving 
relations, 
Theor. 
Comput. 
Sci., 
2 
(1976), 
pp. 
147-154. 
[114] 
J 
.C. 
SHEPHERDSON, 
The 
reduction 
of 
two-way 
automata 
to 
one-way 
automata, 
IBM 
J. 
Re.,. 
Develop., 
3 
(1959), pp. 
198-200. 
[115) 
J 
.R. 
SHOENFIELD, 
Degrees 
of 
Unsolvability, 
North-Holland, 
Amsterdam, 
1971. 
[116] 
R.I. 
SOARE, 
Recur.,ively 
Enumerable 
Set., 
and 
Degree." 
Springer-Verlag, 
Berlin, 
1987. 
[117) 
D. 
STANAT 
AND 
S. 
WEISS, 
A 
pumping 
theorem 
for 
regular 
languages, 
SIGACT 
New.5, 
14 
(1982), 
pp. 
36-37. 
[118] 
L.J. 
STOCKMEYER, 
The 
polynomial-time 
hierarchy, 
Theor. 
Comput. 
Sei., 
3 
(1976), 
pp. 
1-22. 
[119] 
J.W. 
THATCHER 
AND 
J.B. 
WRIGHT, 
Generalized 
finite 
automata 
theory 
with 
an 
application 
to 
adecision 
problem 
of 
second 
order 
logic, 
Math. 
Sy.,t. 
Theory, 
2 
(1968), 
pp. 
57-81. 
(120) 
A.M. 
TURING, 
On 
computable 
numbers 
with 
an 
application 
to 
the 
dungsproblem, 
Proc. 
London 
Math. 
Soc., 
42 
(1936), 
pp. 
230-265. 
Erratum: 
Ibid., 
43 
(1937), pp. 
544-546. 
[121] 
--, 
Systems 
of 
logic 
based 
on 
ordinals, 
Proc. 
London 
Math. 
Soc., 
42 
(1939), 
pp. 
230-265. 
(122) 
M.Y. 
VARDI, 
A 
note 
on 
the 
reduction 
of 
two-way 
automata 
to 
one-way 
automata, 
Information 
Processing 
Letters, 
30 
(1989), 
pp. 
261-264. 
[123] 
A.N. 
WHITEHEAD 
AND 
B. 
RUSSELL, 
Principia 
Mathematiea, 
Cambridge 
University 
Press, 
1910-1913. 
Three 
volumes. 
[124) 
A. 
YASUHARA, 
Rec1.lrsive 
Function 
Theory 
and 
Logie, 
Academic 
Press, 
New 
York,1971. 
[125] 
D.H. 
YOUNGER, 
Recognition 
and 
parsing 
of 
context-free 
languages 
in 
time 
n
3
, 
Information 
and 
Control, 
10 
(1967), 
pp. 
189-208. 

_____________________________________________
Notation 
and 
Abbreviations 
subset 
............................................... 
7 
N 
natural 
numbers 
..................................... 
8 
E 
finite 
alphabet 
...................................... 
8 
a, 
b, 
. . . 
letters 
............................................... 
8 
x, 
y,... 
strings 
.................... 
'" 
., 
..................... 
8 
lxi 
length 
of 
string 
x 
.... 
; .
.. 
-
............................ 
8 
f 
null 
string 
............. 
;' 
............................. 
8 
E 
set 
containment 
..................................... 
8 
an 
string 
of 
n 
a's 
.
...................................... 
8 
E* 
set 
of 
strings 
over 
E 
................................. 
8 
{x 
I 
P(x)} 
set 
of 
all 
x 
satisfying 
property 
P 
..................... 
8 
o 
emptyset 
............................................ 
8 
x
n 
concatenation 
of 
n 
x's 
.
...... 
" 
... 
" 
., 
............... 
9 
#a( 
x) 
number 
of 
a's 
in 
x .
.................................. 
9 
A, 
B, 
. . . 
sets 
of 
strings 
...................................... 
10 
lAI 
cardinality 
of 
set 
A 
..
....................... 
" 
...... 
10 
U 
set 
union 
........................................... 
10 

_____________________________________________
382 
Notation 
and 
Abbreviations 
iff 
n 
DFA 
6 
x 
"6 
L(M) 
#x 
a. 
mod 
n 
if 
and 
only 
if 
....................................... 
10 
set 
intersection 
..................................... 
10 
set 
complement 
.................................... 
10 
nth 
power 
of 
A 
..................................... 
11 
asterate 
of 
A .
...................................... 
11 
= 
AA* 
............................................ 
11 
deterministic 
finite 
automaton 
...................... 
15 
transition 
function 
of 
DFA 
......................... 
15 
Cartesian 
product 
.................................. 
15 
extended 
transition 
function 
of 
DFA 
................ 
16 
language 
accepted 
by 
M 
........................... 
17 
number 
represented 
by 
binary 
string 
x .
............. 
20 
remainder 
upon 
dividing 
a 
by 
n 
.................... 
20 
a 
== 
b 
mod 
n a 
congruent 
to 
b 
mod 
n 
............................ 
20 
CI 
P 
---+ 
q 
E 
P 
---+ 
q 
Ot,ß,·· 
. 
L{Ot) 
# 
@ 
+ 
nondeterministic 
finite 
automaton· 
.................. 
26 
start 
states 
of 
NFA 
................................. 
32 
transition 
function 
of 
NFA 
......................... 
32 
power 
set 
of 
Q 
..................................... 
32 
nondeterministic 
transition 
......................... 
32 
extended 
transition 
function 
of 
NFA 
................ 
33 
E-transition 
........................................ 
36 
patterns, 
regular 
expressions 
....................... 
40 
set 
of 
strings 
matching 
pattern 
Ot 
. 
:'.' 
......Ł.Ł...... 
40 
atomic 
pattern 
..................................... 
41 
atomic 
pattern 
..................................... 
41 
atomic 
pattern 
..................................... 
41 
atomic 
pattern 
.................................... 
41 
pattern 
operator 
................................... 
41 

_____________________________________________
n 
+ 
* 
M(n,K:) 
h(A) 
h-
1
(B) 
o 
> 
'" 
Notation 
and 
Abbreviations 
383 
pattern 
operator 
................................... 
41 
pattern 
operator 
................................... 
41 
pattern 
operator 
................................... 
41 
pattern 
oper.ator 
................................... 
41 
pattern 
operator 
................................... 
41 
inclusion 
(regular 
expressions) 
...................... 
50 
regular 
expression 
for 
paths 
from 
u 
to 
v 
through 
X . 
51 
. 
natural 
order 
in 
a 
Kleene 
algebra 
................... 
56 
n 
x 
n 
matrices 
over 
Kleene 
algebra 
K: 
............... 
58 
image 
of 
A 
und 
er 
h 
................................ 
62 
preimage 
of 
B 
under 
h 
............................. 
62 
collapsing 
relation 
.................................. 
80 
equivalence 
class 
of 
p 
............................... 
80 
quotient 
automaton 
................................ 
80 
n 
choose 
k 
......................................... 
85 
Myhill-Nerode 
relation 
induced 
by 
M 
............... 
90 
DFA 
constructed 
from 
Myhill-Nerode 
relation 
== 
... 
91 
Myhill-Nerode 
relation 
constructed 
from 
M=. 
....... 
92 
automaton 
constructed 
from 
==M 
ŁŁŁŁŁŁŁ.Ł.Ł...Ł.... 
92 
maximal 
Myhill-Nerode 
relation 
for 
R 
.............. 
96 
states 
related 
to 
B 
under 
abisimulation 
........... 
101 
bisimulation 
extended 
to 
sets 
...................... 
101 
relational 
composition 
....... 
" 
............... 
'" 
.. 
102 
maximal 
autobisimulation 
......................... 
103 
bisimulation 
between 
an 
NFA 
and 
its 
quotient 
..... 
104 
signature 
......................................... 
108 
ground 
terms 
over 
1: 
.............................. 
109 
interpretation 
of 
function 
symbol 
f .
............... 
110 

_____________________________________________
384 
Notation 
and 
Abbreviations 
=cr 
2DFA 
2NFA 
f-
-i 
o 
1 
--+ 
'" 
n 
--+ 
'" 
Ł 
--+ 
'" 
L(M) 
Ł 
1. 
BNF 
L(G) 
CFL 
PDA 
LIFO 
CFG 
A,B, 
... 
a,b, 
.
.. 
a,ß,· 
.. 
A-.a 
interpretation 
of 
relation 
symbol 
R .
............... 
110 
interpretation 
of 
constant 
c 
........................ 
110 
interpretation 
of 
ground 
term 
t .
................... 
uo 
set 
accepted 
by 
term 
automaton 
.A 
................ 
111 
term 
algebra 
with 
relation 
R 
interpreted 
as 
A 
.... 
113 
kernel 
of 
homomorphism 
a 
........................ 
114 
two-way 
deterministic 
finite 
automaton 
............ 
119 
two-way 
nondeterministic 
finite 
automaton 
........ 
119 
left 
endmarker 
.................................... 
119 
right 
endmarker 
................................... 
119 
transition 
function 
of 
2DFA 
....................... 
120 
next 
configuration 
relation 
of 
2DFA 
on 
input 
x 
.... 
122 
nth 
iterate 
of 
................................ 
122 
'" 
reflexive 
transitive 
closure 
of 
................. 
122 
'" 
set 
accepted 
by 
a 
2DFA 
........................... 
122 
used 
in 
2DFA 
tables 
.............................. 
124 
used 
in 
2DFA 
tables 
.............................. 
124 
Backus-Naur 
form 
................................ 
129 
for 
separating 
alternatives 
in 
a 
CFG 
............... 
130 
langeage 
generated 
by 
CFG 
G 
.................... 
131 
context-free 
language 
.............................. 
13i 
pushdown 
automaton 
............................. 
131 
last-in-first-out 
.................................... 
132 
context-free 
grammar 
............................. 
132 
nonterminals 
of 
a 
CFG 
............................ 
132 
terminals 
of 
a 
CFG 
............................... 
132 
strings 
of 
terminals 
and 
nonterminals 
of 
a 
CFG 
.... 
132 
production 
of 
a 
CFG 
.............................. 
132 

_____________________________________________
1 
--+ 
G 
Ł 
--+ 
G 
L(G) 
PAREN 
L(z) 
R(z) 
CNF 
GNF 
L 
--+ 
G 
NPDA 
1. 
1 
--+ 
M 
n 
--+ 
M 
Ł 
--+ 
M 
DPDA 
-l 
DCFL 
1. 
T 
A 
V 
-
CKY 
PAREN
n 
PARENr 
Notation 
and 
Abbreviations 
385 
derivation 
step 
of 
a 
CFG 
.......................... 
133 
reflexive 
transitive 
closure 
of 
-7; 
................. 
133 
language 
generated 
by 
CFG 
G 
.................... 
133 
balanced 
strings 
of 
parentheses 
.................... 
135 
number 
of 
left 
parentheses 
in 
z 
.................... 
135 
number 
of 
right 
parentheses 
in 
z 
.................. 
135 
Chomsky 
normal 
form 
............................ 
140 
Greibach 
normal 
form 
............................. 
140 
derivation 
used 
in 
Greibach 
normal 
form 
proof 
.... 
144 
nondeterministic 
pushdown 
automaton 
............ 
157 
initial 
stack 
symbol 
............................... 
158 
next 
configuration 
relation 
of 
PDA 
M .
............ 
159 
nth 
iterate 
of 
2.. 
................................ 
160 
M 
reflexive 
transitive 
closure 
of 
2.. 
'" 
.............. 
160 
M 
deterministic 
pushdown 
automaton 
................ 
176 
right 
endmarker 
of 
a 
DPDA 
....................... 
176 
deterministic 
context-free 
language 
................ 
177 
falsity 
............................................. 
181 
truth 
............................................. 
181 
and· 
............................................... 
181 
or 
................................................ 
181 
implies 
............................................ 
181 
if 
and 
only 
if 
...................................... 
181 
negation 
.............Ł............................ 
181 
Cocke-Kasami-Younger 
algorithm 
................. 
191 
balanced 
strings 
of 
parentheses 
of 
n 
types 
......... 
198 
parenthesis 
language 
on 
parentheses 
r 
............. 
199 
Parikh 
map 
....................................... 
201 

_____________________________________________
386 
Notation 
and 
Abbreviations 
TM 
PA 
w 
w 
a,ß,··· 
1 
---+ 
M 
Ł 
---+ 
M 
L(M) 
r.e. 
co-r.e. 
L(E) 
U 
HP 
MP 
FIN 
Turing 
machine 
................................... 
206 
Peano 
arithmetic 
.................................. 
209 
blank 
symbol 
..................................... 
210 
semi-infinite 
string 
of 
blanles 
...................... 
212 
smallest 
infinite 
ordinal. 
. . . . . . . . . . . . 
.. 
. 
.......... 
212 
Turing 
machine 
configurations 
..................... 
212 
next 
configuration 
relation 
of 
Turing 
machine 
M 
.. 
213 
reflexive 
transitive 
closure 
of 
-2... 
................. 
213 
M 
strings 
accepted 
by 
Turing 
machine 
M 
............ 
213 
recursively 
enumerable 
............................ 
213 
complement 
of 
a 
recursively 
enumerable 
set 
........ 
213 
strings 
enumerated 
by 
enumeration 
machine 
E 
.... 
226 
universal 
Turing 
machine 
.......................... 
229 
halting 
problem 
................................... 
230 
membership 
problem 
..............Ł. 
, 
............. 
230 
many-one 
reducibility 
............................. 
240 
finiteness 
problem 
................................. 
241 
VALCOMPS 
valid 
computation 
histories 
........................ 
250 
CSG 
LBA 
s 
z 
o 
cunst
n 
context-sensitive 
grammar 
......................... 
258 
linear 
bounded 
automaton 
........................ 
258 
successor 
function 
................................. 
258 
zero 
function 
...................................... 
258 
projection 
fUlldion 
............... 
'" 
.............. 
258 
functional 
composition 
............................ 
258 
minimization 
operator 
............................. 
259 
constant 
function 
with 
value 
n 
.................... 
259 
proper 
subtraction 
................................ 
260 
reduction 
in 
the 
>.-calculus 
........................ 
263 

_____________________________________________
'" 
--
r 
s 
K 
Env 
lT[X 
+-
a] 
EMPTY 
TOTAL 
FIN 
COF 
L 
Ti 
3 
Th(N) 
f-
1= 
Notation 
and 
Abbreviations 
387 
a-reduction 
............... 
" 
...................... 
264 
ß-reduction 
....................................... 
264 
numeral 
for 
n 
.......... 
'.' 
................. 
265 
n-fold 
composition 
'of 
I 
........................... 
266 
primitive 
combinator 
.............................. 
266 
primitive 
combinator 
.............................. 
266 
reduction 
in 
combinatory 
logic 
.................... 
267 
set 
of 
all 
environments 
............................ 
270 
function 
lT 
with 
value 
of 
x 
changed 
to 
a 
........... 
271 
Turing 
reducibility 
................................ 
275 
r.e. 
sets 
........................................... 
276 
recursive 
sets 
..................................... 
276 
co-r.e. 
sets 
........................................ 
276 
dass 
in 
arithmetic 
hierarchy 
....................... 
276 
dass 
in 
arithmetic 
hierarchy 
....................... 
276 
dass 
in 
arithmetic 
hierarchy 
....................... 
276 
set 
of 
TMs 
accepting 
0 
........................... 
277 
set 
of 
total 
TMs 
.................................. 
277 
set 
of 
TMs 
accepting 
finite 
sets 
.................... 
278 
set 
of 
TMs 
accepting 
cofinite 
sets 
.................. 
278 
hyperarithmetic 
sets 
.............................. 
281 
inductive 
sets 
..................................... 
281 
language 
of 
number 
theory 
........................ 
282 
for 
all 
............................................. 
283 
there 
.exists 
....................................... 
283 
first-order 
number 
theory 
......................... 
284 
provability 
in 
PA 
.................................. 
292 
truth 
............................................. 
292 

_____________________________________________
388 
Notation 
and 
Abbreviations 
integer 
code 
of 
formula 
cp 
Ł
.ŁŁŁŁŁŁŁŁŁŁŁŁŁŁŁŁŁŁŁŁŁŁ.Ł 
292 
ZF 
Zermelo-Fraenkel 
set 
theory 
....................... 
297 
11 
shufHe 
operator 
................................... 
304 
PAREN2 
halanced strings 
of 
parentheses 
of 
two 
types 
....... 
306 
+-
residuation 
operator 
............................... 
310 
f-closure 
of 
A 
............ 
, 
........................ 
318 
00 
V 
for 
all 
hut 
finitely 
many 
........................... 
324 

_____________________________________________
Index 
Abstraction, 
5 
functional, 
263 
A-, 
263 
Accepta,nce, 
17 
2DFA, 
120, 
122 
.by 
empty 
stack, 
160, 
163, 
164 
by 
final 
state, 
160, 
164 
DFA,16 
DPDA,177 
NFA, 
26,33 
NFA 
with 
E-transitions, 
65, 
318 
term 
automaton, 
111 
Turing 
mAchine, 
210, 213, 
216 
Accept 
state, 
15, 
111, 121, 
158, 
210, 
211 
Accessible, 
29, 
75, 77, 78, 
84, 
94, 95, 
103,116 
Ackermann, 
W., 
273 
Ackermann's 
function, 
261, 
273 
Aho, 
A.V., 
190 
Algorithm 
CKY, 
191-195,249,338 
Cocke-Kasami-Younger, 
"ee 
CKY 
collapsing, 
84-88,106-107,327,365 
Euclidean, 
340, 
364 
maximal 
bisimulation, 
106-107 
minimization, 
"ee 
collapsing 
parsing, 
181-190 
primality, 
217-219 
recursive, 
214 
Alphabet, 
8 
single 
letter, 
73, 74, 
318, 
326, 
: 
346 
stack,158 
a:-reduction, 
264 
Alternating 
finite 
automaton, 
330, 
366 
quantifiers, 
see 
quantifiei 
Ambiguous, 
131, 182, 
186, 
339 
Analytic 
hierarchy, 
"ee 
hierarchy 
Annihilator, 
12, 
56 
Antimatter, 
354 
Antisymmetry, 
56, 
96 
Application 
operator, 
267 
Arbib, 
M.A., 
118 
Arithmetic 
hierarchy,."ee 
hierarc 
Arity, 
108 
Assignment 
statement, 
130 
Associativity, 
9, 
11, 
42, 
45, 
55, 
2 
266, 
285 
Asterate, 
11,56 
closure 
of 
CFLs 
under, 
195 
closure 
of 
regular 
sets 
under, 
Atomic 
pattern, 
40, 
41 
Autobisimulation, 
103 
maximal, 
106 
Automaton 
counter, 
224-225,227,341 

_____________________________________________
390 
Index 
Automaton 
(cant.) 
finite, 
4, 
14 
deterministic, 
15, 
24, 
54 
nondeterministic, 
26, 
32, 
39 
two-way, 
119-128,331 
with 
e:-transitions, 
36-37,65, 
318 
linear 
bounded, 
4, 
258, 268, 
309, 
313 
product, 
22 
pushdown, 
4, 
131, 
175 
deterministic, 
163,176-177,191, 
196 
nondeterministic,157 
one-state, 
172 
quotient, 
80 
term, 
108-118 
two-stack, 
224 
Axioms 
Kleene 
algebra, 
55-56 
Peano 
arithmetic, 
284 
Axiom 
scheme, 
285 
Backhouse, 
R.C., 
60 
Backus, 
J.W., 
129, 
134 
Backus-Naur 
form, 
129, 
134 
Balanced 
parentheses, 
131, 
135-140, 
143,145,161,198,304,306, 
307, 
329 
Bar-Hillel, 
Y., 
71, 
147, 156, 197, 
255 
Basic 
pump, 
203 
Basis, 
21 
Begin 
statement, 
130 
ß-reduction, 
264 
Biconditional, 
181 
Binary 
relation, 
110 
representation, 
19, 
207, 224, 
284, 
301,307,323,329 
symbol,108 
Bisimilar, 
101 
Bisimilarity 
class, 
101 
Bisimulation, 
100, 
331 
strong, 
100 
Blank 
symbol, 
210, 
211 
Bloom, 
S.L., 
60 
BNF, 
&ee 
Backus-Naur 
form 
Boffa, 
M., 
60 
Brainerd, 
W.S., 
118, 
268 
Cantor, 
G., 
230, 
243, 
297, 
298 
Capture 
of 
free 
variable, 
264 
Cardinality, 
10, 
208 
Carrit:r, 
110 
Cell, 
119, 
157 
CFL, 
&ee 
context-free 
language 
Ł 
Characteristic 
function, 
275, 
330 
Chomsky 
hierarchy, 
4, 
257, 
268 
normal 
form, 
&ee 
normal 
form 
Chomsky, 
N., 
4, 
54,134,147,175, 
200, 
268 
Chomsky-Schützenberger 
theorem, 
198-200 
Church 
-Rosser 
property, 
265 
-Turing 
thesis, 
&ee 
Church's 
thesis 
numerals, 
265 
Church, 
A., 
4, 
206, 
207, 
214, 265, 
268 
Church's 
thesis, 
207-208, 
214 
CKY 
algorithm, 
191-195, 
197, 
249, 
338 
Closure 
properties 
of 
CFLs, 
154, 155, 
195'-197,308, 
335,367 
of 
DCFLs, 
177-180,196-197,338 
of 
r 
.e. 
sets, 
340 
of 
regular 
sets, 
21-24,37-39,47, 
62,302 
reflexive 
transitive, 
56, 
110, 
160, 
213,315,322 
CNF, 
&ee 
normal 
form 
Coarser,96 
Coarsest, 
97 
Cocke, 
J., 
197 
Cocka-Kasami-
Younger 
algorithm, 
&ee 
CKY 
algorithm 
Cofinite, 
278 
Collapsing, 
77,81 
algorithm, 
84-88, 
98, 
106-107,327, 
365 
DFA,77-99 
NFA, 
100-107 
relation, 
80, 84, 
98, 
100, 
101 
Combinator, 
207, 
266 
Combinatorial 
completeness, 
267 

_____________________________________________
Combinatory 
logic, 
4, 
206, 
207, 256, 
266, 
268 
Commutative 
image, 
202 
Commutativity, 
12, 
55, 
202 
Compiler 
compiler, 
181 
Complement, 
10 
CFLs 
not 
c10sed 
under, 
155, 
196 
c10sure 
of 
DCFLs 
under, 
177-180, 
196 
c10sure 
of 
recursive 
sets 
under, 
219 
c10sure 
of 
regular 
sets 
under, 
23 
r.e. 
sets 
not 
c10sed 
under, 
219 
Completeness 
for 
a 
c1ass 
of 
decision 
problems, 
278-281,349 
combinatorial, 
267 
deductive, 
286 
Complete 
partial 
order, 
339 
Complexity 
theory, 
281 
Composition 
functional, 
240, 
258, 
263, 
271 
relational, 
57, 
101, 110, 
122 
sequential, 
269 
Compound 
pattern, 
40, 
41 
Computability, 
5, 
206, 
207, 
256 
Computable 
function, 
347 
total, 
273, 
347, 
350 
reduction, 
240 
Concatenation 
c10sure 
of 
CFLs 
under, 
195 
c10sure 
of 
regular 
sets 
under, 
37 
set, 
10,37 
string,9 
Qoncurrency, 
100 
Conditional, 
269 
Configuration. 
2DFA,122 
DPDA,177 
PDA,158 
queue 
machine, 
368 
start, 
122, 
159, 
212, 216, 
250 
Tllring 
machine, 
212, 216, 
250 
Congruence, 
114, 
115, 
117, 
202 
clus, 
116 
right, 
90, 
95,97,116 
Consistency, 
296, 
297 
Constant, 
108 
propositional, 
181 
Context, 
116 
Context-free 
Index 
391 
grammar,4, 
129, 
131, 132, 
134 
language,4, 
131,133,134 
deterministic, 
177, 191, 
196 
Context-sensitive 
grammar, 
4, 
258, 
268, 
313 
language,4 
Contradiction, 
68, 
74 
Contrapositive, 
70, 
153 
Conway, 
J 
.H., 
60 
Cook, 
S.A., 
281 
Co-r.e., 
213 
Coset, 
202 
Counter, 
163, 
224 
automaton, 
224-225,227,341 
Curry, 
263 
Curry, 
B.B., 
4, 
206, 263, 
268 
Cyc1e, 
337 
Davis, 
M., 
268 
DCFL, 
"ee 
deterministic 
context-free 
language 
Dead 
code, 
310 
Decidability, 
220, 
235-236 
Decision 
problem, 
7, 
284 
Demon 
game 
for 
CFLs, 153-155 
för 
regular 
sets, 
71-73 
De 
Morgan 
laws, 
12, 
24, 
45 
Denesting 
rule, 
57 
Depth, 
150,203,330 
Derivation 
leftmost, 
149, 
168 
rightmost, 
149 
tree, 
"ee 
parse 
tree 
Determinism, 
176 
Deterministic 
context-free 
language, 
177, 
191, 
191 
finite 
automaton, 
54 
pushdown 
automaton, 
163, 
175-177,191,196 
DFA, 
"ee 
finite 
automaton 
Diagonalization, 
230-234, 
239, 
243 
Direct 
product, 
108 
Disjoint 
pumps, 
205 

_____________________________________________
392 
Index 
Disjoint 
(cont.) 
union, 
48 
Distributivity, 
12, 
56, 
285 
DPDA, 
&ee 
deterministic 
pushdown 
automaton 
Dyck 
language, 
&ee 
parenthesis 
language 
DYLAN,262 
Dynamic 
programming, 
191 
Effective, 
&ee 
computability 
Ehrenfeucht, 
A., 
71 
Eilenberg, 
S., 
118 
Emptiness 
problem 
for 
CFLs, 
249, 
348 
for 
LBAs, 
31Q 
Empty 
string, 
8 
Encoding, 
207, 
292 
of 
configurations, 
250 
of 
natural 
numbers 
-in 
the 
A-ca1culus, 
265 
of 
natural 
numbers 
in 
ZF, 
297 
Endmarker, 
120, 
163, 
176, 196, 
211 
Engelfriet, 
J., 
118 
Enumeration 
machine, 
225-227 
state, 
226 
Environment, 
270 
Epimorphism, 
113, 
116 
f-c1osure, 
37, 
65, 
318 
f-production, 
141-142,145, 
147,334 
f-transition, 
36-37, 
65, 
158, 179, 
318 
Equationallogic, 
50 
Equivalence 
among 
variations 
of 
TMs, 
221-227 
c1ass, 
80 
letter-, 
203 
of 
acceptance 
by 
final 
state 
and 
empty 
stack, 
164-166 
of 
bisimilar 
automata, 
103 
of 
CSGs 
and 
LBAs, 
258, 
268 
ofDFAs 
and 
2DFAs, 
124-128 
NFAs, 
28-36 
NFAs 
with 
f-transitions, 
318 
regular 
expressions, 
46-54, 
59 
right-linear 
grammars, 
54, 
306 
of 
I'-recursive 
functions 
and 
the 
A-calculus, 
265 
ofNPDAs 
and 
CFGs, 
167-175 
ope-state 
NPDAs, 
172 
of 
primitive 
recursive 
functions 
and 
for 
programs, 
273 
ofTMs 
and 
enumeration 
machines, 
225-227 
four-counter 
automata, 
224-225 
A-calculus, 
268 
Post 
systems, 
257 
two-counter 
automata, 
225 
two-stack 
automata, 
224 
type 
0 
grammars, 
258, 
268, 
343 
of 
while 
programs 
and 
I'-recursive 
functions, 
271 
problem, 
310 
relation, 
49, 
80, 
90, 
96 
Erase, 
217 
Esik, 
Z., 
60 
Euclid,363 
Euc1idean 
algorithm, 
340, 
364 
Evaluation, 
262 
Evey, 
J., 
175 
Exponentiation, 
259 
Expression 
regular, 
"ee 
regular 
expression 
tree,182 
Final 
state, 
&ee 
accept 
state 
Finer,96 
Finite 
automaton, 
4, 
14 
alternating, 
330, 
366 
deterministic, 
15, 
24, 
54 
nondeterministic, 
32, 
39 
two-way, 
119-128, 
331 
with 
f-transitions, 
36-37, 
65, 
318 
control, 
211 
index, 
90, 
95, 
116-117 
-state 
transition 
system, 
14, 
24 
Finitely 
generated, 
202 
Finiteness 
problem 
lor 
CFLs, 
250, 
349 
for 
regular 
sets, 
349 
Fischer, 
P.C., 
180, 
227 
Fixpoint, 
293, 
339, 
346, 
347 

_____________________________________________
lemma, 
292 
For 
loop, 
269 
progrBIn, 
269, 
273 
Formalist 
progrBIn, 
5, 
209, 
282 
Formal 
proof 
system, 
282 
Free 
commutative 
monoid, 
202 
monoid,202 
variable, 
284, 
293 
Function 
Ackermann's, 
261, 
273 
characteristic, 
275, 
330 
computable, 
347 
monotone, 
339 
I'-recursive, 
4, 
206, 
214, 
256, 
258-261,268,269,345 
pairing, 
277 
partial, 
270, 273, 
347 
primitive 
recursive, 
259, 269, 273, 
313 
regu1arity 
preserv:ing, 
76, 
324 
transition, 
lee 
transition 
Junction 
weakly 
regu1arity 
preserving, 
324 
Functional° 
abstraction, 
262-264 
application, 
262-264 
composition, 
I!e 
composition 
Garey, 
M., 
281 
GCD, 
8ee 
greatest 
common 
divisor 
Gocseg, 
F.; 
60, 
118 
285 
Generate, 
202, 257, 
258 
Ginsburg, 
S., 
39, 
180,197,255,335 
Give'on, 
Y., 
118 
GNF, 
8ee 
normal 
form 
Gödel, 
K., 
4, 
5, 
206, 207, 209, 
214, 
258, 
268, 
269, 
273, 282, 
287, 
292 
Gödel 
numbering, 
208 
GQdel's 
fixpoint 
lemma, 
lee 
fixpoint 
lemma 
incompleteneu 
theorem, 
lee 
incompleteness 
theorem 
Goldstine, 
J., 
205 
Grammar, 
4; 
257 
context-free, 
4, 
129. 
132, 
134 
Index 
393 
context-sensitive, 
4, 
258, 268, 
313 
left-linear, 
306 
right-linear,4, 
54, 
257,306 
strongly 
left-linear, 
306 
strongly 
right-linear, 
306 
type 
0, 
2M, 
257, 
268 
type 
1, 
257,313 
type 
2, 
257 
type 
3, 
257 
Greatest 
common 
divisor, 
326, 
340, 
364 
Greibach, 
S.A., 
147,180,307,328 
Greibach 
normal 
form, 
.see 
normal 
form 
Ground 
term, 
109 
Guess 
and 
verify, 
26 
Haines, 
L., 
180 
Halting 
2DFA,122 
configuration, 
251 
DPDA, 
177, 
179-180 
LBA,309 
problem, 
230, 
309 
undecidability 
of, 
231-234, 
244 
Turing 
machine, 
213, 
216 
Hamming 
distance, 
65, 
302 
Hardest 
r.e. 
set, 
278 
Hardness, 
278 
Harrison, 
M.A., 
205 
Hartmanis, 
J., 
281 
Head 
read-only, 
119, 157, 
224 
read/write, 
210, 212, 220, 
221 
write-only, 
225 
Hennie, 
F.C., 
268 
Herbrand, 
J., 
4, 
206, 214, 
268 
Hierarchy 
analytic, 
281 
arithmetic, 
276-281 
Eügman'slemma,329,365 
Hilbert, 
D., 
5, 
209, 
282 
EülltOrical 
notes, 
24, 
39, 
54, 
60, 
71, 
76,99,117,128,134,147,156, 
175, 180, 190, 
197, 
200, 205, 
214, 
227, 243, 248, 255, 268, 
273,281 

_____________________________________________
394 
Index 
Homomorphic 
image, 
62, 
76, 
108, 
113, 
117,198,199 
closure 
of 
CFLs 
under, 
196, 
335, 
367 
closure 
of 
regular 
sets 
under, 
62 
minimal, 
117 
Homomorphic 
preimage, 
62 
closure 
of 
CFLs 
under, 
196, 
335, 
367 
closure 
of 
regular 
sets 
under, 
62 
Homomorphism, 
61-66, 
74,76, 
108, 
112, 
114, 
198, 
319 
:E-algebra, 
112 
Hopcroft, 
J.E., 
24,99, 
197 
Horn 
clause, 
348 
Huffman, 
D.A., 
99 
Idempotence, 
56 
Idempotent 
semiring, 
56 
Identity 
element, 
9, 
12, 
56, 
285 
function, 
267 
relation, 
96, 
105, 
110 
If 
statement, 
130 
Image, 
62 
Implication, 
181 
Inaccessible, 
see 
accessible 
Incompleteness 
theoretn, 
5, 
209, 
282-298 
Induction 
axiom, 
285 
hypothesis, 
20 
step,21 
structural, 
47 
Infinite 
string, 
108 
Initial 
stack 
symbol, 
158, 
163 
Input 
string, 
16 
Integer 
division, 
283 
Interactive 
computation, 
208 
Intersection, 
10 
CFLs 
not 
closed 
under, 
196 
closure 
of 
r.e. 
sets 
under, 
340 
closure 
of 
regular 
sets 
under, 
22-23 
of 
CFLs, 
311, 
312 
of 
CFLs 
with 
regular 
sets, 
195, 
308, 
367 
Intuitionism, 
297 
Inverse, 
260 
Isomorphism, 
78, 
100, 105, 
113, 
229 
of automata, 
30, 
89, 
92-95 
Jaffe, 
J., 
71, 
325 
Johnson, 
D., 
281 
Jones, 
N.D., 
268 
Kapur, 
S., 
329 
Karp, 
R.M., 
281 
Kasami, 
T., 
197 
Kernel, 
114, 115, 
117 
Kleene, 
S.C., 
4, 
24, 
54, 55, 
206, 214, 
227,255,281 
Kleene 
algebra, 
49, 
55-60, 
110, 
321, 
322,358 
Knaster-Tarski 
theorem, 
339 
Knuth, 
D.E., 
190 
Kozen, 
D.C., 
60 
Krob, 
D., 
60 
Kuich,W., 
60, 
205 
Kuroda, 
S.Y., 
268 
Labeled 
tree, 
108, 
109, 
111 
A-abstraction, 
263 
A-calculus,4, 
206, 214, 256, 
262-268 
pure, 
263 
A-notation, 
262 
,x-term, 
207, 262, 263, 
345 
Landweber, 
L.H., 
268 
Landweber, 
P.S., 
268 
Language,4 
accepted 
by 
M, 
17 
context-free, 
4, 
131, 133, 
134 
context-sensitive,4 
deterministic 
context-free, 
177, 
191, 
196 
Dyck, 
see 
parenthesis 
language 
generated 
by 
G, 
133 
of 
number 
theory, 
209, 
282, 
288, 
293, 295, 
297 
parenthesis, 
198 
LBA, 
see 
linear 
bounded 
automaton 
Least 
upper 
bound, 
56, 
339 
Leftmost 
derivation, 
149, 
168 
Length, 
8, 
203 
Letter, 
8 
-equivalence, 
203 
Level,149 

_____________________________________________
Lewis, 
P.M., 
190 
LIFO,157 
Linear 
bounded 
automaton, 
4, 
258, 268, 
309,313 
equations, 
59-60 
order, 
355· 
set, 
202 
time, 
349, 
356 
LISP, 
262 
Logic, 
256 
combinatory, 
4, 
206, 
207, 266, 
268 
equational, 
50 
first-order, 
282, 
297 
propositional, 
181 
second-order, 
281 
Loop, 
75 
Looping 
2DFA,122 
DPDA, 
177, 
179-180 
LBA,309 
PDA,161 
Turing 
machine, 
210, 213, 
216 
Machtey, 
M., 
268 
Many-one 
reduction, 
239 
Markov, 
A.A., 
134 
Markov 
system, 
134 
Match, 
40, 
41 
Matching 
parentheses, 
136 
Matrix, 
322, 353, 357, 358, 
361 
over 
a 
Kleene 
algebra, 
58-59 
Maximal 
autobisimulation, 
106 
McCulloch, 
W.S., 
24 
McNaughton, 
R., 
39, 
54, 
76, 
324 
Membership 
problem, 
230, 
278 
undecidability 
of, 
234 
Metastatement, 
295 
Metasymbol, 
229, 
295 
Metasystem, 
295, 
297 
method, 
263 
Meyer, 
A.R., 
227, 
273 
Miller, 
G.A., 
54 
Minimal 
<1-, 
203 
204 
366 
derivation, 
142 
Index 
395 
DFA, 
78, 
89, 
98 
homomorphic 
image, 
117 
NFA, 
100-101,104,331 
Turing 
machine, 
347 
Minimization 
algorithm, 
"ee 
collapsing 
algorithm 
unbounded, 
259, 
273 
Modus 
ponens, 
285 
Monoid, 
9, 
110,201 
free, 
202 
free 
commutative, 
202 
Monomorphism, 
113 
Monotone 
function, 
339 
operator, 
56 
property, 
247,371 
Moore, 
E.F., 
99 
Multiple 
of 
k, 
301 
of 
three, 
19 
I-'-recursive 
function, 
4, 
206, 214, 
256, 
258-261, 
268, 269, 
345 
Myhill, 
J., 
99, 
118, 
268 
Myhill-Nerode 
relation, 
90-95, 
116, 
126, 
127, 
329 
theorem, 
95-99, 
119, 
126,353,364 
for 
term 
automata, 
116-118, 
332 
theory, 
100, 
108 
N-ary 
symbol, 
108 
Natural 
numbers, 
207, 
230, 
282 
encoding 
in 
the 
>.-calculus, 
265 
in 
ZF, 
297 
Naur, 
P., 
129, 
134 
Nerode, 
A., 
99, 
118 
Next 
configuration 
relation 
2DFA,122 
PDA,159 
queue 
machine, 
368 
Turing 
machine, 
213, 
216 
NFA, 
"ee 
nondeterministic 
finite 
automaton 
Non-CFL, 
216 
Nondeterminism, 
25-26, 
162, 177, 
227 
N 
ondeterministic 
finite 
automaton, 
26, 
32, 
39 
pushdown 
automaton, 
157 

_____________________________________________
396 
Index 
N 
ondeterministic 
(cont.) 
one-state, 
172 
Turing 
machine, 
207 
Nonmonotone 
property, 
247 
Nonregular 
set, 
67, 
70, 
Nonterminal 
symbol 
CFG, 
136, 
132 
Post 
system, 
257 
Nontrivial 
property, 
245, 
246 
Normal 
form 
Chomsky, 
140, 
143-144, 
147, 191, 
192,199,203,334 
Greibach, 
140, 
144-147,307,334 
>.-calculus, 
265 
NPDA, 
$ee 
nondeterministic 
pushdown 
automaton 
Null 
string, 
8 
Number 
theory, 
209, 
284 
Oettinger, 
A.G., 
175 
Ogden, 
W., 
156 
One-way 
Turing 
machine, 
355 
Operand, 
183 
Operations 
set, 
10-13 
string, 
9-10 
Operator 
application, 
267 
Kleene 
algebra, 
55 
>.-calculus, 
263 
monotone, 
56 
number 
theory, 
282 
pattern, 
40, 
41 
precedence, 
45, 
182, 
186-190 
propositional, 
181, 
283 
regular 
expression, 
45 
304,308,337 
Oracle, 
274, 
349 
tape, 
274 
Turing 
machine, 
274 
Output 
tape, 
225 
P-ary 
representation, 
289, 
301 
P 
= 
NP 
problem, 
25 
PA, 
$ee 
Peano 
arithmetic 
Pairing 
function, 
277 
Palindromes, 
131, 
134 
Papadimitriou, 
C.R., 
281 
Paradox, 
295 
Russell's, 
209, 
297 
Paradoxical 
combinator, 
267 
Parentheses, 
$ee 
balanced 
parentheses 
Parenthesis 
language, 
198 
Parikh, 
R., 
71, 
201, 
205 
PariklI. 
map, 
201 
Parikh's 
theorem, 
201-205, 
354 
Parse 
tree, 
148; 
149, 181, 
203 
Parser, 
181 
Parsing, 
181-190 
Partial 
function, 
270, 
273, 
347 
ordet, 
56, 
96, 
187, 
203 
complete, 
339 
Path, 
148, 
203 
Pattern, 
40 
atomic, 
40, 
41 
compound, 
40, 
41 
matching, 
40, 
257 
operator, 
40, 
41 
PDA, 
$ee 
automaton, 
pushdown 
Pelik, 
1., 
60 
Peano 
arithmetic, 
209, 284, 
292, 297, 
350 
Period, 
74 
Periodic, 
179 
Perles, 
M., 
71, 
147, 
156, 197, 
255 
Permutation, 
337 
Pigeonhole 
principle, 
68, 69, 
150, 
179, 
205 
Pitts, 
W., 
24 
Polynomial, 
323. 
353, 
361 
time, 
25 
Pop, 
132, 
157 
Post, 
E.L., 
4, 
134, 
206. 
227, 244, 256, 
268 
Postage 
stamp 
problem, 
326 
Post 
system, 
4, 
134, 
206, 
256-257,268 
Power 
of 
a 
set, 
11 
oftwo, 
283 
set, 
32, 
231 
of 
N, 
230 
Precedence 
operator, 
45, 
182, 
186-190 
relation, 
187, 
308 
Predecessor, 
260 

_____________________________________________
Prefix, 
9, 
135 
proper, 
10, 
138 
Prime, 
7,217,283,324,335,351,363 
relatively, 
364 
Primitive 
recursion, 
259 
recursive 
function, 
259, 
273, 
313 
Principia 
Mathematica, 
209 
Product 
automaton, 
22 
construction, 
22-23, 
74, 
195, 
301, 
308,315 
Production 
CFG,132 
•-, 
141-142,147, 
334 
Post 
system, 
257 
unit, 
141-142,147,334 
Program 
that 
prints 
itself, 
287 
Projection, 
258 
Proof,285 
Prop'er 
prefix, 
10, 
138 
subtraction, 
260 
Property 
monotone, 
247,371 
nonmonotone, 
247 
nontrivial, 
245, 
246 
of 
r.e. 
sets, 
245 
. 
Proposi 
tional 
constant, 
181 
logic,181 
operator, 
181,283 
variable, 
181 
Provability, 
293, 
296 
Pump, 
203 
basic, 
203 
disjoint, 
205 
Pumping 
lemma 
for 
CFLs, 
131, 
148-156,203, 
337, 
354 
for 
regular 
sets, 
70-73, 
325, 354, 
363 
Push, 
132, 
157 
Pushdown 
automaton, 
see 
automaton 
store, 
131, 
157 
Quantifier 
alternation, 
71, 
276 
first-order, 
281, 
283 
second-order, 
281 
Queue, 
340 
Index 
397 
machine, 
257, 
340, 
354,368-370 
Quotient, 
202 
algebra, 
1Q8, 
116 
automaton, 
80, 
104 
construction, 
80-83, 
115 
Rabin, 
M.O., 
39, 
128 
Randomized 
computation, 
208 
Ray 
automaton, 
341 
R.e., 
see 
recursively 
enumerable 
-complete, 
278 
-hard,278 
Read 
head, 
119,157,224 
Read/write 
head, 
210, 
212, 220, 
221 
Recursion 
theorem, 
346-347, 
355 
Recursive, 
216, 
219 
algorithm, 
214 
in, 
275 
relation, 
312 
set, 
214 
Recursively 
enumerable, 
213, 216, 
219, 225, 
227 
in, 
275 
Redko, 
V.N., 
60 
Reduce, 
183, 
239 
Reducibility 
relation, 
240, 
244 
Reducible, 
240 
Reduction, 
234, 237, 
239. 
a-,264 
ß-,264 
combinatory 
logic, 
266, 
267 
many-one, 
239, 
275 
Turing, 
275, 
349 
Redundancy, 
44,45 
Refinement, 
90, 
95-97, 
275 
Reflexive 
transitive 
closure, 
see 
closure 
Reflexivity, 
49, 
56, 
80, 
96 
Regular 
expression, 
4, 
45, 
54, 
199 
operator, 
45 
set, 
17,45,198 
set 
of 
terms, 
111 

_____________________________________________
398 
Index 
Regularity 
preserving 
function, 
76, 
324 
Rejection, 
"ee 
acceptance 
Reject 
state, 
121, 
210, 
211 
Relation 
collapsing, 
80 
equivalence, 
49, 
80 
next 
configuration, 
"ee 
next 
configuration 
relation 
precedence, 
187 
reducibility, 
240, 
244 
transition, 
158 
well-founded, 
281 
Relational 
composition, 
see 
composition 
Relative 
computation, 
274-281 
Relatively 
prime, 
364 
Renaming 
bound 
variables, 
264 
Reversal 
closure 
of 
regular 
sets 
under, 
302 
DCFLs 
not 
closed 
under, 
197,338 
Reverse, 
302, 
353 
Rice,H.G.,248,335 
Rice's 
theorem, 
245-248,345, 
347, 
355,371 
Right 
congruence, 
90, 95, 97, 
116 
-linear 
grammar, 
see 
grammar 
Rightmost 
derivation, 
149 
Ritchie, 
D.M., 
273 
Robust, 
207 
Rogers, 
H., 
268, 
281 
Rose, 
G.F., 
39, 
197, 
255 
Rosenberg, 
A.L., 
227 
Rosenkrantz, 
D.J., 
190 
Rosser, 
J.B., 
265,268 
Rozenberg, 
G., 
71 
Rubik's 
cllbe, 
14 
Rule 
denesting, 
57 
of 
inference, 
284, 
285 
shifting, 
57 
Russell, 
B., 
5, 
209,297 
Russell's 
paradox, 
see 
paradox 
Salomaa, 
A., 
60 
Scheinberg, 
S., 
197 
SCHEME,262 
Schönfinkei, 
M., 
4, 
206, 
268 
Schützenberger, 
M.P., 
175, 180, 
200 
Scott, 
D.S., 
39, 
128 
Second 
incompleteness 
theOl'em, 
295-297 
Seiferas, 
J 
.1., 
76, 
324 
Self-reference, 
208, 
287, 
293 
Semantics, 
270 
Semidecidability, 
220 
Semi-infinite, 
212 
Semilinear 
set, 
202 
Semiring, 
56 
Sentence, 
133 
of 
first-order 
logic, 
284 
Sentential 
form, 
130, 
133 
Sequential 
composition, 
see 
composition 
Set 
accepted 
by 
M, 
17 
complement, 
"ee 
complement 
concatenation, 
see 
concatenation 
intersection, 
see 
intersection 
operations, 
10-13 
regular, 
see 
regular 
set 
theory 
Cantor, 
297 
Zermelo-F'raenkel, 
297 
see 
union 
Shamir, 
E., 
71, 
147, 156, 
197, 
255 
Shepherdson, 
J.C., 
128 
Shifting 
rule, 
57 
Shift-reduce 
parser, 
196 
Shoenfield, 
J 
.R., 
281 
Shuffie 
operator, 
304, 
308, 
337 
Sieve 
of 
Eratosthenes, 
217-219,309 
110 
Signature, 
108, 
339 
group,109 
Kleene 
algebra, 
109 
monoid,108 
Simple 
assignment, 
269 
Simplification 
rules 
for 
reguiar 
expressions, 
49 
Single 
letter 
alphabet, 
see 
alphabet 
Singleton, 
35, 
42, 
46 
Soare, 
R.I., 
281 
Soittola, 
M., 
60 
Soundness, 
285, 
292, 
294, 
296 

_____________________________________________
Spanier, 
E.H., 
197 
Square 
root, 
261, 
349 
Stack, 
131, 
157 
alphabet, 
158 
D., 
71, 
326 
Start 
configuration, 
"ee 
configuration 
state, 
15, 
120, 158, 
210, 
211 
symbol 
CFG,132 
Post 
system, 
257 
State, 
14, 
15, 
111 
accept, 
15, 
111, 
121, 158, 
210, 
211. 
enumeration, 
226 
final, 
"ee 
accept 
minimization, 
"ee 
collapsing 
of 
a 
while 
program, 
270 
reject, 
121, 
210, 
211 
start, 
15, 
120, 
158, 
210, 
211 
Statement, 
130 
Stearns, 
R.E., 
190, 
281 
Steinby, 
M., 
118 
Strategy, 
71, 
153 
String,8 
concatenation, 
"ee 
concatenation 
empty,8 
infinite, 
108 
null,8 
operations, 
9-10 
Subset 
construction, 
28, 
35-37, 
39, 
77,302,316,320,331,353 
Substitution, 
262, 264, 
293 
Successor, 
258, 262, 266, 
285 
Succinctness, 
317 
Support, 
103 
S"ymbol,8 
table, 
183 
Symmetry, 
49, 
80, 
101 
Syntactic 
algebra, 
"ee 
term 
algebra 
Syntax, 
181 
Tape, 
210 
alphabet, 
211 
cell,213 
output, 
225 
Term 
algebra, 
111 
automaton, 
108-118 
ground,109 
A-,207,262,263,345 
Terminal 
symbol 
CFG,132 
Post 
system, 
257 
Ternary 
symbol, 
108 
Thatcher, 
J.W., 
117 
Theorem, 
285 
Timesharing, 
227, 
247 
TM, 
"ee 
Turing 
machine 
Total,213 
Index 
399 
computable 
function, 
273, 
347, 
350 
Turing 
machine, 
216, 
219 
T-predicate, 
255 
Track, 
66, 
220, 
222 
Transition, 
14 
diagram, 
15 
function 
2DFA,120 
DFA,15 
NFA,33 
Turing 
machine, 
210, 
211, 
215 
matrix, 
16, 
317 
relation 
PDA,158 
system, 
14,24 
table,15 
Transitivity, 
49, 
56, 
80, 
96, 
101 
of 
reductions, 
240 
Trick,73 
Truth, 
292, 
296 
Turing, 
A.M., 
4, 
206, 210, 214, 227, 
244,268,281 
Turing 
machine, 
4, 
206, 
207, 210, 214, 
227, 
268 
deterministic 
one-tape. 
210, 
215 
minimal,347 
multi 
dimensional 
tape, 
207,342 
multitape, 
207, 
221 
nondeterministic, 
207 
oracle, 
274 
two-way 
infinite 
tape, 
207, 
223 
universal, 
228-231, 
244 
reduction, 
"ee 
reduction, 
Turing 
Two-stack 
automaton, 
224 
Two-way 
finite 
automaton, 
119-128, 
331 

_____________________________________________
400 
Index 
2DFA, 
see 
two-way 
fiilite 
automat 
on 
2NFA, 
see 
two-way 
finite 
automaton 
Type 
n 
grammar, 
see 
grammar 
theory, 
297 
UHman, 
J 
.D., 
24, 
190, 
197 
Ultimate 
periodicity, 
74-76, 
324, 353, 
354 
Unambiguous, 
131, 182, 
186 
Unary 
relation, 
110 
representation, 
323 
symbol,108 
Unbounded 
minimization, 
259, 
273 
Undecidability, 
236-238 
of 
the 
halting 
problem, 
231-234, 
244 
of 
the 
membership 
problem, 
234 
Union, 
10, 
110 
closure 
of 
CFLs 
under, 
195 
closure 
of 
r.e. 
sets 
under, 
340 
closure 
of 
regular 
sets 
under, 
24 
DCFLs 
not 
closed 
under, 
197 
disjoint, 
48 
Unit 
production, 
141-142, 
147,334 
Universal 
algebra, 
108 
machine, 
208 
relation, 
96 
simulation, 
208 
Turing 
machine, 
228-231, 
244 
UNIX, 
40 
V 
ALCOMPS, 
see 
valid 
computation 
history 
valid 
computation 
history, 
250, 255, 
284, 289, 
310, 
311 
Vardi, 
M.Y., 
128, 
331 
Variable 
combinatory 
logic, 
266 
284 
Kleene 
algebra, 
59 
A-calculus, 
263 
Post 
system, 
257 
programming 
language, 
269 
propositional, 
181 
Weiss, 
S., 
71,326 
WeH 
-defined, 
81 
-founded, 
203, 
281 
-parenthesized, 
182 
While 
loop, 
269 
program, 
256, 
269, 
335, 
348 
statement, 
130 
Whitehead, 
A.N., 
209 
Winning 
strategy, 
71, 
153 
Work 
tape, 
225 
Wright, 
J.B., 
117, 
118 
Yamada, 
H., 
39, 
54 
Yasuhara, 
A., 
268 
Young, 
P., 
268 
Younger, 
D.H., 
197 
Zermelo--Fraenkel 
set 
theory, 
297 
ZF, 
see 
Zermelo--Fraenkel 
set 
theory 
